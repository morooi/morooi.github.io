<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>MapReduce 的学习 - morooi&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="morooi&#039;s Blog"><meta name="msapplication-TileImage" content="img/touch-icon-iphone.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="morooi&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="144x144" href="img/touch-icon-iphone.png"><link rel="apple-touch-icon" sizes="152x152" href="img/touch-icon-ipad.png"><meta name="description" content="HDFS 和 MapReduce 是 Hadoop 的两个重要核心，其中 MapReduce 是 Hadoop 的分布式计算模型。MapReduce 主要分为两步 Map 步和 Reduce 步，引用网上流传很广的一个故事来解释，现在你要统计一个图书馆里面有多少本书，为了完成这个任务，你可以指派小明去统计书架 1，指派小红去统计书架 2，这个指派的过程就是 Map 步，最后，每个人统计完属于自己负"><meta property="og:type" content="blog"><meta property="og:title" content="MapReduce 的学习"><meta property="og:url" content="https://morooi.cn/2019/mapreduce/"><meta property="og:site_name" content="morooi&#039;s Blog"><meta property="og:description" content="HDFS 和 MapReduce 是 Hadoop 的两个重要核心，其中 MapReduce 是 Hadoop 的分布式计算模型。MapReduce 主要分为两步 Map 步和 Reduce 步，引用网上流传很广的一个故事来解释，现在你要统计一个图书馆里面有多少本书，为了完成这个任务，你可以指派小明去统计书架 1，指派小红去统计书架 2，这个指派的过程就是 Map 步，最后，每个人统计完属于自己负"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://morooi.cn/2019/mapreduce/wordcount01.png"><meta property="og:image" content="https://morooi.cn/2019/mapreduce/wordcount03.png"><meta property="og:image" content="https://morooi.cn/2019/mapreduce/wordcount04.png"><meta property="og:image" content="https://morooi.cn/2019/mapreduce/DailyAccessCount01.png"><meta property="og:image" content="https://morooi.cn/2019/mapreduce/DailyAccessCount02.png"><meta property="og:image" content="https://morooi.cn/2019/mapreduce/Temperature01.png"><meta property="og:image" content="https://morooi.cn/2019/mapreduce/Temperature02.png"><meta property="og:image" content="https://morooi.cn/2019/mapreduce/Filter01.png"><meta property="og:image" content="https://morooi.cn/2019/mapreduce/Filter02.png"><meta property="og:image" content="https://morooi.cn/2019/mapreduce/LogAnalyze.png"><meta property="article:published_time" content="2019-04-22T07:57:13.000Z"><meta property="article:modified_time" content="2021-01-06T10:33:02.000Z"><meta property="article:author" content="SJ Zhou"><meta property="article:tag" content="Java"><meta property="article:tag" content="Hadoop"><meta property="article:tag" content="MapReduce"><meta property="article:tag" content="wordcount"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/2019/mapreduce/wordcount01.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://morooi.cn/2019/mapreduce/"},"headline":"MapReduce 的学习","image":["https://morooi.cn/2019/mapreduce/wordcount01.png","https://morooi.cn/2019/mapreduce/wordcount03.png","https://morooi.cn/2019/mapreduce/wordcount04.png","https://morooi.cn/2019/mapreduce/DailyAccessCount01.png","https://morooi.cn/2019/mapreduce/DailyAccessCount02.png","https://morooi.cn/2019/mapreduce/Temperature01.png","https://morooi.cn/2019/mapreduce/Temperature02.png","https://morooi.cn/2019/mapreduce/Filter01.png","https://morooi.cn/2019/mapreduce/Filter02.png","https://morooi.cn/2019/mapreduce/LogAnalyze.png"],"datePublished":"2019-04-22T07:57:13.000Z","dateModified":"2021-01-06T10:33:02.000Z","author":{"@type":"Person","name":"SJ Zhou"},"publisher":{"@type":"Organization","name":"morooi's Blog","logo":{"@type":"ImageObject","url":{"text":"morooi's Blog"}}},"description":"HDFS 和 MapReduce 是 Hadoop 的两个重要核心，其中 MapReduce 是 Hadoop 的分布式计算模型。MapReduce 主要分为两步 Map 步和 Reduce 步，引用网上流传很广的一个故事来解释，现在你要统计一个图书馆里面有多少本书，为了完成这个任务，你可以指派小明去统计书架 1，指派小红去统计书架 2，这个指派的过程就是 Map 步，最后，每个人统计完属于自己负"}</script><link rel="canonical" href="https://morooi.cn/2019/mapreduce/"><link rel="icon" href="/img/icon-32x32.png"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/5.15.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Lato:wght@400"><link rel="stylesheet" href="/css/default.css"><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?15cece15ce32e027e81c277a94bbd778";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-0T8JD2VS0K" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-0T8JD2VS0K');</script><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
        <script>
            function switchTab(element) {
                const id = element.parentElement.id;
                const tabElements = element.parentElement.parentElement.children;
                const contentElements = element.parentElement.parentElement.parentElement.parentElement.children[1].children;
                for (let i = 0; i < tabElements.length; i++) {
                    const $tab = tabElements[i];
                    const $content = contentElements[i];
                    if ($tab.id === id) {
                        $tab.classList.add('is-active');
                    } else {
                        $tab.classList.remove('is-active');
                    }
                    if ($content.id === id) {
                        $content.classList.remove('is-hidden');
                    } else {
                        $content.classList.add('is-hidden');
                    }
                }
            }
        </script>
        <!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">morooi&#039;s Blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">全部文章</a><a class="navbar-item" href="/categories">分类</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/morooi"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-9-tablet is-9-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-04-22T07:57:13.000Z" title="4/22/2019, 3:57:13 PM">2019-04-22</time>发表</span><span class="level-item"><time dateTime="2021-01-06T10:33:02.000Z" title="1/6/2021, 6:33:02 PM">2021-01-06</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span><span class="level-item">37 分钟读完 (大约5542个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">MapReduce 的学习</h1><div class="content"><p>HDFS 和 MapReduce 是 Hadoop 的两个重要核心，其中 MapReduce 是 Hadoop 的分布式计算模型。MapReduce 主要分为两步 Map 步和 Reduce 步，引用网上流传很广的一个故事来解释，现在你要统计一个图书馆里面有多少本书，为了完成这个任务，你可以指派小明去统计书架 1，指派小红去统计书架 2，这个指派的过程就是 Map 步，最后，每个人统计完属于自己负责的书架后，再对每个人的结果进行累加统计，这个过程就是 Reduce 步。</p>
<blockquote>
<p>本代码运行环境为 ubuntu 18.04，使用 Hadoop 2.9.2 版本<br>首先按照之前的方法（<a href="/2019/installhadoop/">在 CentOS 7 中安装 Hadoop 与 HBase</a>）搭建并启动 Hadoop</p>
</blockquote>
<ul>
<li><a href="#mapreduce-%E5%8E%9F%E7%90%86">MapReduce 原理</a><ul>
<li><a href="#%E5%88%86%E6%9E%90-mapreduce-%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B">分析 MapReduce 执行过程</a></li>
<li><a href="#mapper-%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3">Mapper 任务的执行过程详解</a></li>
<li><a href="#reducer-%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3">Reducer 任务的执行过程详解</a></li>
</ul>
</li>
<li><a href="#%E5%AF%BC%E5%85%A5-mapreduce-%E7%9A%84%E7%9B%B8%E5%85%B3%E4%BE%9D%E8%B5%96">导入 MapReduce 的相关依赖</a></li>
<li><a href="#wordcount-%E5%AE%9E%E4%BE%8B">Wordcount 实例</a><ul>
<li><a href="#wordcount-%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B">Wordcount 整体流程</a></li>
<li><a href="#wordcount-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">WordCount 代码实现</a></li>
</ul>
</li>
<li><a href="#%E7%BB%9F%E8%AE%A1%E8%AE%BF%E9%97%AE%E6%AC%A1%E6%95%B0">统计访问次数</a></li>
<li><a href="#%E7%BB%9F%E8%AE%A1%E6%AF%8F%E5%B9%B4%E6%9C%80%E9%AB%98%E6%B0%94%E6%B8%A9">统计每年最高气温</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%8E%BB%E9%87%8D">数据去重</a></li>
<li><a href="#%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E5%88%86%E6%9E%90%E9%9D%9E%E7%BB%93%E6%9E%84%E5%8C%96%E6%96%87%E4%BB%B6">日志分析：分析非结构化文件</a></li>
</ul>
<span id="more"></span>

<h2 id="MapReduce-原理"><a href="#MapReduce-原理" class="headerlink" title="MapReduce 原理"></a>MapReduce 原理</h2><h3 id="分析-MapReduce-执行过程"><a href="#分析-MapReduce-执行过程" class="headerlink" title="分析 MapReduce 执行过程"></a>分析 MapReduce 执行过程</h3><p>MapReduce 运行的时候，会通过 Mapper 运行的任务读取 HDFS 中的数据文件，然后调用自己的方法，处理数据，最后输出。Reducer 任务会接收 Mapper 任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到 HDFS 的文件中。</p>
<h3 id="Mapper-任务的执行过程详解"><a href="#Mapper-任务的执行过程详解" class="headerlink" title="Mapper 任务的执行过程详解"></a>Mapper 任务的执行过程详解</h3><p>每个 Mapper 任务是一个 java 进程，它会读取 HDFS 中的文件，解析成很多的键值对，经过我们覆盖的 map 方法处理后，转换为很多的键值对再输出。整个 Mapper 任务的处理过程又可以分为以下六个阶段。</p>
<ul>
<li><p>第一阶段是把输入文件按照一定的标准分片 (InputSplit)，每个输入片的大小是固定的。默认情况下，输入片 (InputSplit) 的大小与数据块 (Block) 的大小是相同的。如果数据块 (Block) 的大小是默认值 64MB，输入文件有两个，一个是 32MB，一个是 72MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由一个 Mapper 进程处理。这里的三个输入片，会有三个 Mapper 进程处理。</p>
</li>
<li><p>第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键” 是每一行的起始位置 (单位是字节)，“值” 是本行的文本内容。</p>
</li>
<li><p>第三阶段是调用 Mapper 类中的 <code>map</code> 方法。第二阶段中解析出来的每一个键值对，调用一次 <code>map</code> 方法。如果有 1000 个键值对，就会调用 1000 次 <code>map</code> 方法。每一次调用 <code>map</code> 方法会输出零个或者多个键值对。</p>
</li>
<li><p>第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份（如北京、上海、山东等），那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是 Reducer 任务运行的数量。默认只有一个 Reducer 任务。</p>
</li>
<li><p>第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值对 <code>&lt;2,2&gt;,&lt;1,3&gt;,&lt;2,1&gt;</code>，键和值分别是整数。那么排序后的结果是 <code>&lt;1,3&gt;,&lt;2,1&gt;,&lt;2,2&gt;</code>。如果有第六阶段，那么进入第六阶段；如果没有，直接输出到本地的 linux 文件中。</p>
</li>
<li><p>第六阶段是对数据进行归约处理，也就是 <code>reduce</code> 处理。键相等的键值对会调用一次 <code>reduce</code> 方法。经过这一阶段，数据量会减少。归约后的数据输出到本地的 linxu 文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。</p>
</li>
</ul>
<h3 id="Reducer-任务的执行过程详解"><a href="#Reducer-任务的执行过程详解" class="headerlink" title="Reducer 任务的执行过程详解"></a>Reducer 任务的执行过程详解</h3><p>每个 Reducer 任务是一个 Java 进程。Reducer 任务接收 Mapper 任务的输出，归约处理后写入到 HDFS 中，可以分为几个阶段。</p>
<ul>
<li>第一阶段是 Reducer 任务会主动从 Mapper 任务复制其输出的键值对。Mapper 任务可能会有很多，因此 Reducer 会复制多个 Mapper 的输出。</li>
<li>第二阶段是把复制到 Reducer 本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。</li>
<li>第三阶段是对排序后的键值对调用 <code>reduce</code> 方法。键相等的键值对调用一次 <code>reduce</code> 方法，每次调用会产生零个或者多个键值对。最后把这些输出的键值对写入到 HDFS 文件中。在整个 MapReduce 程序的开发过程中，我们最大的工作量是覆盖 map 函数和覆盖 reduce 函数。</li>
</ul>
<h2 id="导入-MapReduce-的相关依赖"><a href="#导入-MapReduce-的相关依赖" class="headerlink" title="导入 MapReduce 的相关依赖"></a>导入 MapReduce 的相关依赖</h2><blockquote>
<p> 使用 Java 语言进行 <code>MapReduce</code> 操作首先需要导入必要的依赖</p>
<p>如果使用的是 maven 管理项目，则需要编辑项目目录的 <code>pom.xml</code> 文件</p>
</blockquote>
<p>打开 <code>pom.xml</code>，在 <code>&lt;dependencies&gt;...&lt;dependencies&gt;</code> 之间加入：</p>
<figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>

<p>如果不使用 Maven 管理，可以手动导入相关包。</p>
<h2 id="Wordcount-实例"><a href="#Wordcount-实例" class="headerlink" title="Wordcount 实例"></a>Wordcount 实例</h2><blockquote>
<p>功能：统计每一个单词在整个数据集中出现的总次数</p>
</blockquote>
<p>WordCount 实现的官方文档：<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r2.9.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v1.0">Example: WordCount v1.0</a> </p>
<h3 id="Wordcount-整体流程"><a href="#Wordcount-整体流程" class="headerlink" title="Wordcount 整体流程"></a>Wordcount 整体流程</h3><p>最简单的 MapReduce 应用程序至少包含 3 个部分：一个 Map 函数、一个 Reduce 函数和一个 main 函数。在运行一个 MapReduce 计算任务时候，任务过程被分为两个阶段：map 阶段和 reduce 阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。main 函数将作业控制和文件输入 / 输出结合起来。</p>
<p>过程如图所示：</p>
<p><img src="/2019/mapreduce/wordcount01.png" alt="WordCount 过程图"></p>
<p>对一个有三行文本的文件进行 MapReduce 操作。</p>
<p><strong>Map 过程</strong>：并行读取文本，对读取的单词进行 <code>map</code> 操作，每个词都以 <code>&lt;key,value&gt;</code> 形式生成。</p>
<p><strong>Reduce 过程</strong>：对 <code>map</code> 的结果进行排序，合并，最后得出词频。</p>
<h3 id="WordCount-代码实现"><a href="#WordCount-代码实现" class="headerlink" title="WordCount 代码实现"></a>WordCount 代码实现</h3><p>新建文件 <code>input.txt</code>，内容：</p>
<figure class="highlight ebnf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Deer Bear River</span></span><br><span class="line"><span class="attribute">Car Car River</span></span><br><span class="line"><span class="attribute">Deer Car Bear</span></span><br></pre></td></tr></tbody></table></figure>

<p>将其上传到 <code>HDFS</code> 中，在终端运行：</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">mkdir</span> /wordcount</span><br><span class="line">hadoop fs -<span class="built_in">mkdir</span> /wordcount/input</span><br><span class="line">hadoop fs -put input.txt /wordcount/input</span><br></pre></td></tr></tbody></table></figure>

<p>完整代码如下：</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCount</span> {</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    map 函数的输入键、输入值、输出键和输出值 Text 相当于java的string类型</span></span><br><span class="line"><span class="comment">    IntWritable 相当于java中的Integer</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TokenizerMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt;{</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException{</span><br><span class="line">            <span class="comment">//content用于输出内容的写入</span></span><br><span class="line">            <span class="comment">//map()方法的输入是一个键和一个值。我们首先将包含一行输入的Text值转换成java中的string类型</span></span><br><span class="line">            <span class="type">StringTokenizer</span> <span class="variable">itr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString()); <span class="comment">//这是一个分割字符串的类，java中默认的分隔符是:"空格","\t"制表符,"\n"换行符,"\r"回车符</span></span><br><span class="line">            <span class="keyword">while</span> (itr.hasMoreTokens()) { <span class="comment">//判断是否还有分隔符</span></span><br><span class="line">                word.set(itr.nextToken()); <span class="comment">//下一个字符串转换为Text类型，nextToken()：返回从当前位置到下一个分隔符的字符串。</span></span><br><span class="line">                context.write(word, one);</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">//reduce函数也有四个形式参数类型用于指定输入和输出类型。</span></span><br><span class="line">    <span class="comment">//reduce函数的输入类型必须匹配map函数的输出类型:即Text类型和Intwritable在这种情况下,reduce的输出也是Text和Intwritable</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">IntSumReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; {</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException{</span><br><span class="line">            <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable val : values){</span><br><span class="line">                sum += val.get();</span><br><span class="line">            }</span><br><span class="line">            result.set(sum);</span><br><span class="line">            context.write(key, result);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception {</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">"Word Count"</span>);</span><br><span class="line">        job.setJarByClass(WordCount.class);</span><br><span class="line">        job.setMapperClass(TokenizerMapper.class);  <span class="comment">//指定要用的map类型</span></span><br><span class="line">        job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">        job.setReducerClass(IntSumReducer.class);   <span class="comment">//指定要用的reduce类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);  <span class="comment">//控制reduce函数的输出类型</span></span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        <span class="comment">//定义输入数据的路径,可以是单个文件,也可以是一个目录(此时,将目录下所有文件当做输入)</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">"hdfs://localhost:8020/wordcount/input/input.txt"</span>)); </span><br><span class="line">        <span class="comment">//定义输出路径,指定reduce函数输出文件的写入目录。在运行作业前该目录是不应该存在的，否则Hadoop会报错并拒绝运行作业</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">"hdfs://localhost:8020/wordcount/output"</span>));</span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p><strong>实验结果</strong></p>
<p>运行程序，成功后可以看到在 <code>/wordcount/output/</code> 下多出两个文件</p>
<p><img src="/2019/mapreduce/wordcount03.png" alt="运行成功后多出两个文件"></p>
<p>其中第二个文件里为运行完成后的结果</p>
<p><img src="/2019/mapreduce/wordcount04.png" alt="WordCount 结果"></p>
<p><strong>WordCount 完成！</strong></p>
<h2 id="统计访问次数"><a href="#统计访问次数" class="headerlink" title="统计访问次数"></a>统计访问次数</h2><p>统计用户在 2019 年度每个自然日的总访问次数。原始数据文件中提供了用户名称与访问日期。这个任务就是要获取以每个自然日为单位的所有用户访问次数的累加值。如果通过 MapReduce 编程实现这个任务，首先要考虑的是，Mapper 与 Reducer 各自的处理逻辑是怎样的；然后根据处理逻辑编写出核心代码；最后在 Idea 中编写完整代码，编译打包后提交给集群运行。</p>
<p><strong>分析思路和逻辑</strong></p>
<ol>
<li><p>输入 / 输出格式。</p>
<p>这里社交网站用户的访问日期在格式上都属于文本格式，访问次数为整型数据格式。其组成的键值对为 <code>&lt;访问日期，访问次数&gt;</code>，因此 Mapper 的输出与 Reducer 的输出都选用 Text 类与 IntWritble 类。</p>
</li>
<li><p>Mapper 要实现的计算逻辑</p>
<p>Map 函数的主要任务是读取用户访问文件中的数据，输出所有访问日期与初始次数的键值对。<code>&lt;访问日期，1 &gt;</code></p>
</li>
<li><p>Reducer 要实现的计算逻辑</p>
<p>读取 Mapper 输出的键值对 <code>&lt;访问日期，1&gt;</code>，进行累加。</p>
</li>
</ol>
<p><code>user_login.txt</code> 访问日期内容如下</p>
<p>将其上传到 <code>HDFS</code> 中，路径为 <code>/DailyAccessCount/input</code></p>
<figure class="highlight txt"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Nehru,2019-01-01</span><br><span class="line">Dane,2019-01-01</span><br><span class="line">Walter,2019-01-01</span><br><span class="line">Gloria,2019-01-01</span><br><span class="line">Clarke,2019-01-01</span><br><span class="line">Madeline,2019-01-01</span><br><span class="line">Kevyn,2019-01-01</span><br><span class="line">Rebecca,2019-01-01</span><br><span class="line">Calista,2019-01-01</span><br><span class="line">Madeline,2019-01-02</span><br><span class="line">Kevyn,2019-01-02</span><br><span class="line">Rebecca,2019-01-03</span><br><span class="line">Calista,2019-01-04</span><br><span class="line">Walter,2019-03-12</span><br><span class="line">Gloria,2019-03-12</span><br><span class="line">Clarke,2019-03-12</span><br></pre></td></tr></tbody></table></figure>

<p>程序代码如下</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DailyAccessCount</span> {</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MyMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; {</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException {</span><br><span class="line">            <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line">            String array[] = line.split(<span class="string">","</span>);   <span class="comment">//指定逗号为分隔符，组成数组</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">keyOutput</span> <span class="operator">=</span> array[<span class="number">1</span>];        <span class="comment">//提取数组中的访问日期作为key</span></span><br><span class="line">            context.write(<span class="keyword">new</span> <span class="title class_">Text</span>(keyOutput), one);    <span class="comment">//组成键值对</span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MyReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; {</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException {</span><br><span class="line">            <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;    <span class="comment">//定义累加器，初始值为0</span></span><br><span class="line">            <span class="keyword">for</span> (IntWritable val : values) {</span><br><span class="line">                sum += val.get();   <span class="comment">//将相同键的所有值进行累加</span></span><br><span class="line">            }</span><br><span class="line">            result.set(sum);</span><br><span class="line">            context.write(key, result); <span class="comment">//输出访问日期，总访问次数</span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception {</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">"Daily Access Count"</span>);  <span class="comment">//新建Job并设置主类</span></span><br><span class="line">        job.setJarByClass(DailyAccessCount.class);</span><br><span class="line">        job.setMapperClass(MyMapper.class); <span class="comment">//为作业设置map类</span></span><br><span class="line">        job.setReducerClass(MyReducer.class);   <span class="comment">//为作业设置reduce类</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);   <span class="comment">//设置map输出key的格式类</span></span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);  <span class="comment">//设置map输出value的格式类</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);  <span class="comment">//设置输出键的格式类</span></span><br><span class="line">        job.setOutputValueClass(IntWritable.class); <span class="comment">//设置输出值的格式类</span></span><br><span class="line">        <span class="comment">//定义输入数据的路径,可以是单个文件,也可以是一个目录(此时,将目录下所有文件当做输入)</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">"hdfs://localhost:8020/DailyAccessCount/input/user_login.txt"</span>));</span><br><span class="line">        <span class="comment">//定义输出路径,指定reduce函数输出文件的写入目录。在运行作业前该目录是不应该存在的，否则Hadoop会报错并拒绝运行作业</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">"hdfs://localhost:8020/DailyAccessCount/output"</span>));</span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);   <span class="comment">//通知集群运行这个作业，并阻塞直到作业完成</span></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p><strong>实验结果</strong></p>
<p>运行程序，成功后可以看到在 <code>/DailyAccessCount/output/</code> 下多出两个文件</p>
<p><img src="/2019/mapreduce/DailyAccessCount01.png" alt="运行成功后多出两个文件"></p>
<p>其中第二个文件里为运行完成后的结果</p>
<p><img src="/2019/mapreduce/DailyAccessCount02.png" alt="DailyAccessCount 结果"></p>
<p><strong>统计访问次数完成！</strong></p>
<h2 id="统计每年最高气温"><a href="#统计每年最高气温" class="headerlink" title="统计每年最高气温"></a>统计每年最高气温</h2><p>有如下文件 <code>input_temperature.txt</code>，其中 <code>2010012325</code> 表示在 2010 年 01 月 23 日的气温为 25 度。使用 MapReduce，计算每一年出现过的最大气温。</p>
<p>将其上传到 <code>HDFS</code> 中，路径为 <code>/Temperature/input</code></p>
<figure class="highlight txt"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">2014010114</span><br><span class="line">2014010216</span><br><span class="line">2014010317</span><br><span class="line">2014010410</span><br><span class="line">2014010506</span><br><span class="line">2012010609</span><br><span class="line">2012010732</span><br><span class="line">2012010812</span><br><span class="line">2012010919</span><br><span class="line">2012011023</span><br><span class="line">2001010116</span><br><span class="line">2001010212</span><br><span class="line">2001010310</span><br><span class="line">2001010411</span><br><span class="line">2001010529</span><br><span class="line">2013010619</span><br><span class="line">2013010722</span><br><span class="line">2013010812</span><br><span class="line">2013010929</span><br><span class="line">2013011023</span><br><span class="line">2008010105</span><br><span class="line">2008010216</span><br><span class="line">2008010337</span><br><span class="line">2008010414</span><br><span class="line">2008010516</span><br><span class="line">2007010619</span><br><span class="line">2007010712</span><br><span class="line">2007010812</span><br><span class="line">2007010999</span><br><span class="line">2007011023</span><br><span class="line">2010010114</span><br><span class="line">2010010216</span><br><span class="line">2010010317</span><br><span class="line">2010010410</span><br><span class="line">2010010506</span><br><span class="line">2015010649</span><br><span class="line">2015010722</span><br><span class="line">2015010812</span><br><span class="line">2015010999</span><br><span class="line">2015011023</span><br></pre></td></tr></tbody></table></figure>

<p>设计代码如下：</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Temperature</span> {</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 四个泛型类型分别代表：</span></span><br><span class="line"><span class="comment">    * KeyIn     Mapper的输入数据的Key，这里是每行文字的起始位置（0,11,....）</span></span><br><span class="line"><span class="comment">    * ValueIn   Mapper的输入数据的Value，这里是每行文字</span></span><br><span class="line"><span class="comment">    * KeyOut    Mapper的输出数据的Key，这里是每行文字中的“年份”</span></span><br><span class="line"><span class="comment">    * ValueOut  Mapper的输出数据的Value，这里是每行文字中的“气温”</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TempMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt; {</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException {</span><br><span class="line">            <span class="comment">//打印样本：Before Mapper:0,2014010114</span></span><br><span class="line">            System.out.print(<span class="string">"Before Mapper:"</span> + key + <span class="string">","</span> + value);</span><br><span class="line">            <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line">            <span class="type">String</span> <span class="variable">year</span> <span class="operator">=</span> line.substring(<span class="number">0</span>, <span class="number">4</span>);</span><br><span class="line">            <span class="type">int</span> <span class="variable">temperature</span> <span class="operator">=</span> Integer.parseInt(line.substring(<span class="number">8</span>));</span><br><span class="line">            context.write(<span class="keyword">new</span> <span class="title class_">Text</span>(year), <span class="keyword">new</span> <span class="title class_">IntWritable</span>(temperature));</span><br><span class="line">            <span class="comment">//打印样本：After Mapper:2014,14</span></span><br><span class="line">            System.out.println(<span class="string">" ==&gt; After Mapper:"</span> + <span class="keyword">new</span> <span class="title class_">Text</span>(year) + <span class="string">","</span> + <span class="keyword">new</span> <span class="title class_">IntWritable</span>(temperature));</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 四个泛型类型分别代表：</span></span><br><span class="line"><span class="comment">     * KeyIn        Reducer的输入数据的Key，这里是每行文字中的“年份”</span></span><br><span class="line"><span class="comment">     * ValueIn      Reducer的输入数据的Value，这里是每行文字中的“气温”</span></span><br><span class="line"><span class="comment">     * KeyOut       Reducer的输出数据的Key，这里是不重复的“年份”</span></span><br><span class="line"><span class="comment">     * ValueOut     Reducer的输出数据的Value，这里是这一年中的“最高气温”</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TempReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; {</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException {</span><br><span class="line">            <span class="type">int</span> <span class="variable">maxValue</span> <span class="operator">=</span> Integer.MIN_VALUE;</span><br><span class="line">            <span class="type">StringBuffer</span> <span class="variable">sb</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuffer</span>();</span><br><span class="line">            <span class="comment">//取values的最大值</span></span><br><span class="line">            <span class="keyword">for</span> (IntWritable value : values) {</span><br><span class="line">                maxValue = Math.max(maxValue, value.get());</span><br><span class="line">                sb.append(value).append(<span class="string">","</span>);</span><br><span class="line">            }</span><br><span class="line">            <span class="comment">//打印样本：Before Reduce:2001,12,10,11,29,16,</span></span><br><span class="line">            System.out.print(<span class="string">"Before Reduce:"</span> + key + <span class="string">","</span> + sb.toString());</span><br><span class="line">            context.write(key, <span class="keyword">new</span> <span class="title class_">IntWritable</span>(maxValue));</span><br><span class="line">            <span class="comment">//打印样本：After Reduce:2001,29</span></span><br><span class="line">            System.out.println(<span class="string">" ==&gt; After Reduce:"</span> + key + <span class="string">","</span> + maxValue);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception {</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">"Temperature"</span>);</span><br><span class="line">        job.setJarByClass(Temperature.class);</span><br><span class="line">        job.setMapperClass(TempMapper.class);   <span class="comment">//为作业设置map类</span></span><br><span class="line">        job.setReducerClass(TempReducer.class); <span class="comment">//为作业设置reduce类</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);   <span class="comment">//设置map输出key的格式类</span></span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);  <span class="comment">//设置map输出value的格式类</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);  <span class="comment">//设置输出键的格式类</span></span><br><span class="line">        job.setOutputValueClass(IntWritable.class); <span class="comment">//设置输出值的格式类</span></span><br><span class="line">        <span class="comment">//定义输入数据的路径,可以是单个文件,也可以是一个目录(此时,将目录下所有文件当做输入)</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">"hdfs://localhost:8020/Temperature/input/input_temperature.txt"</span>));</span><br><span class="line">        <span class="comment">//定义输出路径,指定reduce函数输出文件的写入目录。在运行作业前该目录是不应该存在的，否则Hadoop会报错并拒绝运行作业</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">"hdfs://localhost:8020/Temperature/output"</span>));</span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);   <span class="comment">//通知集群运行这个作业，并阻塞直到作业完成</span></span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p><strong>实验结果</strong></p>
<p>运行程序，成功后可以看到在 <code>/Temperature/output/</code> 下多出两个文件</p>
<p><img src="/2019/mapreduce/Temperature01.png" alt="运行成功后多出两个文件"></p>
<p>其中第二个文件里为运行完成后的结果</p>
<p><img src="/2019/mapreduce/Temperature02.png" alt="Temperature 结果"></p>
<p><strong>统计每年最高气温完成！</strong></p>
<h2 id="数据去重"><a href="#数据去重" class="headerlink" title="数据去重"></a>数据去重</h2><p><strong>实验原理</strong></p>
<p>“数据去重” 主要是为了掌握和利用并行化思想来对数据进行有意义的筛选。统计大数据集上的数据种类个数、从网站日志中计算访问地等这些看似庞杂的任务都会涉及数据去重。</p>
<p>数据去重的最终目标是让原始数据中出现次数超过一次的数据在输出文件中只出现一次。在 MapReduce 流程中，map 的输出 <code>&lt;key,value&gt;</code> 经过 shuffle 过程聚集成 <code>&lt;key,value-list&gt;</code> 后交给 reduce。我们自然而然会想到将同一个数据的所有记录都交给一台 reduce 机器，无论这个数据出现多少次，只要在最终结果中输出一次就可以了。具体就是 reduce 的输入应该以数据作为 key，而对 value-list 则没有要求（可以设置为空）。当 reduce 接收到一个 <code>&lt;key,value-list&gt;</code> 时就直接将输入的 key 复制到输出的 key 中，并将 value 设置成空值，然后输出 <code>&lt;key,value&gt;</code>。</p>
<p><strong>实验数据</strong></p>
<p>现有一个某电商网站的数据文件，名为 <code>buyer_favorite1</code>，记录了用户收藏的商品以及收藏的日期，文件 <code>buyer_favorite1</code> 中包含（用户 id，商品 id，收藏日期）三个字段，数据内容以 <code>\t</code> 分割，由于数据很大，所以为了方便统计我们只截取它的一部分数据，内容如下:</p>
<figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">user_id</span> product_id date</span><br><span class="line"><span class="attribute">10181</span> <span class="number">1000481</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">04</span> <span class="number">16</span>:<span class="number">54</span>:<span class="number">31</span></span><br><span class="line"><span class="attribute">20001</span> <span class="number">1001597</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">07</span> <span class="number">15</span>:<span class="number">07</span>:<span class="number">52</span></span><br><span class="line"><span class="attribute">20001</span> <span class="number">1001560</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">07</span> <span class="number">15</span>:<span class="number">08</span>:<span class="number">27</span></span><br><span class="line"><span class="attribute">20042</span> <span class="number">1001368</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">08</span> <span class="number">08</span>:<span class="number">20</span>:<span class="number">30</span></span><br><span class="line"><span class="attribute">20067</span> <span class="number">1002061</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">08</span> <span class="number">16</span>:<span class="number">45</span>:<span class="number">33</span></span><br><span class="line"><span class="attribute">20056</span> <span class="number">1003289</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">12</span> <span class="number">10</span>:<span class="number">50</span>:<span class="number">55</span></span><br><span class="line"><span class="attribute">20056</span> <span class="number">1003290</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">12</span> <span class="number">11</span>:<span class="number">57</span>:<span class="number">35</span></span><br><span class="line"><span class="attribute">20056</span> <span class="number">1003292</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">12</span> <span class="number">12</span>:<span class="number">05</span>:<span class="number">29</span></span><br><span class="line"><span class="attribute">20054</span> <span class="number">1002420</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">14</span> <span class="number">15</span>:<span class="number">24</span>:<span class="number">12</span></span><br><span class="line"><span class="attribute">20055</span> <span class="number">1001679</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">14</span> <span class="number">19</span>:<span class="number">46</span>:<span class="number">04</span></span><br><span class="line"><span class="attribute">20054</span> <span class="number">1010675</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">14</span> <span class="number">15</span>:<span class="number">23</span>:<span class="number">53</span></span><br><span class="line"><span class="attribute">20054</span> <span class="number">1002429</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">14</span> <span class="number">17</span>:<span class="number">52</span>:<span class="number">45</span></span><br><span class="line"><span class="attribute">20076</span> <span class="number">1002427</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">14</span> <span class="number">19</span>:<span class="number">35</span>:<span class="number">39</span></span><br><span class="line"><span class="attribute">20054</span> <span class="number">1003326</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">20</span> <span class="number">12</span>:<span class="number">54</span>:<span class="number">44</span></span><br><span class="line"><span class="attribute">20056</span> <span class="number">1002420</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">11</span>:<span class="number">24</span>:<span class="number">49</span></span><br><span class="line"><span class="attribute">20064</span> <span class="number">1002422</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">11</span>:<span class="number">35</span>:<span class="number">54</span></span><br><span class="line"><span class="attribute">20056</span> <span class="number">1003066</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">11</span>:<span class="number">43</span>:<span class="number">01</span></span><br><span class="line"><span class="attribute">20056</span> <span class="number">1003055</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">11</span>:<span class="number">43</span>:<span class="number">06</span></span><br><span class="line"><span class="attribute">20056</span> <span class="number">1010183</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">11</span>:<span class="number">45</span>:<span class="number">24</span></span><br><span class="line"><span class="attribute">20056</span> <span class="number">1002422</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">11</span>:<span class="number">45</span>:<span class="number">49</span></span><br><span class="line"><span class="attribute">20056</span> <span class="number">1003100</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">11</span>:<span class="number">45</span>:<span class="number">54</span></span><br><span class="line"><span class="attribute">20056</span> <span class="number">1003094</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">11</span>:<span class="number">45</span>:<span class="number">57</span></span><br><span class="line"><span class="attribute">20056</span> <span class="number">1003064</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">11</span>:<span class="number">46</span>:<span class="number">04</span></span><br><span class="line"><span class="attribute">20056</span> <span class="number">1010178</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">16</span>:<span class="number">15</span>:<span class="number">20</span></span><br><span class="line"><span class="attribute">20076</span> <span class="number">1003101</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">27</span></span><br><span class="line"><span class="attribute">20076</span> <span class="number">1003103</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">05</span></span><br><span class="line"><span class="attribute">20076</span> <span class="number">1003100</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">18</span></span><br><span class="line"><span class="attribute">20076</span> <span class="number">1003066</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">31</span></span><br><span class="line"><span class="attribute">20054</span> <span class="number">1003103</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">16</span>:<span class="number">40</span>:<span class="number">14</span></span><br><span class="line"><span class="attribute">20054</span> <span class="number">1003100</span> <span class="number">2010</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">16</span>:<span class="number">40</span>:<span class="number">16</span></span><br></pre></td></tr></tbody></table></figure>
<!-- | user_id | product_id | date                |
| ------- | ---------- | ------------------- |
| 10181   | 1000481    | 2010-04-04 16:54:31 |
| 20001   | 1001597    | 2010-04-07 15:07:52 |
| 20001   | 1001560    | 2010-04-07 15:08:27 |
| 20042   | 1001368    | 2010-04-08 08:20:30 |
| 20067   | 1002061    | 2010-04-08 16:45:33 |
| 20056   | 1003289    | 2010-04-12 10:50:55 |
| 20056   | 1003290    | 2010-04-12 11:57:35 |
| 20056   | 1003292    | 2010-04-12 12:05:29 |
| 20054   | 1002420    | 2010-04-14 15:24:12 |
| 20055   | 1001679    | 2010-04-14 19:46:04 |
| 20054   | 1010675    | 2010-04-14 15:23:53 |
| 20054   | 1002429    | 2010-04-14 17:52:45 |
| 20076   | 1002427    | 2010-04-14 19:35:39 |
| 20054   | 1003326    | 2010-04-20 12:54:44 |
| 20056   | 1002420    | 2010-04-15 11:24:49 |
| 20064   | 1002422    | 2010-04-15 11:35:54 |
| 20056   | 1003066    | 2010-04-15 11:43:01 |
| 20056   | 1003055    | 2010-04-15 11:43:06 |
| 20056   | 1010183    | 2010-04-15 11:45:24 |
| 20056   | 1002422    | 2010-04-15 11:45:49 |
| 20056   | 1003100    | 2010-04-15 11:45:54 |
| 20056   | 1003094    | 2010-04-15 11:45:57 |
| 20056   | 1003064    | 2010-04-15 11:46:04 |
| 20056   | 1010178    | 2010-04-15 16:15:20 |
| 20076   | 1003101    | 2010-04-15 16:37:27 |
| 20076   | 1003103    | 2010-04-15 16:37:05 |
| 20076   | 1003100    | 2010-04-15 16:37:18 |
| 20076   | 1003066    | 2010-04-15 16:37:31 |
| 20054   | 1003103    | 2010-04-15 16:40:14 |
| 20054   | 1003100    | 2010-04-15 16:40:16 | -->

<p>将其上传到 <code>HDFS</code> 中，路径为 <code>/Filter/input</code></p>
<p>要求用 Java 编写 MapReduce 程序，根据商品 id 进行去重，统计用户收藏商品中都有哪些商品被收藏。</p>
<!-- | product_id |
| ---------- |
|  1000481   |
|  1001368   |
|  1001560   |
|  1001597   |
|  1001679   |
|  1002061   |
|  1002420   |
|  1002422   |
|  1002427   |
|  1002429   |
|  1003055   |
|  1003064   |
|  1003066   |
|  1003094   |
|  1003100   |
|  1003101   |
|  1003103   |
|  1003289   |
|  1003290   |
|  1003292   |
|  1003326   |
|  1010178   |
|  1010183   |
|  1010675   | -->

<p><strong>实验源码</strong></p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Filter</span> {</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Map</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, NullWritable&gt; {</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="type">Text</span> <span class="variable">newKey</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException {</span><br><span class="line">            <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line">            System.out.println(<span class="string">"line is "</span> + line);</span><br><span class="line">            <span class="keyword">if</span> (line != <span class="literal">null</span>) {</span><br><span class="line">                String arr[] = line.split(<span class="string">" "</span>);</span><br><span class="line">                System.out.println(<span class="string">"a[1] is "</span> + arr[<span class="number">1</span>]);</span><br><span class="line">                newKey.set(arr[<span class="number">1</span>]);</span><br><span class="line">                context.write(newKey, NullWritable.get());</span><br><span class="line">                System.out.println(<span class="string">"the new key is "</span> + newKey);</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Reduce</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, NullWritable, Text, NullWritable&gt; {</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException {</span><br><span class="line">            context.write(key, NullWritable.get());</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException{</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">"Filter"</span>);</span><br><span class="line">        job.setJarByClass(Filter.class);</span><br><span class="line">        job.setMapperClass(Map.class);</span><br><span class="line">        job.setReducerClass(Reduce.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">        FileInputFormat.addInputPath(job ,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">"hdfs://localhost:8020/Filter/input/buyer_favorite.txt"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">"hdfs://localhost:8020/Filter/output"</span>));</span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p><strong>实验结果</strong></p>
<p>运行程序，成功后可以看到在 <code>/Filter/output/</code> 下多出两个文件</p>
<p><img src="/2019/mapreduce/Filter01.png" alt="运行成功后多出两个文件"></p>
<p>其中第二个文件里为运行完成后的结果</p>
<p><img src="/2019/mapreduce/Filter02.png" alt="数据去重结果"></p>
<p><strong>数据去重完成！</strong></p>
<h2 id="日志分析：分析非结构化文件"><a href="#日志分析：分析非结构化文件" class="headerlink" title="日志分析：分析非结构化文件"></a>日志分析：分析非结构化文件</h2><p><strong>根据 tomcat 日志计算 url 访问情况</strong></p>
<p>要求：区别统计 GET 和 POST URL 访问量</p>
<p>结果为：访问方式、URL、访问量</p>
<p>测试数据集：</p>
<figure class="highlight txt"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">196.168.2.1 - - [03/Jul/2014:23:36:38 +0800] "GET /course/detail/3.htm HTTP/1.0" 200 38435 0.038</span><br><span class="line">182.131.89.195 - - [03/Jul/2014:23:37:43 +0800] "GET /html/notes/20140617/888.html HTTP/1.0" 301 - 0.000</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:38:27 +0800] "POST /service/notes/addViewTimes_23.htm HTTP/1.0" 200 2 0.003</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:39:03 +0800] "GET /html/notes/20140617/779.html HTTP/1.0" 200 69539 0.046</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:43:00 +0800] "GET /html/notes/20140318/24.html HTTP/1.0" 200 67171 0.049</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:43:59 +0800] "POST /service/notes/addViewTimes_779.htm HTTP/1.0" 200 1 0.003</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:45:51 +0800] "GET /html/notes/20140617/888.html HTTP/1.0" 200 70044 0.060</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:46:17 +0800] "GET /course/list/73.htm HTTP/1.0" 200 12125 0.010</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:46:58 +0800] "GET /html/notes/20140609/542.html HTTP/1.0" 200 94971 0.077</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:48:31 +0800] "POST /service/notes/addViewTimes_24.htm HTTP/1.0" 200 2 0.003</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:48:34 +0800] "POST /service/notes/addViewTimes_542.htm HTTP/1.0" 200 2 0.003</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:49:31 +0800] "GET /notes/index-top-3.htm HTTP/1.0" 200 53494 0.041</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:50:55 +0800] "GET /html/notes/20140609/544.html HTTP/1.0" 200 183694 0.076</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:53:32 +0800] "POST /service/notes/addViewTimes_544.htm HTTP/1.0" 200 2 0.004</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:54:53 +0800] "GET /service/notes/addViewTimes_900.htm HTTP/1.0" 200 151770 0.054</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:57:42 +0800] "GET /html/notes/20140620/872.html HTTP/1.0" 200 52373 0.034</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:58:17 +0800] "POST /service/notes/addViewTimes_900.htm HTTP/1.0" 200 2 0.003</span><br><span class="line">196.168.2.1 - - [03/Jul/2014:23:58:51 +0800] "GET /html/notes/20140617/888.html HTTP/1.0" 200 70044 0.057</span><br><span class="line">186.76.76.76 - - [03/Jul/2014:23:48:34 +0800] "POST /service/notes/addViewTimes_542.htm HTTP/1.0" 200 2 0.003</span><br><span class="line">186.76.76.76 - - [03/Jul/2014:23:46:17 +0800] "GET /course/list/73.htm HTTP/1.0" 200 12125 0.010</span><br><span class="line">8.8.8.8 - - [03/Jul/2014:23:46:58 +0800] "GET /html/notes/20140609/542.html HTTP/1.0" 200 94971 0.077</span><br></pre></td></tr></tbody></table></figure>

<p>将其上传到 <code>HDFS</code> 中，路径为 <code>/LogAnalyze/input</code></p>
<p>由于 Tomcat 日志是不规则的，需要先过滤清洗数据。这里使用 MapReduce 进行操作。</p>
<p><strong>代码实现：</strong></p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogAnalyze</span> {</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">LogMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text , IntWritable&gt; {</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">val</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException {</span><br><span class="line">            <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString().trim();</span><br><span class="line">            <span class="type">String</span> <span class="variable">tmp</span> <span class="operator">=</span> handlerLog(line);</span><br><span class="line">            <span class="keyword">if</span> (tmp.length() &gt; <span class="number">0</span>) {</span><br><span class="line">                context.write(<span class="keyword">new</span> <span class="title class_">Text</span>(tmp), val);</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">//196.168.2.1 - - [03/Jul/2014:23:36:38 +0800] "GET /course/detail/3.htm HTTP/1.0" 200 38435 0.038</span></span><br><span class="line">        <span class="keyword">private</span> String <span class="title function_">handlerLog</span><span class="params">(String line)</span> {</span><br><span class="line">            <span class="type">String</span> <span class="variable">result</span> <span class="operator">=</span> <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">try</span> {</span><br><span class="line">                <span class="keyword">if</span> (line.length() &gt; <span class="number">20</span>) {</span><br><span class="line">                    <span class="keyword">if</span> (line.indexOf(<span class="string">"GET"</span>) &gt; <span class="number">0</span>) {</span><br><span class="line">                        result = line.substring(line.indexOf(<span class="string">"GET"</span>), line.indexOf(<span class="string">"HTTP/1.0"</span>)).trim();</span><br><span class="line">                    } <span class="keyword">else</span> <span class="keyword">if</span> (line.indexOf(<span class="string">"POST"</span>) &gt; <span class="number">0</span>) {</span><br><span class="line">                        result = line.substring(line.indexOf(<span class="string">"POST"</span>), line.indexOf(<span class="string">"HTTP/1.0"</span>)).trim();</span><br><span class="line">                    }</span><br><span class="line">                }</span><br><span class="line">            } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">                System.out.println(line);</span><br><span class="line">            }</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">LogReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; {</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException {</span><br><span class="line">            <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable val : values) {</span><br><span class="line">                sum += val.get();</span><br><span class="line">            }</span><br><span class="line">            context.write(key, <span class="keyword">new</span> <span class="title class_">IntWritable</span>(sum));</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception {</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">"Log Analyze"</span>);</span><br><span class="line">        job.setJarByClass(LogAnalyze.class);</span><br><span class="line">        job.setMapperClass(LogMapper.class);</span><br><span class="line">        job.setReducerClass(LogReducer.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">"hdfs://localhost:8020/LogAnalyze/input/data.txt"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">"hdfs://localhost:8020/LogAnalyze/output"</span>));</span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p><strong>分析结果</strong></p>
<p><img src="/2019/mapreduce/LogAnalyze.png" alt="分析结果"></p>
<p><strong>Log 分析完成！</strong></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>MapReduce 的学习</p><p><a href="https://morooi.cn/2019/mapreduce/">https://morooi.cn/2019/mapreduce/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>SJ Zhou</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-04-22</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-01-06</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Java/">Java</a><a class="link-muted mr-2" rel="tag" href="/tags/Hadoop/">Hadoop</a><a class="link-muted mr-2" rel="tag" href="/tags/MapReduce/">MapReduce</a><a class="link-muted mr-2" rel="tag" href="/tags/wordcount/">wordcount</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019/spark/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">弹性分布式数据集 (Resilient Distributed Datasets, RDD)</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/anaconda/"><span class="level-item">Conda 常用命令的整理</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "c90c7ada5c15067f4cfc8d8895c73dec",
            repo: "morooi.github.io",
            owner: "morooi",
            clientID: "dbcea16be06da0b68055",
            clientSecret: "f6f0210aa145ed4e19054f45161b51eb88565a53",
            admin: ["morooi"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.jpg" alt="morooi"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">morooi</p><p class="is-size-6 is-block">morooi&#039;s Blog</p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">24</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">36</p></a></div></div></nav><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/morooi"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/morooi_"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Telegram" href="https://t.me/morooi"><i class="fab fa-telegram"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://weibo.com/zhoushaojing"><i class="fab fa-weibo"></i></a></div></div></div><div class="card widget is-sticky" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#MapReduce-原理"><span class="level-left"><span class="level-item">MapReduce 原理</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#分析-MapReduce-执行过程"><span class="level-left"><span class="level-item">分析 MapReduce 执行过程</span></span></a></li><li><a class="level is-mobile" href="#Mapper-任务的执行过程详解"><span class="level-left"><span class="level-item">Mapper 任务的执行过程详解</span></span></a></li><li><a class="level is-mobile" href="#Reducer-任务的执行过程详解"><span class="level-left"><span class="level-item">Reducer 任务的执行过程详解</span></span></a></li></ul></li><li><a class="level is-mobile" href="#导入-MapReduce-的相关依赖"><span class="level-left"><span class="level-item">导入 MapReduce 的相关依赖</span></span></a></li><li><a class="level is-mobile" href="#Wordcount-实例"><span class="level-left"><span class="level-item">Wordcount 实例</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Wordcount-整体流程"><span class="level-left"><span class="level-item">Wordcount 整体流程</span></span></a></li><li><a class="level is-mobile" href="#WordCount-代码实现"><span class="level-left"><span class="level-item">WordCount 代码实现</span></span></a></li></ul></li><li><a class="level is-mobile" href="#统计访问次数"><span class="level-left"><span class="level-item">统计访问次数</span></span></a></li><li><a class="level is-mobile" href="#统计每年最高气温"><span class="level-left"><span class="level-item">统计每年最高气温</span></span></a></li><li><a class="level is-mobile" href="#数据去重"><span class="level-left"><span class="level-item">数据去重</span></span></a></li><li><a class="level is-mobile" href="#日志分析：分析非结构化文件"><span class="level-left"><span class="level-item">日志分析：分析非结构化文件</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Linux-%E4%BD%BF%E7%94%A8/"><span class="level-start"><span class="level-item">Linux 使用</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"><span class="level-start"><span class="level-item">大数据</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">学习笔记</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%B0%E5%BD%95/"><span class="level-start"><span class="level-item">记录</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-05T06:15:06.000Z">2021-01-05</time></p><p class="title"><a href="/2021/acme-sh/">使用 acme.sh 自动签发 ZeroSSL 的 ECC 证书</a></p><p class="categories"><a href="/categories/%E8%AE%B0%E5%BD%95/">记录</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-21T11:38:58.000Z">2020-12-21</time></p><p class="title"><a href="/2020/FixPD16/">解决 Parallels Desktop 16 by TNT 网络初始化失败的问题</a></p><p class="categories"><a href="/categories/%E8%AE%B0%E5%BD%95/">记录</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-01T13:49:44.000Z">2020-12-01</time></p><p class="title"><a href="/2020/docker-maven/">使用 dockerfile-maven-plugin 插件构建并推送 Docker 镜像</a></p><p class="categories"><a href="/categories/%E8%AE%B0%E5%BD%95/">记录</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-08-16T08:05:01.000Z">2020-08-16</time></p><p class="title"><a href="/2020/Java-%E9%9D%A2%E8%AF%95/">一些容易忘掉的知识点 - Java</a></p><p class="categories"><a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-07-13T11:34:37.000Z">2020-07-13</time></p><p class="title"><a href="/2020/%E6%90%AD%E5%BB%BADNS%E6%9C%8D%E5%8A%A1%E5%99%A8/">使用 Docker 自建支持 DoH、DoT 的 DNS 服务器</a></p><p class="categories"><a href="/categories/%E8%AE%B0%E5%BD%95/">记录</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">morooi&#039;s Blog</a><p class="is-size-7"><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p><p class="is-size-7"><span>&copy; 2022 SJ Zhou</span>  <a target="_blank" href="http://www.beian.miit.gov.cn/">鄂 ICP 备 20002590 号</a><br>Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">本网站由 <a href="https://www.aliyun.com/product/cdn" target="_blank"><img src="/img/aliyun_logo.png" style="height:15px;display:inline"></a>、<a href="https://www.cloudflare.com/cdn" target="_blank"><img src="/img/cloudflare_logo.svg" style="height:15px;display:inline"></a>、<a href="https://www.upyun.com/?utm_source=lianmeng&amp;utm_medium=referral" target="_blank"><img src="/img/upyun_logo.png" style="height:15px;display:inline"></a> 提供 CDN 加速服务</p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>