{"posts":[{"title":"解决 Parallels Desktop 16 by TNT 网络初始化失败的问题","text":"Parallels Desktop 16 “歪门邪道版” 在 macOS Big Sur 下面启动会提示：网络初始化失败（您的虚拟机将继续正常运作，但将无法连接网络。） 目前网络上的方法都是用 sudo 启动 prl_client_app 进程，但是效果并不好，而且 USB 不可以使用。 这里通过修改文件参数的方式，立即生效，完美解决！ 本文使用环境如下： macOS Big Sur 11.1 Parallels Desktop 16.1.1-49141 by TNT 解决网络初始化失败问题关闭 PD16，使用 sudo 编辑文件 /Library/Preferences/Parallels/network.desktop.xml 1sudo vim /Library/Preferences/Parallels/network.desktop.xml 更改 UseKextless 项（我的环境中在 5 行）， -1 改为 0 /Library/Preferences/Parallels/network.desktop.xml12- &lt;UseKextless&gt;-1&lt;/UseKextless&gt;+ &lt;UseKextless&gt;0&lt;/UseKextless&gt; 保存并退出，启动 PD16 立即生效！ 解决 USB 无法使用的问题关闭 PD16，使用 sudo 编辑文件 /Library/Preferences/Parallels/dispatcher.desktop.xml 1sudo vim /Library/Preferences/Parallels/dispatcher.desktop.xml 更改 Usb 项（我的环境中在 367 行），0 改为 1 /Library/Preferences/Parallels/dispatcher.desktop.xml12- &lt;Usb&gt;0&lt;/Usb&gt;+ &lt;Usb&gt;1&lt;/Usb&gt; 保存并退出，启动 PD16 立即生效！","link":"/2020/FixPD16/"},{"title":"Conda 常用命令的整理","text":"Anaconda 是一个针对数据分析领域 Python 发行版本，它提供了包管理（packages）工具和虚拟环境（environment）管理， conda 命令可用于安装、卸载、更新包、创建不同版本的 Python 独立环境，可用于替换 pip 和 virtualenv 这两个工具。 文中记录了使用 conda 的常用命令。 可直接从 Anaconda 官方网站 https://www.anaconda.com/download/ 进行下载，选择 Python 3.7 的版本。 也可以从清华镜像 https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ 下载。 MinicondaMiniconda 是一个 Anaconda 的轻量级替代，默认只包含了 python 和 conda，但是可以通过 pip 和 conda 来安装所需要的包。 Miniconda 安装包可以到 https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/ 下载。 或从官方文档处 https://docs.conda.io/en/latest/miniconda.html 下载。 也可以使用包管理工具安装，如 apt-get, yum, brew cask 等 安装好 Conda 后，如使用 zsh，输入： 123export PATH=\"/usr/local/miniconda3/bin:$PATH\" # 注意修改成自己的安装路径conda init zsh # 或其他 shellconda config --set auto_activate_base false # 关闭自动激活环境 使用清华的 Anaconda 仓库镜像运行以下命令 123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes 即可添加 Anaconda Python 免费仓库。 运行 conda install numpy 测试一下吧。 清华的 Anaconda 镜像使用帮助：https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/ 常用 conda 命令1234567891011121314151617# 检查 conda 版本conda --version# 升级当前版本的 condaconda update conda# 查看已安装的包conda list# 包更新conda update $package# 删除包conda remove $package# 取消自动激活conda环境conda config --set auto_activate_base false 环境管理123456789101112131415161718192021# 创建环境，$envname 换成新建的环境名，'python='后可以指定版本conda create --name $envname python=3.7# 创建环境并指定路径conda create --prefix=$dirpath python=3.7# 激活环境activate $envnameconda activate $envname # Linux or macOS# 退出当前环境deactivate $envname# 列出所有环境conda info -e# 删除环境conda remove -n $envname --all# 删除指定路径的环境conda remove --prefix=$dirpath --all 复制 Conda 环境在本地的 conda 里已经有一个 AAA 的环境，我想创建一个新环境跟它一模一样的叫 BBB，那么这样一句就搞定了： 1234conda create -n BBB --clone AAAconda create -p $dirpath_new --clone $dirpath_old","link":"/2019/anaconda/"},{"title":"一些容易忘掉的知识点 - Java","text":"1 Java 基础1.1 快速失败（fail-fast）和安全失败（fail-safe）机制快速失败（fail-fast）是 Java 集合的一种错误检测机制。 在使用迭代器对集合进行遍历的时候，在多线程下操作非安全失败（fail-safe）的集合类可能就会触发 fail-fast 机制，导致抛出 ConcurrentModificationException 异常。 在单线程下，如果在遍历过程中对集合对象的内容进行了修改的话也会触发 fail-fast 机制。 例如：多线程下，如果线程 1 正在对集合进行遍历，此时线程 2 对集合进行修改（增加、删除、修改），或者线程 1 在遍历过程中对集合进行修改，都会导致线程 1 抛出 ConcurrentModificationException 异常。 原理 迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变 modCount 的值。 每当迭代器使用 hashNext ()/next () 遍历下一个元素之前，都会检测 modCount 变量是否为 expectedmodCount 值，是的话就返回遍历；否则抛出异常，终止遍历。 采用安全失败（fail-safe）机制的集合容器（JUC 包下的容器），在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历，可以在多线程下并发使用，并发修改。所以，在遍历过程中对原集合所作的修改并不能被迭代器检测到，故不会抛 ConcurrentModificationException 异常。 1.2 HashMap 与 ConcurrentHashMap 对 null 值的处理问题结论：HashMap 的 key 和 value 都可以为 null，ConcurrentHashMap 的均不可以为 null HashTable 也是不允许 key 和 value 为空的，原因和 ConcurrentHashMap 一致 上源码！ HashMap，当 key 为 null 的时候，hash 值为 0，后续的 put 方法中也没有判断 value 是否为 null 的代码。 1234static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);} ConcurrentHashMap，key 或 value 为 null 直接抛出空指针异常！ 123final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); ...... 这么做的原因： 会有一个问题，当通过 get(k) 获取对应的 value 时，如果获取到的是 null，无法判断它是 put(k,v) 的时候 value 为 null，还是这个 key 从来没有做过映射。 HashMap 多用于非并发场景中，可以通过 contains(key) 来做这个判断。而在多用于并发场景中的 ConcurrentHashMap 中调用 m.contains(key) 判断然后再 m.get(key) 时，\u001dm 可能已经被其他线程修改而不同了。为了避免这种二义性，ConcurrentHashMap 在设计之初就直接令 key 和 value 都不为 null。 1.3 Arrays.asList()参考：数组转集合 《阿里巴巴 Java 开发手册》对 Arrays.asList() 的描述： 注意 1 传递的数组必须是对象数组，而不是基本类型。 因为 Arrays.asList() 是泛型方法，传入的对象必须是对象数组。对基本类型来说，可以使用包装类型数组可以解决这个问题。 1Integer[] myArray = {1, 2, 3}; 注意 2 使用集合的修改方法: add()、remove()、clear() 会抛出异常。 Arrays.asList() 方法返回的并不是 java.util.ArrayList ，而是 java.util.Arrays 的一个内部类，这个内部类并没有实现集合的修改方法或者说并没有重写这些方法。 1234List myList = Arrays.asList(1, 2, 3);myList.add(4); // 运行时报错：UnsupportedOperationExceptionmyList.remove(1); // 运行时报错：UnsupportedOperationExceptionmyList.clear(); // 运行时报错：UnsupportedOperationException 1.4 Java 中用到的排序算法Arrays.sort() 在该方法的源码中调用了 DualPivotQuicksort.sort()，发现在此方法中有如下判断： 123456789static void sort(int[] a, int left, int right, int[] work, int workBase, int workLen) { // Use Quicksort on small arrays if (right - left &lt; QUICKSORT_THRESHOLD) { sort(a, left, right, true); return; } ... 可以发现如果数组的长度小于 QUICKSORT_THRESHOLD 的话就会使用双轴快速排序（DualPivotQuicksort），这个值为 286。 在双轴快速排序中，有如下判断。如果数组长度小于 INSERTION_SORT_THRESHOLD == 47，则使用插入排序。 1234// Use insertion sort on tiny arraysif (length &lt; INSERTION_SORT_THRESHOLD) { ...} 如果数组长度不小于 286，会继续检查该数组的连续升序和连续降序性好不好 (Check if the array is nearly sorted)，如果好的话就用归并排序，不好就用双轴快速排序。 总结：Arrays.sort() 方法，如果数组长度小于 286 且大于等于 47 的话就用双轴快速排序，如果长度小于 47 的话就用插入排序。如果数组长度大于等于 286 且连续性好的话，就用 TimSort 归并排序（归并排序的优化版本），如果大于等于 286 且连续性不好的话就用双轴快速排序。 数组长度 l 排序方法 l &lt; 47 插入排序 47 &lt;= l &lt; 286 双轴快速排序 286 &lt;= l （连续性好） TimSort 归并排序 286 &lt;= l （连续性不好） 双轴快速排序 Collections.sort() 针对集合类容器排序，首先调用 this.toArray() 方法将集合元素放进 Object 数组，使用 Array.sort() 的重载方法对该 Object 数组排序。如果用户指定 LegacyMergeSort.userRequested == true，则使用归并排序，否则使用 TimSort 排序 1.5 BIO、NIO同步和异步： 同步：两个同步任务相互依赖，并且一个任务必须以依赖于另一任务的某种方式执行。 比如在 A-&gt;B 事件模型中，你需要先完成 A 才能执行 B。 再换句话说，同步调用中被调用者未处理完请求之前，调用不返回，调用者会一直等待结果的返回。 异步：两个异步的任务完全独立的，一方的执行不需要等待另外一方的执行。再换句话说，异步调用种一调用就返回结果不需要等待结果返回，当结果返回的时候通过回调函数或者其他方式拿着结果再做相关事情， 阻塞和非阻塞： 阻塞：阻塞就是发起一个请求，调用者一直等待请求结果返回，也就是当前线程会被挂起，无法从事其他任务，只有当条件就绪才能继续。 非阻塞：非阻塞就是发起一个请求，调用者不用一直等着结果返回，可以先去干其他事情。 在程序里： 同步阻塞，相当于一个线程在等待。 同步非阻塞，相当于一个线程在正常运行。 异步阻塞，相当于多个线程都在等待。 异步非阻塞，相当于多个线程都在正常运行。 BIO：同步阻塞 IO (Blocking I/O) 同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，可以使用线程池机制改善。 BIO 方式适用于连接数比较小且固定的架构 NIO：同步非阻塞 IO（New I/O, Non-blocking I/O） 同步非阻塞，服务器实现为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器（Selector）上，Selector 轮询到连接有 IO 请求时才启动一个线程进行处理 NIO 适用于连接数目多且比较短的架构，比如聊天服务器，编程复杂，JDK1.4 才开始支持 为什么 NIO 是同步非阻塞的？ 因为无论多少客户端都可以接入服务端，客户端接入并不会耗费一个线程，只会创建一个连接然后注册到 Selector 上去，这样就可以去干其他想干的其他事情了。一个 Selector 线程不断的轮询所有的 Socket 连接（Channel），发现有事件了就通知，然后就启动一个线程处理一个请求即可，这个过程的话就是非阻塞的。但是这个处理的过程中，还是要先读取数据，处理，再返回的，这是个同步的过程。 多路复用机制是如何支持海量连接？ NIO 的线程模型对 Socket 发起的连接不需要每个都创建一个线程，使用一个 Selector 来多路复用监听 N 多个 Channel 是否有请求，该请求是对应的连接请求，还是发送数据的请求。 一个 Selector 不断的轮询多个 Channel，这样避免了创建多个线程。只有当某个 Channel 有对应的请求的时候才会创建线程，可能说 1000 个请求中只有 100 个请求是有数据交互的，这个时候可能 Server 端就提供 10 个线程就能够处理这些请求。这样的话就可以避免了创建大量的线程。 选择器 Selector 使用单个线程处理多个通道。因此，它需要较少的线程来处理这些通道。 NIO 中的 Buffer 是什么？ IO 面向流（Stream oriented），NIO 面向缓冲区（Buffer oriented） 通过 NIO 写数据到文件或者网络，或者是从文件和网络读取数据出来，此时就需要通过 Buffer 缓冲区来进行。 在 NIO 库中，所有数据都是用缓冲区处理的。在读取数据时，读到缓冲区中；在写入数据时，写入到缓冲区中。任何时候访问 NIO 中的数据，都是通过缓冲区进行操作。 每一种 Java 基本类型（除了 Boolean 类型）都对应有一种缓冲区。 NIO 中的 Channel 是什么？ Channel（通道）是 NIO 中的数据通道，NIO 通过 Channel 进行读写。类似流，但是又有些不同 通道是双向的，既可从 Channel 中读取数据，又可以写数据到 Channel 中。流的读写通常是单向的 Channel 中的数据总是要先读到一个 Buffer 中，或者从 Buffer 中将数据写到 Channel 中。因为 Buffer，Channel 可以异步的读写 1.6 HashMap 的 put () 方法 为什么要重写 hashCode 方法？ 保证相同的对象返回相同的 hash 值，不同的对象返回不同的 hash 值。和重写 equals 类似。 hash 算法 1234static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);} h 右移 16 位，向下传播较高位的影响，异或是合并这些影响。应为在小表中可能永远用不到高位信息。 1.7 HashMap 的 resize () 方法 为什么链表大小超过 8 个时会自动转化为红黑树，当删除小于 6 时重新变为链表？ 根据泊松分布，在负载因子默认为 0.75 的时候，单个 hash 槽内元素个数为 8 的概率小于百万分之一，所以将 7 作为一个分水岭，等于 7 的时候不转换，大于等于 8 的时候才进行转换，小于等于 6 的时候就化为链表。 loadFactor 为什么默认为 0.75？ loadFactor 加载因子是控制数组存放数据的疏密程度，loadFactor 越趋近于 1，那么数组中存放的数据（entry）也就越多、越密，会让链表的长度增加；loadFactor 越小，也就是趋近于 0，数组中存放的数据（entry）也就越少，也就越稀疏。 loadFactor 太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor 的默认值为 0.75f 是官方给出的一个比较好的临界值。 1.8 HashMap 获取所有的 Key遍历获取 HashMap 中所有的 value 时，使用遍历 EntrySet 的方法比先取 KeySet 再取值快。 1234567HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;();Set&lt;String&gt; keySet = map.keySet(); // 获取所有的 keySet&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = map.entrySet(); // 获取所有的 entryfor (Map.Entry&lt;String, String&gt; entry : entrySet) { String key = entry.getKey(); // 单个 entry 的 key String value = entry.getValue(); // 单个 entry 的 value} 1.9 ConcurrentHashMapConcurrentHashMap &amp; Hashtable JDK 1.8 中：采用了 自旋 + CAS + synchronized 来保证并发安全性。跟 HashMap 很像，把 Node 中的 value 和 next 使用 volatile 修饰，保证了可见性，并且也引入了红黑树，在链表大于一定值的时候会转换（默认是 8）。 ConcurrentHashMap 的 put 操作大致可以分为以下步骤： 根据 key 计算出 hashcode 判断是否需要进行初始化，若需要，执行 initTable() 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功 如果当前位置的 hashcode == MOVED == -1, 则需要进行扩容 helpTransfer() 如果都不满足，则利用 synchronized 锁写入数据 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树 ConcurrentHashMap 的初始化 initTable() 是通过自旋和 CAS 操作完成的。里面需要注意的是变量 sizeCtl ，它的值决定着当前的初始化状态。 -1 说明正在初始化 -N 说明有 N-1 个线程正在进行扩容 表示 table 初始化大小，如果 table 没有初始化 表示 table 容量，如果 table 已经初始化。 1.10 内部类有什么作用 封装性 作为一个类的编写者，很显然需要对这个类的使用访问者的访问权限做出一定的限制，将一些我们不愿意让别人看到的操作隐藏起来，如果我们的内部类不想轻易被任何人访问，可以选择使用 private 修饰内部类（对于普通类是不能用的），这样我们就无法通过创建对象的方法来访问，想要访问只需要在外部类中定义一个 public 修饰的方法，间接调用。 123456789101112131415public interface Demo { void show();}class Outer { private class test implements Demo { public void show() { System.out.println(\"密码备份文件\"); } } public Demo getInner() { return new test(); }} 实现多继承 Java 是不可以实现多继承的，一次只能继承一个类，可以用接口来实现多继承的效果，即一个接口有多个实现，但是这里也是有一点弊端的，那就是，一旦实现一个接口就必须实现里面的所有方法，有时候就会出现一些累赘，但是使用内部类可以很好的解决这些问题。 编写两个待继承的类 Demo1 和 Demo2，在 MyDemo 类中书写了两个内部类，test1 和 test2 两者分别继承了 Demo1 和 Demo2 类，这样 MyDemo 中就间接的实现了多继承。 12345678910111213141516171819202122232425262728293031323334353637383940 public class Demo1 { public String name() { return \"BWH_Steven\"; } } public class Demo2 { public String email() { return \"xxx.@163.com\"; } } public class MyDemo { private class test1 extends Demo1 { public String name() { return super.name(); } } private class test2 extends Demo2 { public String email() { return super.email(); } } public String name() { return new test1().name(); } public String email() { return new test2().email(); } public static void main(String args[]) { MyDemo md = new MyDemo(); System.out.println(\"我的姓名:\" + md.name()); System.out.println(\"我的邮箱:\" + md.email()); }} 1.11 创建对象的几种方式 使用 new 关键字（最常见） 使用 Class 类的 newInstance() 方法（反射） 只能调用无参构造函数，不能调用私有的构造函数 123User user = (User)Class.forName(\"cn.morooi.User\").newInstance();// 或User user = User.class.newInstance(); 使用 Constructor 类的 newInstance() 方法（反射） 12Constructor constructor = Class.forName(\"cn.morooi.User\").getConstructor();User user = (User)constructor.newInstance() 使用 clone() 方法（不会调用任何构造函数） 反序列化 1.12 双亲委派机制类加载器详解 在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。加载的时候，首先会把该请求委派该父类加载器的 loadClass() 处理，因此所有的请求最终都应该传送到顶层的启动类加载器 BootstrapClassLoader 中。当父类加载器无法处理时，才由自己来处理。当父类加载器为 null 时，会使用启动类加载器 BootstrapClassLoader 作为父类加载器。 好处： 双亲委派模型保证了 Java 程序的稳定运行，可以避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类），也保证了 Java 的核心 API 不被篡改。 如果没有使用双亲委派模型，而是每个类加载器加载自己的话就会出现一些问题，比如我们编写一个称为 java.lang.Object 类的话，那么程序运行的时候，系统就会出现多个不同的 Object 类。 三个 Classloader： BootstrapClassLoader（启动类加载器）：最顶层的加载类，由 C++ 实现，负责加载 %JAVA_HOME%/lib 目录下的 jar 包和类或者或被 -Xbootclasspath 参数指定的路径中的所有类。 ExtensionClassLoader（扩展类加载器）：主要负责加载目录 %JRE_HOME%/lib/ext 目录下的 jar 包和类，或被 java.ext.dirs 系统变量所指定的路径下的 jar 包。 AppClassLoader（应用程序类加载器）：面向用户的加载器，负责加载当前应用 classpath 下的所有 jar 包和类。 1.13 实现只读集合Collections.unmodifiable...() 可以实现只读集合 1234567891011121314151617181920import java.util.*; public class Main { public static void main(String[] argv) throws Exception { List stuff = Arrays.asList(new String[] { \"a\", \"b\" }); List list = new ArrayList(stuff); list = Collections.unmodifiableList(list); try { list.set(0, \"new value\"); } catch (UnsupportedOperationException e) { } Set set = new HashSet(stuff); set = Collections.unmodifiableSet(set); Map map = new HashMap(); map = Collections.unmodifiableMap(map); System.out.println(\"集合现在是只读\"); }} 1.14 接口和抽象类的区别 接口方法的默认修饰符是 public，在 JDK 8 以前所有方法在接口中不能有实现。JDK 8 中可以在接口中定义默认方法和静态方法。静态方法直接用接口名调用，实现类和实现是不可以调用的。如果同时实现两个接口，接口中定义了一样的默认方法，则必须重写。 抽象方法可以有 public、protected 和 default 这些修饰符，也可以有非抽象的方法。 一个类可以实现多个接口，但只能实现一个抽象类。接口自己本身可以通过 extends 关键字扩展多个接口。 接口中的变量只能用 static final 修饰，不能有其他变量，而抽象类中则不一定。 从设计层面来说，抽象是对类的抽象，是一种模板设计，而接口是对行为的抽象，是一种行为的规范。 JDK 9 在接口中引入了私有方法和私有静态方法。 1.15 类的生命周期、类加载过程类加载过程详解 Java 类加载机制 Java 类加载过程通常分为以下三个阶段： 1. 加载（Loading） 在加载阶段，类加载器负责查找类的字节码文件并将其加载到内存中。类加载器根据类的全名（包括包路径）来查找对应的 .class 文件。 找到类文件后，将其读取到内存中，并创建一个表示该类的 Class 对象。 类加载器可以是系统类加载器（Bootstrap ClassLoader）、扩展类加载器（Extension ClassLoader）或应用程序类加载器（Application ClassLoader） 2. 链接（Linking） 链接阶段进一步分为三个步骤： 验证（Verification）：Java 虚拟机执行验证步骤以确保类文件的完整性和合法性。这包括检查字节码文件的格式、语法和语义，以防止安全漏洞和运行时错误。 准备（Preparation）：Java 虚拟机为类的静态变量分配内存空间，并设置默认初始值，例如数字类型的默认值是 0，对象引用的默认值是 null。这些变量的内存分配在类加载阶段进行，而不是在类初始化阶段。 解析（Resolution）：解析是可选的步骤，根据需要执行。在解析阶段，虚拟机将符号引用（如类、方法、字段的引用）转换为直接引用，以便在后续的类初始化和运行中快速访问类和其成员。 3. 初始化（Initialization） 初始化是类加载的最后一个阶段。在初始化阶段，Java 虚拟机执行类的初始化代码，这包括执行静态初始化块和静态字段初始化。初始化阶段确保在使用类之前，类的状态已经准备好。 类的初始化是一个延迟操作，只有在首次使用类时才会触发。 类初始化的时机包括： 创建类的实例（对象初始化时）。 调用类的静态方法。 访问类的静态字段（除 final 常量外）。 初始化类的子类。 Java 的类加载机制确保了类的懒加载（只有在需要时才加载）和类初始化的顺序。每个类只会被加载和初始化一次，除非它被卸载（通常不会发生）。这个机制有助于提高性能和节省内存。 1.16 几个容器的声明123456789101112131415161718192021222324252627282930// ArrayListpublic class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable {...} // LinkedListpublic class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable {...}// HashMappublic class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable {...}// ConcurrentHashMappublic class ConcurrentHashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements ConcurrentMap&lt;K,V&gt;, Serializable {...}// HashTablepublic class Hashtable&lt;K,V&gt; extends Dictionary&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, java.io.Serializable {...}// CopyOnWriteArrayListpublic class CopyOnWriteArrayList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable {...}// HashSetpublic class HashSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable {...} 1.17 StringBuilder 和 StringBuffer 的区别String、StringBuffer、StringBuilder 的区别？ StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 抽象类，在 AbstractStringBuilder 中也是使用字符数组保存字符串 char[]value，但是没有用 final 关键字修饰，所以这两种对象都是可变的。 StringBuilder 与 StringBuffer 的方法都是调用父类的方法实现的，区别在于 StringBuffer 的方法都使用 synchronized 关键字修饰保证线程安全。 1.18 错误与异常Java 中所有的异常都继承于 java.lang.Throwable 类。Throwable 有两个重要的子类：Exception（异常） 和 Error（错误）。 错误 Error：是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java 虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止。 异常 Exception：是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 异常由 Java 虚拟机抛出。NullPointerException（要访问的变量没有引用任何对象时，抛出该异常）、ArithmeticException（算术运算异常，一个整数除以 0 时，抛出该异常）和 ArrayIndexOutOfBoundsException（下标越界异常） 异常和错误的区别：异常能被程序本身处理，错误无法处理 1.19 String 类的一些用法 新建一个 String 对象 1234567String s1 = \"s\";String s2 = new String(\"s\"); // s2 == \"s\"char[] chars = {'a', 'b', 'c', 'd'};String s3 = new String(chars); // s3 == \"abcd\"String s4 = new String(chars, 1, 2); // s4 == \"bc\"byte[] bytes = new byte[]{65, 66};String s5 = new String(bytes); // s5 == \"AB\" 常用方法 1234567891011121314151617181920212223242526272829303132333435363738// 提取子串String s = \"abcdef\";String s1 = s.substring(s, 2); // s1 == \"cdef\"String s2 = s.substring(s, 2, 4); // s2 == \"cd\"，左闭右开区间// 字符串连接String s = \"aa\".concat(\"bb\").concat(\"cc\"); // s == \"aabbcc\", 相当于 \"aa\" + \"bb\" + \"cc\"// 查找参数字符串在本字符串中首次出现的索引位置，没有返回 -1String str = \"I am a good student\";int a = str.indexOf('a'); // a == 2int b = str.indexOf(\"good\"); // b == 7int c = str.indexOf(\"w\", 2); // c == -1，该方法从第二个参数位置向后查找int d = str.lastIndexOf(\"a\"); // d == 5，该方法从字符串的末尾位置向前查找int e = str.lastIndexOf(\"a\", 3); // e == 2，该方法从第二个参数位置向前查找// 转换public char[] toCharArray(); // 将当前字符串拆分成为字符数组作为返回值public byte[] getBytes(); // 获取当前字符串底层的字节数组public String toLowerCase(); //返回将当前字符串中所有字符转换成小写后的新串public String toUpperCase(); //返回将当前字符串中所有字符转换成大写后的新串// 替换// 将所有出现的老字符串替换成为新的字符串，返回替换后的结果新字符串public String replace(CharSequence oldString, CharSequence newString);// 用字符 replacement 的内容替换当前字符串中遇到的第一个和字符串 regex 相匹配的子串public String replaceFirst(String regex, String replacement);// 用字符 replacement 的内容替换当前字符串中遇到的所有和字符串regex相匹配的子串public String replaceAll(String regex, String replacement);// 去掉空格public String trim(); // 去掉首尾的空格，中间的不处理str.replace(\" \", \"\"); // 使用 replace() 方法去掉所有空格，包括首尾、中间str.replaceAll(\" \", \"\"); // 使用 replaceAll() 方法去掉所有空格，包括首尾、中间str.replaceAll(\"\\\\s*\", \"\"); // 使用 replaceAll() 方法去掉所有空白字符，包括空格、制表符、换页符等// 分割public String[] split(String regex); // 按照参数规则将字符串切分成若干部分，返回 String 数组 字符串与基本类型的转换 1234567891011121314151617// 基本类型转字符串，使用 String 类中的 valueOf() 方法public static String valueOf(char data[])public static String valueOf(char data[], int offset, int count)public static String valueOf(boolean b)public static String valueOf(char c)public static String valueOf(int i)public static String valueOf(long l)public static String valueOf(float f)public static String valueOf(double d)// 字符串转基本类型，使用基本类型的包装类中的 parseXxx() 方法public static byte parseByte(String s)public static short parseShort(String s)public static int parseInt(String s)public static long parseLong(String s)public static float parseFloat(String s)public static double parseDouble(String s) 1.20 线程安全的容器 同步容器：HashTable, Vector 并发容器：ConcurrentHashMap, CopyOnWriteArrayList, CopyOnWriteArraySet 队列：ConcurrentLinkedQueue, ConcurrentLinkedDeque 阻塞队列：ArrayBlockingQueue, LinkedBlockingQueue, PriorityBlockingQueue, DelayQueue, SynchronousQueue 1.21 HashMap 为什么线程不安全HashMap 不是线程安全的主要原因是多个线程可以同时修改 HashMap 中的键值对，导致不一致性。具体来说，当两个线程同时尝试在 HashMap 中添加或删除键值对时，可能会发生以下情况： 两个线程同时调用 put 方法并且插入相同的键，这会导致其中一个线程的插入操作被覆盖，并且可能会导致该键值对丢失。 当一个线程正在调用 put 方法添加键值对时，另一个线程同时调用 get 方法来获取相同的键值对，这可能会导致 get 方法返回旧值，而不是最新的值。 当一个线程正在调用 put 方法添加键值对时，另一个线程同时调用 remove 方法来删除相同的键值对，这可能会导致删除失败或者删除了错误的键值对。 1.22 HashMap 为什么扩容总是 2 的幂次HashMap 的扩容总是 2 的幂次方，是因为这样可以在进行哈希映射计算时，用位运算的方式代替除法运算，从而提高运算速度。 具体来说，在 HashMap 中，对于一个 key，它的哈希值会先通过以下方式转换为在数组中的索引位置：index = (hash &amp; (capacity - 1)) 其中，**hash** 是 key 的哈希值，**capacity** 是数组的容量，**&amp;** 是按位与运算符。这种转换方式可以保证 index 的值不会超过数组的大小，而且可以均匀地分布在数组中。 如果数组的大小不是 2 的幂次，那么 (capacity - 1) 得到的值的二进制表示中会有一些 0 后面跟着一些 1，这样做位运算时就会多次执行操作。而如果数组大小是 2 的幂次，**(capacity - 1)** 得到的值的二进制表示中所有的位都是 1，这样做位运算时只需要执行一次操作就可以了。 此外，由于扩容时需要将所有的元素重新计算索引位置并放到新数组中，如果数组大小是 2 的幂次，那么元素的索引位置计算也会更加高效。这是因为，如果数组大小是 2 的幂次，那么元素在新数组中的索引位置要么不变，要么变为原索引位置加上旧数组大小。这种计算方式只需要用位运算来实现，非常高效。 综上所述，HashMap 的扩容总是 2 的幂次方，可以提高运算速度并且使得索引位置的计算更加高效。 1.23 HashMap 为什么要用红黑树 不用其他树在 HashMap 的实现中，当链表长度达到一定阈值时，会将链表转换为红黑树。这是因为红黑树相对于其他树，有以下优势： 查询、插入和删除的时间复杂度为 O (log n)，与平衡树的性质有关，相对于链表的线性复杂度，性能更高。 红黑树相对于其他平衡树来说，实现比较简单，而且旋转操作次数较少，因此更适合在 HashMap 中使用。 红黑树是一种自平衡二叉搜索树，能够保证树的高度始终在 O (log n) 的范围内，从而保证了插入、删除和查询的时间复杂度都能够达到 O (log n)。 相对于其他平衡树如 AVL 树，红黑树的旋转操作较少，实现较为简单，而且能够保证插入、删除和查询的时间复杂度也能够达到 O (log n)，因此更适合在 HashMap 中使用。 1.24 HashMap 为什么到 8 的时候变为红黑树当链表长度较小时，使用链表能够较为高效地进行查找、插入和删除操作，而且实现比较简单。但是当链表长度较大时，链表的性能会逐渐变差，因为查找、插入和删除操作的时间复杂度为 O (n)。 为了解决这个问题，Java 8 引入了红黑树来替代链表，因为红黑树能够保证插入、删除和查找的时间复杂度为 O (log n)，相对于链表而言具有更好的性能表现。而 8 这个阈值是通过经验和实验得到的一个比较优秀的值，能够在实际使用中达到较好的性能表现。 需要注意的是，当红黑树中节点数目小于等于 6 时，HashMap 又会将红黑树转换为链表（在 resize 的时候），这是为了避免红黑树的过度复杂化和占用过多的内存空间。 1.25 符号引用和直接引用1. 符号引用（Symbolic Reference） 符号引用是在编译阶段生成的，它用符号来表示要引用的类、方法、字段等元素，而不包含具体的内存地址或偏移量信息。 符号引用提供了元素的名称、类型以及所在类的引用，但它不包含具体的内存地址。这使得编译器和链接器可以在后续的处理阶段（如类加载阶段）进行解析，并将符号引用映射到具体的内存地址或偏移量。 符号引用的使用使得 Java 虚拟机能够支持动态链接和类加载，因为它们可以在运行时动态解析为直接引用。 2. 直接引用（Direct Reference） 直接引用是在虚拟机执行时实际使用的引用，它包含了元素的内存地址或偏移量信息，可以直接访问该元素。 直接引用通常在符号引用解析之后生成，以便虚拟机能够高效地访问类、方法、字段等元素。 直接引用的使用使得虚拟机能够快速访问内存中的数据，而不需要重新计算或查找。 总结来说，符号引用是编程语言在编译阶段使用的抽象引用，不包含具体的内存地址信息，而直接引用是在执行阶段使用的实际引用，包含了元素的内存地址或偏移量信息，以便能够直接访问数据。在类加载和链接过程中，符号引用会被解析为直接引用，以便程序可以正确地执行。这种分离使得 Java 虚拟机能够支持动态链接和类加载，并且在执行时具有高效的内存访问。 2 多线程2.1 sleep ()、wait ()、join () 的区别 sleep() 是线程线程类（Thread）的方法，调用会暂停此线程指定的时间，但监控依然保持，不会释放对象锁，到时间自动恢复。 wait() 是 Object 的方法，并且只能在同步方法或同步块中使用，如果当前线程不是锁的持有者，该方法抛出一个 IllegalMonitorStateException 异常。调用会放弃对象锁，进入等待队列，待调用 notify() / notifyAll() 唤醒指定的线程或者所有线程后才会进入锁池，再次获得对象锁才会进入运行状态。 join() 是线程线程类（Thread）的方法，调用 join() 方法不会释放锁，调用线程会一直等待被调用的线程执行完毕（转换为 TERMINATED 状态） sleep() 方法可以在任何地方使用，并且必须要指定时间，wait() 方法则只能在同步方法或同步块中使用。 2.2 线程的生命周期和状态Java 线程在运行的生命周期中的指定时刻只可能处于下面 6 种不同状态的其中一个状态。 线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换。 2.3 锁升级见：synchronize 与锁 锁的升级流程 每一个线程在准备获取共享资源时： 检查 MarkWord 里面是不是放的自己的 ThreadId , 如果是，表示当前线程是处于 “偏向锁” 如果 MarkWord 不是自己的 ThreadId，锁升级，这时候，用 CAS 来执行切换，新的线程根据 MarkWord 里面现有的 ThreadId，通知之前线程暂停，之前线程将 Markword 的内容置为空 两个线程都把锁对象的 HashCode 复制到自己新建的用于存储锁的记录空间，接着开始通过 CAS 操作， 把锁对象的 MarKword 的内容修改为自己新建的记录空间的地址的方式竞争 MarkWord 第三步中成功执行 CAS 的获得资源，失败的则进入自旋 自旋的线程在自旋过程中，成功获得资源 (即之前获的资源的线程执行完成并释放了共享资源)，则整个状态依然处于 轻量级锁的状态，如果自旋失败 进入重量级锁的状态，这个时候，自旋的线程进行阻塞，等待之前线程执行完成并唤醒自己 总的来说，先使用偏向锁优先同一线程然后再次获取锁，如果失败，就升级为 CAS 轻量级锁，如果失败就会短暂自旋，防止线程被系统挂起。最后如果以上都失败就升级为重量级锁。 各种锁的优缺点对比 锁 优点 缺点 适用场景 偏向锁 加锁和解锁不需要额外的消耗，和执行非同步方法比仅存在纳秒级的差距。 如果线程间存在锁竞争，会带来额外的锁撤销的消耗。 适用于只有一个线程访问同步块场景。 轻量级锁 竞争的线程不会阻塞，提高了程序的响应速度。 如果始终得不到锁竞争的线程使用自旋会消耗 CPU。 追求响应时间。同步块执行速度非常快。 重量级锁 线程竞争不使用自旋，不会消耗 CPU。 线程阻塞，响应时间缓慢。 追求吞吐量。同步块执行时间较长。 2.4 线程为什么比进程快进程和线程的区别 进程是一个独立的运行环境，而线程是在进程中执行的一个任务。他们两个本质的区别是是否单独占有内存地址空间及其它系统资源（比如 I/O）： 进程单独占有一定的内存地址空间，所以进程间存在内存隔离，数据是分开的，数据共享复杂但是同步简单，各个进程之间互不干扰；而线程共享所属进程占有的内存地址空间和资源，数据共享简单，但是同步复杂。 进程单独占有一定的内存地址空间，一个进程出现问题不会影响其他进程，不影响主程序的稳定性，可靠性高；一个线程崩溃可能影响整个程序的稳定性，可靠性较低。 进程单独占有一定的内存地址空间，进程的创建和销毁不仅需要保存寄存器和栈信息，还需要资源的分配回收以及页调度，开销较大；线程只需要保存寄存器和栈信息，开销较小。 另外一个重要区别是，进程是操作系统进行资源分配的基本单位，而线程是操作系统进行调度的基本单位，即 CPU 分配时间的单位 。 2.5 使用多线程的好处多进程的方式也可以实现并发，为什么我们要使用多线程？ 多进程方式确实可以实现并发，但使用多线程，有以下几个好处： 进程间的通信比较复杂，而线程间的通信比较简单，通常情况下，我们需要使用共享资源，这些资源在线程间的通信比较容易。 进程是重量级的，而线程是轻量级的，故多线程方式的系统开销更小。 2.6 使用多线程可能会带来的问题内存泄漏 死锁 线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。 死锁发生的条件： 互斥条件：资源 x 的任意一个时刻只被一个线程持有 占有且等待：线程 1 占有资源 x 的同时等待资源 y ，并不释放 x 不可抢占：资源 x 一旦被线程 1 占有，其他线程不能抢占 x 循环等待：线程 1 持有 x，等待 y，线程 2 持有 y，等待 x，当全部满足时才会死锁 避免死锁： 破坏互斥条件 ：这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。 破坏占有且等待 ：一次性申请所有的资源。 破坏不可抢占 ：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待 ：靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。 使用 tryLock 方法（ReentrantLock、ReentrantReadWriteLock），设置超时时间，超时可以退出防止死锁。 线程不安全 2.7 synchronized 与 Lock 的区别 类别 synchronized Lock 存在层次 Java 的关键字，在 JVM 层面上 是一个类 锁的释放 1. 已获取锁的线程执行完同步代码，释放锁2. 线程执行发生异常，JVM 会让线程释放锁 必须在 finally 中释放锁，不然容易造成线程死锁 锁的获取 假设 A 线程获得锁，B 线程等待如果 A 线程阻塞，B 线程会一直等待 分情况而定，Lock 有多个锁获取的方式大致就是可以尝试获得锁，线程可以不用一直等待 锁状态 无法判断 可以判断 锁类型 可重入、不可中断、非公平 可重入、可判断、可公平（两者皆可） 性能 少量同步 大量同步 synchronized 有什么不足之处？ 如果临界区是只读操作，其实可以多线程一起执行，但使用 synchronized 的话，同一时间只能有一个线程执行。 synchronized 无法知道线程有没有成功获取到锁 使用 synchronized，如果临界区因为 IO 或者 sleep 方法等原因阻塞了，而当前线程又没有释放锁，就会导致所有线程等待。 Lock 接口有什么优势？ 可以使锁更公平 可以使线程在等待锁的时候响应中断 可以让线程尝试获取锁，并在无法获取锁的时候立即返回或者等待一段时间 可以在不同的范围，以不同的顺序获取和释放锁 整体上来说 Lock 是 synchronized 的扩展版，Lock 提供了无条件的、可轮询的（tryLock 方法）、定时的（tryLock 带参方法）、可中断的（lockInterruptibly）、可多条件队列的（newCondition 方法）锁操作。另外 Lock 的实现类基本都支持非公平锁（默认）和公平锁，synchronized 只支持非公平锁，当然，在大部分情况下，非公平锁是高效的选择。 ReentrantLock ReentrantLock 是 Lock 接口的 JDK 默认实现，特点： 可重入锁 支持公平锁和非公平锁，默认为非公平锁 在 ReentrantLock 的构造方法里，可以传入一个 boolean 类型的参数，来指定它是否是一个公平锁，默认情况下是非公平的。这个参数一旦实例化后就不能修改，只能通过 isFair() 方法来查看。 是排他锁，独占不能共享 ReentrantReadWriteLock ReentrantReadWriteLock 是 ReadWriteLock 接口的 JDK 默认实现，特点： 可重入锁 支持公平锁和非公平锁，默认为非公平锁 支持读写锁，在 “写” 操作的时候，其它线程不能写也不能读，称这种现象为 “写饥饿”。 StampedLock StampedLock 把读分为了悲观读和乐观读，悲观读就等价于 ReadWriteLock 的读，而乐观读在一个线程写共享变量时，不会被阻塞，乐观读是不加锁的。所以没锁肯定是比有锁的性能好，这样的话在大并发读情况下效率就更高了。StampedLock 在获取锁和乐观读时，都会返回一个 stamp，解锁时需要传入这个 stamp，在乐观读时是用来验证共享变量是否被其他线程写过。 2.8 线程池来自：线程池原理 使用线程池主要有以下三个原因： 创建 / 销毁线程需要消耗系统资源，线程池可以复用已创建的线程。 控制并发的数量。并发数量过多，可能会导致资源消耗过多，从而造成服务器崩溃。（主要原因） 可以对线程做统一管理。 Java 中的线程池顶层接口是 Executor 接口，ThreadPoolExecutor 是这个接口的实现类。 四种常见的线程池 Executors 类中提供的几个静态方法来创建线程池。 newFixedThreadPool 12345public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());} 核心线程数量和总线程数量相等，都是传入的参数 nThreads，所以只能创建核心线程，不能创建非核心线程。 newSingleThreadExecutor 123456public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));} 有且仅有一个核心线程（corePoolSize == maximumPoolSize == 1），使用了 LinkedBlockingQueue（容量很大），所以，不会创建非核心线程。所有任务按照先来先执行的顺序执行。如果这个唯一的线程不空闲，那么新来的任务会存储在任务队列里等待执行。 newCachedThreadPool 12345public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());} 不创建核心线程，线程池最大为 Integer.MAX_VALUE。尝试将任务添加到 SynchronousQueue 队列。如果 SynchronousQueue 入列成功，等待被当前运行的线程空闲后拉取执行。如果当前没有空闲线程，那么就创建一个非核心线程，然后从 SynchronousQueue 拉取任务并在当前线程执行。如果 SynchronousQueue 已有任务在等待，入列操作将会阻塞。 newScheduledThreadPool 123456789public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize);}// class ScheduledThreadPoolExecutorpublic ScheduledThreadPoolExecutor(int corePoolSize) { super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue());} 创建一个定长线程池，支持定时及周期性任务执行。 线程池的参数 int corePoolSize：该线程池中核心线程数最大值 int maximumPoolSize：该线程池中线程总数最大值。(等于核心线程数量 + 非核心线程数量) long keepAliveTime：非核心线程闲置超时时长。 非核心线程如果处于闲置状态超过该值，就会被销毁。 如果设置 allowCoreThreadTimeOut(true)，则会也作用于核心线程。 TimeUnit unit：keepAliveTime 的单位。 BlockingQueue workQueue：阻塞队列，维护着等待执行的 Runnable 任务对象。 ThreadFactory threadFactory：创建线程的工厂 ，用于批量创建线程。 RejectedExecutionHandler handler：拒绝处理策略，线程数量大于最大线程数就会采用拒绝处理策略。 核心线程：线程池中有两类线程，核心线程和非核心线程。核心线程默认情况下会一直存在于线程池中，即使这个核心线程什么都不干（铁饭碗），而非核心线程如果长时间的闲置，就会被销毁（临时工）。 常用的几个阻塞队列： LinkedBlockingQueue 链式阻塞队列，底层数据结构是链表，默认大小是 Integer.MAX_VALUE，也可以指定大小。 ArrayBlockingQueue 数组阻塞队列，底层数据结构是数组，需要指定队列的大小。 SynchronousQueue 同步队列，内部容量为 0，每个 put 操作必须等待一个 take 操作，反之亦然。 DelayQueue 延迟队列，该队列中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素。 四种拒绝处理策略： ThreadPoolExecutor.AbortPolicy：默认拒绝处理策略，丢弃任务并抛出 RejectedExecutionException 异常。 ThreadPoolExecutor.DiscardPolicy：丢弃新来的任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列头部（最旧的）的任务，然后重新尝试执行程序（如果再次失败，重复此过程）。 ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务。 线程池主要的任务处理流程： 线程总数量 &lt; corePoolSize，无论线程是否空闲，都会新建一个核心线程执行任务（让核心线程数量快速达到 corePoolSize，在核心线程数量 &lt; corePoolSize 时）。注意，这一步需要获得全局锁。 线程总数量 &gt;= corePoolSize 时，新来的线程任务会进入任务队列中等待，然后空闲的核心线程会依次去缓存队列中取任务来执行（体现了线程复用）。 当缓存队列满了，说明这个时候任务已经多到爆棚，需要一些 “临时工” 来执行这些任务了。于是会创建非核心线程去执行这个任务。注意，这一步需要获得全局锁。 缓存队列满了， 且总线程数达到了 maximumPoolSize，则会采取上面提到的拒绝策略进行处理。 2.9 ThreadLocalJava 并发 - ThreadLocal 详解 ThreadLocal 详解 ThreadLocal 到底会不会内存泄漏？实战直接告诉你答案！ 证明：ThreadLocal 的 get,set 方法无法防止内存泄漏 ThreadLocal 面试场景 2.10 volatilevolatile 详解 volatile 为什么可以保证内存可见性？ volatile 是在汇编层面加 Lock，使用缓存一致性协议（MESI）解决并发可见性的。 为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存 (L1，L2 或其他) 后再进行操作，但操作完不知道何时会写到内存。 如果对声明了 volatile 的变量进行写操作，JVM 就会向处理器发送一条 lock 前缀的指令，将这个变量所在缓存行的数据写回到系统内存。 为了保证各个处理器的缓存是一致的，实现了缓存一致性协议（MESI），每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。 所有多核处理器下还会完成：当处理器发现本地缓存失效后，就会从内存中重读该变量数据，即可以获取当前最新值。 volatile 变量通过这样的机制就使得每个线程都能获得该变量的最新值。 volatile 是怎么禁止指令重排序的？ volatile 关键字通过内存屏障（Memory Barrier）来禁止指令重排序。 在每个 volatile 写操作的前面插入一个 StoreStore 屏障。 在每个 volatile 写操作的后面插入一个 StoreLoad 屏障。 在每个 volatile 读操作的后面插入一个 LoadLoad 屏障。 在每个 volatile 读操作的后面插入一个 LoadStore 屏障。 通过插入这些屏障，volatile 变量的读写操作的顺序在编译器和处理器中都得到了限制，保证了它们的顺序与代码中的顺序一致。这样就避免了在多线程环境下由于指令重排而导致的读取脏数据的问题。 3 JVM3.1 CMS 收集器来自：JVM 垃圾回收详解、G1 收集器 CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。 CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 从名字中的 Mark Sweep 这两个词可以看出，CMS 收集器是一种 “标记 - 清除” 算法实现的，整个过程分为四个步骤： 初始标记：暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快。 并发标记：同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记：重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短。 并发清除：开启用户线程，同时 GC 线程开始对未标记的区域做清扫。 主要优点：并发收集、低停顿 主要缺点： 对 CPU 资源敏感 无法处理浮动垃圾 它使用的 “标记 - 清除” 算法会导致收集结束时会有大量空间碎片产生 3.2 G1 收集器来自：JVM 垃圾回收详解、G1 收集器 G1 (Garbage-First) 是一款面向服务器的垃圾收集器，主要针对配备多颗处理器及大容量内存的机器。以极高概率满足 GC 停顿时间要求的同时，还具备高吞吐量性能特征. 被视为 JDK1.7 中 HotSpot 虚拟机的一个重要进化特征。它具备以下特点： 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 空间整合：与 CMS 的 “标记 - 清理” 算法不同，G1 从整体来看是基于 “标记 - 整理” 算法实现的收集器，从局部上来看是基于 “复制” 算法实现的。 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。 G1 之前的 JVM 内存模型 G1 收集器的内存模型 G1 堆内存结构堆内存会被切分成为很多个固定大小区域（Region），每个是连续范围的虚拟内存。 堆内存中一个区域（Region）的大小可以通过 -XX:G1HeapRegionSize 参数指定，大小区间最小 1M、最大 32M，总之是 2 的幂次方。 默认把堆内存按照 2048 份均分。 G1 堆内存分配每个 Region 被标记了 E、S、O 和 H，这些区域在逻辑上被映射为 Eden，Survivor 和老年代。 存活的对象从一个区域转移（即复制或移动）到另一个区域。区域被设计为并行收集垃圾，可能会暂停所有应用线程。 如上图所示，区域可以分配到 Eden，survivor 和老年代。此外，还有第四种类型，被称为巨型区域（Humongous Region）。Humongous 区域是为了那些存储超过 50% 标准 region 大小的对象而设计的，它用来专门存放巨型对象。如果一个 H 区装不下一个巨型对象，那么 G1 会寻找连续的 H 分区来存储。为了能找到连续的 H 区，有时候不得不启动 Full GC。 G1 回收流程G1 收集器的运作大致分为以下几个步骤（类似于 CMS 收集器）： 初始标记：这个阶段是 STW (Stop the World) 的，所有应用线程会被暂停，标记出从 GC Root 开始直接可达的对象。 并发标记：从 GC Roots 开始对堆中对象进行可达性分析，找出存活对象，耗时较长。当并发标记完成后，开始最终标记（Final Marking）阶段 最终标记：标记那些在并发标记阶段发生变化的对象，将被回收。 筛选回收：首先对各个 Regin 的回收价值和成本进行排序，根据用户所期待的 GC 停顿时间指定回收计划，回收一部分 Region。 G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region (这也就是它的名字 Garbage-First 的由来)。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。 3.3 Java 内存中各部分的内容参考：Java 内存区域详解、可能是把 Java 内存区域讲的最清楚的一篇文章 线程共享的 堆 Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 方法区 各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 运行时常量池JDK1.7 前在方法区中，1.7 及之后在 Java 堆中的一块区域存放运行时常量池，用于存放编译期生成的各种字面量和符号引用。 JDK1.7 之前运行时常量池逻辑包含字符串常量池存放在方法区，此时 hotspot 虚拟机对方法区的实现为永久代 JDK1.7 字符串常量池被从方法区拿到了堆中，这里没有提到运行时常量池，也就是说字符串常量池被单独拿到堆，运行时常量池剩下的东西还在方法区，也就是 hotspot 中的永久代 JDK1.8 hotspot 移除了永久代用元空间 (Metaspace) 取而代之，这时候字符串常量池还在堆，运行时常量池还在方法区，只不过方法区的实现从永久代变成了元空间 (Metaspace) 线程私有的 程序计数器 唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 虚拟机栈 由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息 局部变量表主要存放了编译器可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。 本地方法栈 和虚拟机栈所发挥的作用非常相似，虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，本地方法栈则为虚拟机使用到的 Native 方法服务。在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 3.4 永久代会有垃圾回收吗永久代也是可以回收的，条件是 该类的实例都被回收 加载该类的 classLoader 已经被回收 该类不能通过反射访问到其方法，而且该类的 java.lang.class 没有被引用 当满足这 3 个条件时，是可以回收，但回不回收还得看 JVM。 3.5 JDK 8 的默认垃圾回收器使用 java -XX:+PrintCommandLineFlags -version 可以查看 123456java -XX:+PrintCommandLineFlags -version-XX:InitialHeapSize=268435456 -XX:MaxHeapSize=4294967296 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGCjava version \"1.8.0_231\"Java(TM) SE Runtime Environment (build 1.8.0_231-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.231-b11, mixed mode) 其中 UseParallelGC 表示使用 Parallel Scavenge + Parallel Old 垃圾收集器。 3.6 JVM 思维导图 4 数据库4.1 MyISAM 与 InnoDB 的区别MyISAM 和 InnoDB 有什么区别？ MySQL 5.5 版之前 MyISAM 是默认数据库引擎，5.5 版本后默认的存储引擎为 InnoDB。 两者的对比： 1. 是否支持行级锁 MyISAM 只有表级锁 (table-level locking)，而 InnoDB 支持行级锁 (row-level locking) 和表级锁，默认为行级锁 2. 是否支持事务 MyISAM 不提供事务支持。InnoDB 提供事务支持，实现了 SQL 标准定义了四个隔离级别，具有提交 (commit) 和回滚 (rollback) 事务的能力。并且，InnoDB 默认使用的 REPEATABLE-READ（可重读）隔离级别是可以解决幻读问题发生的（基于 MVCC 和 Next-Key Lock） 3. 是否支持外键 MyISAM 不支持，InnoDB 支持。 4. 是否支持数据库异常崩溃后的安全恢复 MyISAM 不支持，InnoDB 支持。使用 InnoDB 的数据库在异常崩溃后，数据库重新启动的时候会保证数据库恢复到崩溃前的状态。这个恢复的过程依赖于 redo log。 5. 是否支持 MVCC 仅 InnoDB 支持。应对高并发事务 MVCC 比单纯的加锁更高效，MVCC 只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作，MVCC 可以使用乐观锁和悲观锁来实现。 总结： InnoDB 支持行级别的锁粒度，MyISAM 不支持，只支持表级别的锁粒度 MyISAM 不提供事务支持，InnoDB 提供事务支持，实现了 SQL 标准定义了四个隔离级别 MyISAM 不支持外键，InnoDB 支持 MyISAM 不支持 MVVC，InnoDB 支持 虽然 MyISAM 引擎和 InnoDB 引擎都是使用 B+Tree 作为索引结构，但是两者的实现方式不太一样 MyISAM 不支持数据库异常崩溃后的安全恢复，InnoDB 支持 InnoDB 的性能比 MyISAM 更强大。 4.2 数据库事务MySQL 事务隔离级别详解 Spring 事务详解 Spring 事务传播行为详解 4.3 数据库索引MySQL 索引详解 《爱上面试官》系列 - 数据库索引 索引的使用 MySQL 的索引是怎么加速查询的？ 合适建索引的字段： 不为 NULL 的字段 被频繁查询的字段 被作为条件查询的字段 被经常频繁用于连接的字段 经常需要排序的列 不合适创建索引的字段： 被频繁更新的字段 不被经常查询的字段 特大型表的话维护开销会很大，不适合建索引 其他注意： 注意遵守最左前缀原则 尽可能的考虑建立联合索引而不是单列索引 注意避免冗余索引 考虑在字符串类型的字段上使用前缀索引代替普通索引 4.4 SQL 语句的执行过程见：SQL 语句在 MySQL 中的执行过程 4.5 MySQL 的锁机制见：MySQL 锁机制简单了解一下 4.6 MVCC 多版本并发控制MVCC 和事务隔离级别的关系 MySQL-InnoDB-MVCC 多版本并发控制 4.7 delete , truncate, drop 删除的区别特点 delete： 删除指定数据：delete from test where username = 'abc' 删除整个表：delete * from test 或 delete from test，仅删除表内的所有内容，保留表的定义，不释放空间，可以回滚恢复。 drop：drop 表名，删除表，并释放空间，将表删除的一干二净。 truncate：truncate 表名，删除表里的内容，并释放空间，但表结构及其列、约束、索引等保持不变。 是否可以恢复？ delete 语句是数据库操作语言（DML），这个操作会放到 rollback segement 中，事务提交之后才生效，可以回滚。 truncate 和 drop 是数据库定义语言（DDL），操作立即生效，原数据不放到 rollback segment 中，不能回滚。 执行速度：drop &gt; truncate &gt; delete 4.8 内连接、左连接、右连接 内连接（inner join），交集 12345678select *from a_table a inner join b_table b on a.a_id = b.b_id;# 或省去 innerselect *from a_table a join b_table b on a.a_id = b.b_id; 以上语句等价于 1234select *from a_table a, b_table bwhere a.a_id = b.b_id; 左外连接（左连接，left outer join），差集 12345678select * from a_table a left outer join b_table b on a.a_id = b.b_id;# 或省去 outerselect * from a_table a left join b_table b on a.a_id = b.b_id; 左连接，左表（a_table）的记录将会全部表示出来，而右表（b_table）只会显示符合搜索条件的记录，右表记录不足的地方均为 NULL。 右外连接（右连接，right outer join），差集 12345678select * from a_table a right outer join b_table b on a.a_id = b.b_id;# 或省去 outerselect * from a_table a right join b_table b on a.a_id = b.b_id; 与左连接相反，右连接，左表（a_table）只会显示符合搜索条件的记录，而右表（b_table）的记录将会全部表示出来。左表记录不足的地方均为 NULL。 右连接可以与左连接等价互换 合并（union），并集 123456789101112131415161718/*union 联合 合并: 将多条查询语句的结果合并成一个结果语法： 查询语句1 union 查询语句2 union ...*/SELECT *FROM employeesWHERE email LIKE '%a%'UNIONSELECT *FROM employeesWHERE department_id &gt; 90; union 特点： 要求多条查询语句的查询列数是一致的 要求多条查询语句的查询的每一列的类型和顺序最好一致 union 关键字默认去重，如果使用 union all 可以包含重复项 用于要查询的结果来自于多个表，且多个表没有直接的连接关系，但查询的信息一致时 4.9 MySQL 中的常用函数123456789101112131415161718192021222324252627282930313233343536373839404142434445# 聚合函数COUNT(col) # 统计查询结果的行数MIN(col) # 查询指定列的最小值MAX(col) # 查询指定列的最大值SUM(col) # 求和，返回指定列的总和AVG(col) # 求平均值，返回指定列数据的平均值# 数值函数ABS(x) &nbsp;# 返回x的绝对值BIN(x) &nbsp;# 返回x的二进制CEILING(x) &nbsp;# 返回大于x的最小整数值EXP(x) &nbsp;# 返回值e（自然对数的底）的x次方FLOOR(x) &nbsp;# 返回小于x的最大整数值GREATEST(x1,x2,...,xn) &nbsp;# 返回集合中最大的值LEAST(x1,x2,...,xn) &nbsp;# 返回集合中最小的值LN(x) &nbsp;# 返回x的自然对数LOG(x,y) &nbsp;# 返回x的以y为底的对数MOD(x,y) &nbsp;# 返回x/y的模（余数）PI() &nbsp;# 返回pi的值（圆周率）RAND() &nbsp;# 返回０到１内的随机值,可以通过提供一个参数(种子)使RAND()随机数生成器生成一个指定的值ROUND(x,y) &nbsp;# 返回参数x的四舍五入的有y位小数的值TRUNCATE(x,y) &nbsp;# 返回数字x截短为y位小数的结果# 字符串函数LENGTH(s) &nbsp;# 计算字符串长度函数，返回字符串的字节长度CONCAT(s1, s2..., sn) &nbsp;# 合并字符串函数，返回结果为连接参数产生的字符串，参数可以是一个或多个INSERT(str, x, y, instr) &nbsp;# 将字符串str从第x位置开始，y个字符长的子串替换为字符串instr，返回结果LOWER(str) &nbsp;# 将字符串中的字母转换为小写UPPER(str) &nbsp;# 将字符串中的字母转换为大写LEFT(str, x) &nbsp;# 返回字符串str中最左边的x个字符RIGHT(str, x) &nbsp;# 返回字符串str中最右边的x个字符TRIM(str) &nbsp;# 删除字符串左右两侧的空格REPLACE(s，s1，s2) &nbsp;# 使用字符串 s2 替换字符串 s 中所有的字符串 s1SUBSTRING(s，n，len) &nbsp;# 从字符串 s 返回一个长度同 len 字符相同的子字符串，起始于位置 nREVERSE(str) &nbsp;# 返回颠倒字符串str的结果# 时间函数CURDATE() / CURRENT_DATE() # 两个函数作用相同，返回当前系统的日期值CURTIME() / CURRENT_TIME() # 两个函数作用相同，返回当前系统的时间值NOW() / SYSDATE() # 两个函数作用相同，返回当前系统的日期和时间值UNIX_TIMESTAMP() # 获取 UNIX 时间戳TIMETOSEC() # 将时间参数转换为秒数SECTOTIME() # 将秒数转换为时间ADDTIME() # 时间加法运算，在原始时间上添加指定的时间SUBTIME() # 时间减法运算，在原始时间上减去指定的时间 5 分布式5.1 分布式架构分布式系统的经典基础理论 三歪非要听我说完分布式才肯睡 5.2 分布式事务小米 - 分布式事务，这一篇就够了 敖丙 - 分布式事务 两阶段提交 / XA TCC (Try-Confirm-Cancel) 本地消息表 可靠消息最终一致性 (消息事务) 5.3 Spring Cloud 技术图 6 Redis缓存雪崩、穿透，缓存与数据库双写一致 妈妈再也不担心我面试被 Redis 问得脸都绿了 6.1 数据库的双写一致性问题面试前必须要知道的 Redis 面试题 先更新数据库，后删除缓存（多用） 正常的情况是这样的： 先操作数据库，成功； 再删除缓存，也成功； 如果原子性被破坏了： 第一步成功（操作数据库），第二步失败（删除缓存），会导致数据库里是新数据，而缓存里是旧数据。 如果第一步（操作数据库）就失败了，我们可以直接返回错误（Exception），不会出现数据不一致。 如果在高并发的场景下，出现数据库与缓存数据不一致的概率特别低，也不是没有： 缓存刚好失效 线程 A 查询数据库，得一个旧值 线程 B 将新值写入数据库 线程 B 删除缓存 线程 A 将查到的旧值写入缓存 要达成上述情况，还是说一句概率特别低： 因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。 删除缓存失败的解决思路： 将需要删除的 key 发送到消息队列中 自己消费消息，获得需要删除的 key 不断重试删除操作，直到成功 先删除缓存，后更新数据库 正常情况是这样的： 先删除缓存，成功； 再更新数据库，也成功； 如果原子性被破坏了： 第一步成功（删除缓存），第二步失败（更新数据库），数据库和缓存的数据还是一致的。 如果第一步（删除缓存）就失败了，我们可以直接返回错误（Exception），数据库和缓存的数据还是一致的。 看起来是很美好，但是我们在并发场景下分析一下，就知道还是有问题的了： 线程 A 删除了缓存 线程 B 查询，发现缓存已不存在 线程 B 去数据库查询得到旧值 线程 B 将旧值写入缓存 线程 A 将新值写入数据库 所以也会导致数据库和缓存不一致的问题。 并发下解决数据库与缓存不一致的思路： 将删除缓存、修改数据库、读取缓存等的操作积压到队列里边，实现串行化。 两种策略各自有优缺点： 先删除缓存，再更新数据库 在高并发下表现不如意，在原子性被破坏时表现优异 先更新数据库，再删除缓存（Cache Aside Pattern 设计模式） 在高并发下表现优异，在原子性被破坏时表现不如意 6.2 RDB 与 AOF持久化：RDB 和 AOF 机制详解 Redis 持久化机制详解 RDB 优点 只有一个文件 dump.rdb，方便持久化。 容灾性好，一个文件可以保存到安全的磁盘。 性能最大化，fork 子进程来完成写操作，让主进程继续处理命令，所以使 IO 最大化。使用单独子进程来进行持久化，主进程不会进行任何 IO 操作，保证了 Redis 的高性能 相对于数据集大时，比 AOF 的 启动效率 更高。 RDB 缺点 数据安全性低。RDB 是间隔一段时间进行持久化，如果持久化之间 Redis 发生故障，会发生数据丢失。所以这种方式更适合数据要求不严谨的时候。 AOF 优点 数据安全，aof 持久化可以配置 appendfsync 属性，有 always，每进行一次命令操作就记录到 aof 文件中一次。 通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof 工具解决数据一致性问题。 AOF 机制的 rewrite 模式。AOF 文件没被 rewrite 之前（文件过大时会对命令 进行合并重写），可以删除其中的某些命令（比如误操作的 flushall） AOF 缺点 AOF 文件比 RDB 文件大，且恢复速度慢。 数据集大的时候，比 RDB 启动效率低。 Redis 的数据恢复有着如下的优先级 如果只配置 AOF ，重启时加载 AOF 文件恢复数据； 如果同时配置了 RDB 和 AOF ，启动只加载 AOF 文件恢复数据； 如果只配置 RDB，启动将加载 dump 文件恢复数据。 有 AOF 就用 AOF，没有就 RDB 6.3 数据结构Redis 数据类型 数据类型：5 种基础数据类型详解 数据结构：底层数据结构详解 6.4 Redis 集群高可拓展：分片技术（Redis Cluster）详解 史上最强「集群」入门实践教程 6.5 Redis 过期 key 的删除策略 定时删除：在设置键的过期时间的同时，创建一个定时器，过期时间到达时，由定时器任务立即执行对应键的删除操作。 惰性删除：放任键过期不管，但是每次获取键时，都检查取得的键是否过期，如果过期的话，就删除该键；如果没有过期，就返回该键。 定期删除：周期性轮训 Redis 库中的时效性数据，采用随机抽取的策略，利用过期数据占比的方式控制删除频度。 删除策略 内存占用 CPU 占用 特点 定时删除 节约内存，无占用 不分时段占用 CPU 资源，频度高 用时间换空间 惰性删除 内存占用严重 延时执行，CPU 利用率高 用空间换时间 定期删除 内存定期随机清理 每秒花费固定的 CPU 资源维护内存 随机抽查，重点抽查 6.6 内存淘汰机制 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的） volatile-lfu：当内存不足以容纳新写入数据时，在过期密集的键中，使用 LFU 算法进行删除 key。 allkeys-lfu：当内存不足以容纳新写入数据时，使用 LFU 算法移除所有的 key。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期的键中，随机删除一个 key。 allkeys-random：当内存不足以容纳新写入数据时，随机删除一个或者多个 key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。 noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。 6.7 scan 命令 1 亿个 key，其中有 10 万个 key 是以某个固定的已知的前缀开头的，如果将它们全部找出来？ 使用 keys 指令可以扫出指定模式的 key 列表。但是它有两个主要的缺点： 没有 offset、limit 参数，一次性吐出所有满足条件的 key keys 算法是遍历算法，复杂度是 O (n)，如果实例中有千万级以上的 key，这个指令就会导致 Redis 服务卡顿，所有读写 Redis 的其它的指令都会被延后甚至会超时报错，因为 Redis 是单线程程序，顺序执行所有指令，其它指令必须等到当前的 keys 指令执行完了才可以继续。 scan 相比 keys 具备有以下特点: 复杂度虽然也是 O (n)，但是它是通过游标分步进行的，不会阻塞线程 提供 limit 参数，可以控制每次返回结果的最大条数 同 keys 一样，它也提供模式匹配功能 服务器不需要为游标保存状态，游标的唯一状态就是 scan 返回给客户端的游标整数 返回的结果可能会有重复，需要客户端去重复，这点非常重要 遍历的过程中如果有数据修改，改动后的数据能不能遍历到是不确定的 单次返回的结果是空的并不意味着遍历结束，而要看返回的游标值是否为 0 scan 指令可以无阻塞的提取出指定模式的 key 列表，但是会有一定的重复概率，在客户端做一次去重就可以了 ，但是整体所花费的时间会比直接用 keys 指令长。 6.8 异步队列和延迟队列异步队列 Redis 通过 list 数据结构来实现消息队列，主要使用到如下命令： lpush 和 rpush 入队列 lpop 和 rpop 出队列 blpop 和 brpop 阻塞式出队列 当队列里面没有消息时，使用 sleep 阻塞一段时间，但是 sleep 会导致消息的处理延迟增加。这个问题我们可以通过 blpop / brpop 来阻塞式读取队列。 blpop / brpop 在队列没有数据的时候，会立即进入休眠状态，一旦数据到来，则立刻醒过来。消息的延迟几乎为零。 延迟队列 如下场景： 订单下单后超过一小时用户未支付，需要关闭订单 订单的评论如果 7 天未评价，系统需要自动产生一条评论 可以使用延时队列，顾名思义就是需要延迟一段时间后执行。Redis 可通过 zset 实现。想要执行时间的时间戳作为 score，消息内容作为 key 调用 zadd 来生产消息，消费者用 zrangebyscore 指令获取 N 秒之前的数据轮询进行处理。 6.9 Redis 线程模型Redis 线程模型 为什么 Redis 选择单线程模型 美团二面：Redis 究竟是单线程还是多线程？ 7 Spring 全家桶7.1 Spring 中依赖注入的方式浅谈 Spring 为什么推荐使用构造器注入 常见的三种注入方式: field 注入 构造器注入（Spring 推荐） setter 注入 构造器注入的好处： 保证依赖不可变（final 关键字） 保证依赖不为空（省去了我们对其检查） 保证返回客户端（调用）的代码的时候是完全初始化的状态 避免了循环依赖（如果有循环依赖会主动提醒） 提升了代码的可复用性 7.2 Spring 如何解决循环依赖Spring 中的循环依赖及解决 7.3 SpringBoot 的启动流程面试官：说一下 Spring Boot 的启动过程吧 SpringBoot 的启动流程是怎样的？ processOn 创建 SpringApplication 对象 确定应用类型（Servlet、Reactive、None） 设置初始化器（Initializers） 设置监听器（Listeners） 执行 SpringApplication 对象的 run 方法 获取监听器，发送 ApplicationStartingEvent 事件 准备环境 (prepareEnvironment)，创建 StandardServletEnvironment 对象，发送 ApplicationEnvironmentPreparedEvent 事件 创建 IOC 容器 ApplicationContext (createApplicationContext)，类名为 AnnotationConfigServletWebServerApplicationContext 准备容器 (prepareContext)，调用初始化器，发送事件 ApplicationPreparedEvent，加载资源，发送事件 ApplicationPreparedEvent 刷新容器 (refreshContext) – 重要 获取 beanFactory (obtainFreshBeanFactory)，为 DefaultListableBeanFactory 准备 beanFactory (prepareBeanFactory) 调用 beanFactory 后置处理 (invokeBeanFactoryPostProcessors)，先处理 BeanDefinitionRegistryPostProcessor 相关的，调用 postProcessBeanDefinitionRegistry 处理 Bean 的定义（在 Bean 定义注册之前）；后处理 BeanFactoryPostProcessor 相关的，调用 postProcessBeanFactory（在 Bean 定义之后、实例化之前） 注册 Bean 的后置处理器 (registerBeanPostProcessors)，创建了 BeanPostProcessor 类型的对象 初始化 MessageSource 组件 (initMessageSource) 初始化 ApplicationEventMulticaster (initApplicationEventMulticaster) 初始化特殊的 Bean (onRefresh)，执行 ServletWebServerApplicationContext#onRefresh 启动 tomcat 注册监听器 (registerListeners) 初始化剩下的普通单例 (finishBeanFactoryInitialization) 容器刷新完成 (finishRefresh) 发送 ApplicationStartedEvent 事件 回调 Runner 方法 (callRunners)，ApplicationRunner、CommandLineRunner 返回容器 SpringApplication 的创建 SpringApplication 的启动 SpringBoot 完整启动过程 7.4 过滤器和拦截器有什么区别什么是过滤器（Filter）？ 与 Servlet 相似，过滤器是一些 web 应用程序组件，可以绑定到一个 web 应用程序中。但是与其他 web 应用组件不同的是，过滤器是 “链” 在容器的处理过程中的。这就意味着它们可以在请求达到 Servlet 之前对其进行访问，也可以在响应信息返回到客户端之前对其进行拦截。这种访问使得过滤器可以检查并修改请求和响应的内容。 什么是拦截器（Interceptor）？ 拦截器是 AOP 的一种实现策略，用于在某个方法或字段被访问前对它进行拦截，然后在其之前或之后加上某些操作。同 Filter 一样，Interceptor 也是链式调用。每个 Interceptor 的调用会依据它的声明顺序依次执行。一般来说拦截器可以用于以下方面 ： 日志记录 ：记录请求信息的日志，以便进行信息监控、信息统计等 权限检查 ：对用户的访问权限，认证，或授权等进行检查 性能监控 ：通过拦截器在进入处理器前后分别记录开始时间和结束时间，从而得到请求的处理时间 通用行为 ：读取 cookie 得到用户信息并将用户对象放入请求头中，从而方便后续流程使用 区别？ 两者最大的区别在于：过滤器是在 Servlet 规范中定义的，是由 Servlet 容器支持的，只能用于过滤请求；拦截器是在 Spring 容器内的，由 Spring 框架支持。 过滤器作用于请求到达 Servlet 之前，在 Spring 中也就是在 DispacherServlet 之前。 拦截器最早只能作用于请求到达 Servlet 之后。 拦截器作为 Spring 的一个组件，可以通过 IOC 容器进行管理，获取其中的各个 bean 实例，对 Spring 中的各种资源、对象，如 Service 对象、数据源、事务管理等进行调用；而过滤器则不能。 总的来说，两者主要在如下方面存在着差异 ： 过滤器是基于函数的回调，而拦截器是基于 Java 反射机制的 过滤器可以修改 Request，而拦截器则不能 过滤器需要在 Servlet 容器中实现，拦截器可以适用于 JavaEE、JavaSE 等各种环境 拦截器可以调用 IOC 容器中的各种依赖，而过滤器不能 过滤器只能在请求的前后使用，而拦截器可以详细到每个方法 Spring 中的过滤器与拦截器 7.5 SpringBoot 自动配置原理SpringBoot 自动装配原理详解 7.6 Spring IoC 容器中 Bean 的生命周期如何记忆 Spring Bean 的生命周期 Bean 的生命周期概括起来就是 4 个阶段： 实例化（Instantiation） 属性赋值（Populate） 初始化（Initialization） 销毁（Destruction） 实例化：第 1 步，实例化一个 bean 对象； 属性赋值：第 2 步，为 bean 设置相关属性和依赖； 初始化：第 3~7 步，步骤较多，其中第 5、6 步为初始化操作，第 3、4 步为在初始化前执行，第 7 步在初始化后执行，该阶段结束，才能被用户使用； 销毁：第 8~10 步，第 8 步不是真正意义上的销毁（还没使用呢），而是先在使用前注册了销毁的相关调用接口，为了后面第 9、10 步真正销毁 bean 时再执行相应的方法。 7.7 SpringMVC 的工作流程SpringMVC 执行流程及工作原理 8 计算机网络8.1 HTTP 版本HTTP/2 HTTP/1.1 和 HTTP/2.0 有什么区别 HTTP/2.0 和 HTTP/3.0 有什么区别 8.2 HTTPSHTTP 和 HTTPS 有什么区别 HTTP vs HTTPS HTTPs 8.3 HTTP 方法HTTP 方法 8.4 HTTP 的头HTTP 首部 8.5 HTTP 状态码HTTP 状态码 8.6 TCP 与 UDPTCP 与 UDP TCP 三次握手和四次挥手 8.7 输入 URL 到页面加载过程从输入 URL 到页面展示到底发生了什么 输入 URL 到页面加载过程详解 9 Linux9.1 常用命令查看剩余内存：free -h 显示目前所有文件系统的可用空间及使用情形：df -h 查看某个路径下所有文件大小：du -h --max-depth=1 . chown chmod mount tail cat 9.2 修改执行权限通过 ls -lah 命令可以查看某个目录下的文件或目录的权限。 第一列的内容的信息解释如下： 文件的类型： d： 代表目录 -： 代表文件 l： 代表软链接（可以认为是 window 中的快捷方式） Linux 中权限分为以下几种 r：代表权限是可读，r 也可以用数字 4 表示 w：代表权限是可写，w 也可以用数字 2 表示 x：代表权限是可执行，x 也可以用数字 1 表示 文件和目录权限的区别： 对文件和目录而言，读写执行表示不同的意义。 对于文件： 权限名称 可执行操作 r 可以使用 cat 查看文件的内容 w 可以修改文件的内容 x 可以将其运行为二进制文件 对于目录： 权限名称 可执行操作 r 可以查看目录下列表 w 可以创建和删除目录下文件 x 可以使用 cd 进入目录 需要注意的是： 超级用户可以无视普通用户的权限，即使文件目录权限是 000，依旧可以访问。 修改文件 / 目录的权限的命令：chmod 示例：修改 aaa.txt 的权限为文件所有者有全部权限，文件所有者所在的组有读写权限，其他用户只有读的权限。 chmod u=rwx,g=rw,o=r aaa.txt 或者 chmod 764 aaa.txt 10 MyBatisMyBatis 面试常见问题 10.1 MyBatis 怎么防止 SQL 注入MyBatis 中使用 #{} 防止 SQL 注入 1234567@ResultMap(\"BaseResultMap\")@Select(\"select * from user where username = #{username}\")User getUserByParas1(@Param(\"username\") String username);@ResultMap(\"BaseResultMap\")@Select(\"select * from user where username = ${username}\")List&lt;User&gt; getUserByParas2(@Param(\"username\") String username); #{} 使用了 PreparedStatement 来进行预处理，然后通过 set 的方式对占位符进行设置，而 ${} 则是通过 Statement 直接进行查询，当有参数时直接拼接进行查询。 Mybatis 中使用 #{} 可以防止 SQL 注入，${} 并不能防止 SQL 注入 Mybatis 实现防止 SQL 注入的原理是调用了 JDBC 中的 PreparedStatement 来进行预处理。 10.2 实体类的属性名和表中的字段名不一样怎么办TO DO 10.3 缓存TO DO 11 消息队列11.1 为什么要使用消息队列消息队列（一）为什么要使用消息队列？ 11.2 如何保证消息的顺序性消息队列（五）如何保证消息的顺序性？ 其他红黑树红黑树特点 每个节点非红即黑 根节点总是黑色的 每个叶子节点都是黑色的空节点（NIL 节点） 如果节点是红色的，则它的子节点必须是黑色的（反之不一定） 从根节点到叶节点或空子节点的每条路径，必须包含相同数目的黑色节点（即相同的黑色高度） 红黑树的应用 TreeMap、TreeSet 以及 JDK1.8 的 HashMap、ConcurrentHashMap 底层都用到了红黑树。 为什么要用红黑树？ 简单来说红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。 图解红黑树 排序算法常见排序算法知识体系详解 排序算法时间复杂度、空间复杂度、稳定性比较 排序算法 平均时间复杂度 最坏时间复杂度 最好时间复杂度 空间复杂度 稳定性 冒泡排序 O(n²) O(n²) O(n) O(1) 稳定 直接选择排序 O(n²) O(n²) O(n²) O(1) 不稳定 直接插入排序 O(n²) O(n²) O(n) O(1) 稳定 快速排序 O(nlogn) O(n²) O(nlogn) O(nlogn) 不稳定 堆排序 O(nlogn) O(nlogn) O(nlogn) O(1) 不稳定 希尔排序 O(nlogn) O(n²) O(n) O(1) 不稳定 归并排序 O(nlogn) O(nlogn) O(nlogn) O(n) 稳定 计数排序 O(n+k) O(n+k) O(n+k) O(n+k) 稳定 基数排序 O(N*M) O(N*M) O(N*M) O(M) 稳定 注：基数排序时间复杂度为 O (N*M)，其中 N 为数据个数，M 为数据位数。 直接选择排序的时间复杂度与初始序列无关，都为 O (n²) 辅助记忆 时间复杂度记忆 冒泡、选择、插入排序需要两个 for 循环，每次只关注一个元素，平均时间复杂度为 O (n²)（一遍找元素 O (n)，一遍找位置 O (n)） 快速、归并、希尔、堆基于二分思想，log 以 2 为底，平均时间复杂度为 O (nlogn)（一遍找元素 O (n)，一遍找位置 O (logn)） 稳定性记忆 快速排序、希尔排序、选择排序、堆排序，不稳定 “快希选堆”（快牺牲稳定性） 快速排序是最快的通用排序算法，它的内循环的指令很少，而且它还能利用缓存，因为它总是顺序地访问数据。它的运行时间近似为～cNlogN，这里的 c 比其它线性对数级别的排序算法都要小。 使用三向切分快速排序，实际应用中可能出现的某些分布的输入能够达到线性级别，而其它排序算法仍然需要线性对数时间。 LRU 算法LRU 原理和 Redis 实现 12345678910111213141516171819import java.util.LinkedHashMap;import java.util.Map;public class LRU&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; { private final int CACHE_SIZE; public LRU(int cacheSize) { // true 表示按访问顺序排序，最近访问放在头部，最老访问放在尾部 super((int)Math.ceil(cacheSize / 0.75) + 1, 0.75f, true); CACHE_SIZE = cacheSize; } @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) { // 当容量大于设定好的 cache size 时删除最早的节点 return size() &gt; CACHE_SIZE; }} 设计模式图说设计模式 面试官：“谈谈 Spring 中都用到了那些设计模式？” 单例模式好处： 对于频繁使用的对象，可以省略创建对象所花费的时间，这对于那些重量级对象而言，是非常可观的一笔系统开销 由于 new 操作的次数减少，因而对系统内存的使用频率也会降低，这将减轻 GC 压力，缩短 GC 停顿时间。 1、饿汉式：全局的单例实例在类装载时构建（线程安全） 1234567891011public class Singleton { private static final Singleton INSTANCE = new Singleton(); // 用户无法通过 new 方法创建该对象实例 private Singleton() { } public static Singleton getInstence() { return INSTANCE; }} 2、懒汉式（线程不安全） 12345678910111213public class Singleton { private static Singleton instance; private Singleton() { } public static Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; }} 3、懒汉式：双重检查（线程安全） 123456789101112131415161718public class Singleton { // volatile保证当 instance 变量被初始化成 Singleton 实例时，多个线程可以正确处理 instance 变量 private volatile static Singleton instance; private Singleton() { } public static Singleton getInstance() { if (instance == null) { synchronized (Singleton.class) { if (instance == null) { instance = new Singleton(); } } } return instance; }} 4、懒汉式：静态内部类（线程安全） 12345678910111213public class Singleton { private Singleton() { } private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } public static Singleton getInstance() { return SingletonHolder.INSTANCE; }} 5、枚举（线程安全，也能防止反序列化导致重新创建新的对象） 123public enum Singleton{ INSTANCE;} 工厂模式观察者模式求最大公约数和最小公倍数最大公约数 GCD： 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 递归求最大公约数 */public int GCD(int a, int b) { if (b == 0) { return a; } return GCD(b, a % b);}/** * 辗转相除法求最大公约数 * 有两整数 a 和 b： * 1. a % b 得余数 c * 2. 若 c = 0，则 b 即为两数的最大公约数 * 3. 若 c != 0，则 a = b，b = c，再回去执行 1 */public int divisionGCD(int a, int b) { int c; while (b != 0) { c = a % b; a = b; b = c; } return a;}/** * 相减法求最大公约数 * 有两整数 a 和 b： * 1. 若 a &gt; b，则 a = a - b * 2. 若 a &lt; b，则 b = b - a * 3. 若 a = b，则 a（或 b）即为两数的最大公约数 * 4. 若 a != b，则再回去执行 1 */public int subtractionGCD(int a, int b) { while (a != b) { if (a &gt; b) { a = a - b; } else { b = b - a; } } return a;} 最小公倍数 LCM： 123public int LCM(int a, int b) { return a * b / GCD(a, b);} 多线程编程用三个线程，顺序打印字母 A-Z，输出结果是 1A 2B 3C 1D 2E… 1234567891011121314151617181920212223242526272829303132public class Test { private static char c = 'A'; private static int i = 0; public static void main(String[] args) { Runnable runnable = new Runnable() { @Override public void run() { synchronized (this) { try { int id = Integer.parseInt(Thread.currentThread().getName()); while (i &lt; 26) { if (i % 3 == id - 1) { System.out.println(\"线程: \" + id + \" \" + c++); i++; notifyAll(); } else { wait(); } } } catch (InterruptedException e) { e.printStackTrace(); } } } }; new Thread(runnable, \"1\").start(); new Thread(runnable, \"2\").start(); new Thread(runnable, \"3\").start(); }}","link":"/2020/Java-%E9%9D%A2%E8%AF%95/"},{"title":"使用 acme.sh 自动签发 ZeroSSL 的 ECC 证书","text":"acme.sh 使用 ACME 协议自动申请 SSL 证书，项目地址：acme.sh 环境：CentOS 8 安装 acme.sh先安装好 curl，然后： 1curl https://get.acme.sh | sh 该安装脚本做了几件事： 把 acme.sh 安装到了 home 目录下：~/.acme.sh/ 创建了一个 bash 的 alias, 方便使用: alias acme.sh=~/.acme.sh/acme.sh 创建了 cronjob，每天 0:00 点自动检测所有的证书，如果快过期了，则会自动更新证书。 如果因为网络问题，无法连接到 https://raw.githubusercontent.com 域名，则要手动更改安装脚本内容： 1234567# 下载并保存脚本curl https://get.acme.sh -o acme_install.sh# 更改脚本内域名，使用代理加速sed -i 's/raw/ghproxy.com\\/https:\\/\\/raw/g' acme_install.sh# 安装chmod +x acme_install.sh./acme_install.sh 注册 ZeroSSL 账号 acme.sh 默认使用 Let’s Encrypt 进行证书注册，本本章使用 ZeroSSL 进行注册，所以需要进行相关配置 官网注册：https://zerossl.com/ 注册完成之后进入 Dashboard，选择右侧的 Developer 标签，生成 EAB Credentials 并记录下来。 将 acme.sh 的注册服务器改为 ZeroSSL 文档：https://github.com/acmesh-official/acme.sh/wiki/ZeroSSL.com-CA 运行，替换 eab-kid 和 eab-hmac-key 为上一步中生成的： 123acme.sh --register-account --server zerossl \\ --eab-kid xxxxxxxxxxxx \\ --eab-hmac-key xxxxxxxxx 将其设置为默认 Server 1acme.sh --set-default-ca --server zerossl 签发证书本文章使用 DNS 验证的方法来验证域名，acme.sh 可以通过 DNS 提供商的 API 自动设置验证记录，具体用法详见文档：https://github.com/acmesh-official/acme.sh/wiki/dnsapi 1234acme.sh --issue \\ --dns dns_dp \\ -d morooi.com -d \\*.morooi.com \\ --keylength ec-256 其中： --dns 指定 DNS 服务商，dns_dp 代表 DNSPod，还有 dns_cf 代表 CloudFlare，更多的字段见 https://github.com/acmesh-official/acme.sh/tree/master/dnsapi如果不使用 API 自动添加验证，则不用添加后续参数，如：acme.sh --issue --dns -d morooi.com .... -d \\*.morooi.com 表示签发泛域名证书，\\* 为转义（本人使用 zsh shell，实测需要转义，bash 貌似并不需要加反斜线） --keylength ec-256 表示签发 ECC 证书，不添加则签发 RSA 证书，可选参数还有 ec-384 和 ec-521 安装证书使用 --install-cert 命令安装证书到指定目录，切记不要手动去复制签发的证书文件，会影响后续使用 1234acme.sh --install-cert -d morooi.com --ecc \\ --key-file \"/ssl/_.morooi.com.key\" \\ --fullchain-file \"/ssl/_.morooi.com.crt\" \\ --reloadcmd \"nginx -s reload\" 其中： --ecc 表示安装 ECC 证书 --key-file 和 -fullchain-file 后接想要安装到的目录及证书名 --reloadcmd 后接服务的重启命令，程序自动续期证书后会运行该命令使其生效 大功告成！ 更多信息可参考如下： acme.sh wiki acme.sh 自动更新 RSA、ECC 双证书实践 Blogs and tutorials","link":"/2021/acme-sh/"},{"title":"使用 dockerfile-maven-plugin 插件构建并推送 Docker 镜像","text":"这篇笔记的目标是使用本地 IDEA 编译 SpringBoot 工程为 jar 包，并且使用安装好 Docker 的远程 Linux 主机构建 Docker 镜像，Push 到阿里云的镜像托管服务。 添加环境变量 修改 POM 文件配置 隐藏 POM 文件中的密码 编写 Dockerfile 首先需要一台安装好 Docker 的 Linux 主机或虚拟机，并且开放远程控制（见 -&gt; 开启 Docker 的远程控制） 添加环境变量IDEA 设置里添加 Maven 环境变量，告知 Docker Host 地址，具体操作如下： 找到 Preference -&gt; Build, Execution, Deployment -&gt; Build Tools -&gt; Maven -&gt; Runner 在 Environment variables 中填写 DOCKER_HOST=tcp://xxx.xxx.xxx.xxx:2375 修改 POM 文件配置在项目 POM 文件中添加 dockerfile-maven-plugin 插件配置，注意修改 repository, username, password。 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;dockerfile-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.4.13&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;build-tag-push-version&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;goal&gt;tag&lt;/goal&gt; &lt;goal&gt;push&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tag&gt;${project.version}&lt;/tag&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;tag-push-latest&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;tag&lt;/goal&gt; &lt;goal&gt;push&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tag&gt;latest&lt;/tag&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;repository&gt;registry.cn-shanghai.aliyuncs.com/xxxxx/xxxxx&lt;/repository&gt; &lt;username&gt;userName&lt;/username&gt; &lt;password&gt;userPassword&lt;/password&gt; &lt;buildArgs&gt; &lt;JAR_FILE&gt;${project.build.finalName}.jar&lt;/JAR_FILE&gt; &lt;/buildArgs&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 本配置的作用是，运行 mvn package 操作时： jar 包编译 使用项目根目录下的 Dockerfile 构建以版本号为 Tag 的 Docker 镜像，并 push 到阿里云镜像托管服务 构建 Tag 为 latest 的 Docker 镜像，并 push 到阿里云镜像托管服务 更多配置选项查看官方信息： https://github.com/spotify/dockerfile-maven 多 Tag 的配置可参考此 issue： https://github.com/spotify/dockerfile-maven/issues/10 隐藏 POM 文件中的密码maven 提供密码加密的功能，具体过程见：https://maven.apache.org/guides/mini/guide-encryption.html 配置完 ~/.m2/setting-security.xml 后，在 ~/.m2/setting.xml 找到 servers 标签，添加： 1234567&lt;servers&gt; &lt;server&gt; &lt;id&gt;registry.cn-shanghai.aliyuncs.com&lt;/id&gt; &lt;username&gt;userName&lt;/username&gt; &lt;password&gt;{encryptedPassword}&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 修改 POM 文件中的配置即可 123456789&lt;configuration&gt; &lt;repository&gt;registry.cn-shanghai.aliyuncs.com/xxxxx/xxxxx&lt;/repository&gt;- &lt;username&gt;userName&lt;/username&gt;- &lt;password&gt;userPassword&lt;/password&gt;+ &lt;useMavenSettingsForAuth&gt;true&lt;/useMavenSettingsForAuth&gt; &lt;buildArgs&gt; &lt;JAR_FILE&gt;${project.build.finalName}.jar&lt;/JAR_FILE&gt; &lt;/buildArgs&gt;&lt;/configuration&gt; 编写 Dockerfile在项目的根目录中新建 Dockerfile，注意其中 ARG JAR_FILE 字段是与 POM 文件中的设置对应的，例： 123456789101112FROM openjdk:11.0.8-jreMAINTAINER SJ Zhou &lt;morooiu@gmail.com&gt;WORKDIR /projectARG JAR_FILEADD target/${JAR_FILE} project.jarRUN ln -snf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ &amp;&amp; echo Asis/Shanghai &gt; /etc/timezoneEXPOSE 8080CMD [\"java\", \"-jar\", \"project.jar\"]","link":"/2020/docker-maven/"},{"title":"Docker 安装、使用记录","text":"内容: 安装 Docker CE 配置国内 Docker 仓库镜像 常用命令 DockerFile 开启 Docker 的远程控制 尚硅谷 Docker 入门视频：https://www.bilibili.com/video/av26993050 安装 Docker CE 多参考官方文档（Install Docker Engine），里面提供了各种 Linux 发行版的安装方式。 由于网络原因，使用官方镜像可能速度不够理想，可以使用阿里云提供的镜像进行安装，这里以 CentOS 7 为例，其他系统安装详见阿里云地址：https://developer.aliyun.com/mirror/docker-ce 运行如下，即可安装成功 123456789101112# step 1: 安装必要的一些系统工具sudo yum install -y yum-utils device-mapper-persistent-data lvm2# Step 2: 添加软件源信息sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# Step 3: 更新并安装Docker-CEsudo yum makecache fastsudo yum -y install docker-ce# Step 4: 开启Docker服务sudo systemctl start docker 配置国内 Docker 仓库镜像以阿里云镜像为例，打开 https://www.aliyun.com/product/acr 申请阿里云的容器镜像服务 (免费) 进入控制台，选择镜像加速器 按照操作文档操作 附 CentOS 7 / Ubuntu 的操作命令 12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'{ \"registry-mirrors\": [\"https://xxxxxx.mirror.aliyuncs.com\"]}EOFsudo systemctl daemon-reloadsudo systemctl restart docker 安装完成后运行: docker run hello-world，出现如下内容即安装运行成功。 常用命令可参考官方文档： Docker run reference docker (base command) 镜像命令： 12345678910111213141516171819202122# Docker 帮助docker --help -h# 查看本地主机上的镜像docker images [OPTION] -a: 列出本地所有的镜像(含中间映像层) -q: 只显示镜像 ID --digests: 显示镜像的摘要信息 --no-trunc: 显示镜像的完整信息# 搜索镜像名字docker search -s: 列出收藏数不小于指定值的镜像 (docker search -s 30 xxx)# 下载镜像docker pull xxx[:TAG] # 不写[:TAG]默认下载最新# 删除镜像docker rmi xxx1[:TAG] xxx2[:TAG] # 不写[:TAG]默认删除最新docker rmi $(docker images -qa) # 全部删除 -f: 强制删除 容器命令： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 新建并启动容器docker run [OPTION] image [COMMAND] [ARG...] --name: 别名 -i: 以交互模式运行容器, 通常与 -t 同时使用 -t: 为容器重新分配一个伪输入终端, 通常与 -i 同时使用 -d: 后台运行容器, 并返回容器 ID, 也即启动守护式容器 -P: 随机端口映射 -p: 指定端口映射, 有以下四种格式 ip:hostPort:containerPort ip::containerPort hostPort:containerPort containerPort -v 宿主机绝对路径目录:/容器内目录(:ro 只读不可写): 添加容器数据卷 # 列出当前正在运行的容器docker ps [OPTION] -a: 列出当前所有正在运行的容器 + 历史上运行过的 -l: 显示最近创建的容器 -n: 显示最近 n 个创建的容器 -q: 静默模式, 只显示容器编号 --no-trunc: 不截断输出# 退出容器(在容器里)exit # 退出并停止容器Ctrl + P + Q # 退出不停止容器# 启动容器docker start [容器名或容器ID]# 重启容器docker restart [容器名或容器ID]# 停止容器docker stop [容器名或容器ID]# 强行停止容器docker kill [容器名或容器ID]# 删除已停止的容器docker rm [容器名或容器ID] -f: 强制删除docker rm -f $(docker ps -aq) # 删除全部容器docker ps -aq | xargs docker rm # 删除全部容器# 查看容器日志docker logs [OPTION] [容器ID] -t: 加入时间戳 -f: 跟随最新的日志打印 --tail: 数字 显示最后多少条# 查看容器内进程docker top [容器ID]# 查看容器内部细节docker inspect [容器ID]# 进入正在运行的容器并以命令行交互docker exec -it [容器ID] COMMAND [ARG...] # 在容器中打开新的终端, 可以启动新的进程docker attach [容器ID] # 直接进入容器启动命令的终端, 不会启动新的进程# 从容器内拷贝文件到主机docker cp 容器ID:容器内路径 本机路径 DockerFile官方文档：Dockerfile reference 保留关键字: FROM：基础镜像，当前新镜像是基于哪个镜像的 MAINTAINER：镜像维护者的姓名和邮箱 RUN：容器构建时需要运行的命令 EXPOSE：当前容器对外暴露出的端口 WORKDIR：指定在创建容器后，终端默认登录进来工作目录 ENV：用来构建镜像过程中设计环境变量 ADD：将宿主机目录下的文件拷贝进镜像，且 ADD 命令会自动处理 URL 和解压 tar 压缩包 COPY：类似 ADD，拷贝文件和目录到镜像中 VOLUME：容器数据卷，用于数据保存和持久化工作 CMD：指定一个容器启动时要运行的命令（可以有多个，但只有最后一个生效，CMD 会被 docker run 之后的参数替换） ENTRYPOINT：指定一个容器启动时要运行的命令（目的和 CMD 一样，都是在指定容器启动程序及参数） ONBUILD：当构建一个被继承的 Dockerfile 时运行命令，父镜像在被子继承后父镜像的 onbuild 被触发 开启 Docker 的远程控制编辑 /usr/lib/systemd/system/docker.service 服务 /usr/lib/systemd/system/docker.service12345678[Service]Type=notify- ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock+ ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375 --containerd=/run/containerd/containerd.sockExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=always 添加 -H tcp://0.0.0.0:2375 表示监听本机所有 IP 的 2375 端口，也可指定特定的 IP，如 -H tcp://192.168.123.132:2375 重启服务以生效 12systemctl daemon-reloadsystemctl restart docker 其他开启方式见参考一 参考： Docker 官方文档 - Daemon socket option Docker 开启 Remote API 访问 2375 端口","link":"/2020/docker/"},{"title":"在 macOS 中使用 GPG 签名提交至 Github","text":"在 Github 显示的 commit 中发现有一个绿色的 “Verified” 小标记，于是尝试实现。 使用 GPG 在本地签名 commit，GitHub 可以验证这些签名，以便其他人知道提交来自可信的来源。 安装 GPGmacOS 下，使用 homebrew 进行安装 1$ brew install gpg … 生成 GPG 密钥对 如果使用的是 2.1.17 或更高版本，使用以下命令生成 GPG 密钥对 1$ gpg --full-generate-key 如果使用的不是 2.1.17 或更高版本，可使用以下命令生成 GPG 密钥对 1$ gpg --default-new-key-algo rsa4096 --gen-key 加密方式默认选择: RSA and DSA 输入所需的密钥长度: 4096 有效时长：按需指定 输入用户姓名 输入邮箱，这一步需要输入 GitHub 帐户中经过验证的电子邮件地址。 输入自定义的安全密码（界面可能会是乱码，不影响密码的输入） 查看并导出生成的密钥对使用命令 1$ gpg --list-secret-keys --keyid-format LONG 从 GPG 密钥列表中复制想要使用的 GPG 密钥 ID。 在此例中，GPG 密钥 ID 是 3AA5C34371567BD2： 123456$ gpg --list-secret-keys --keyid-format LONG/Users/hubot/.gnupg/secring.gpg------------------------------------sec 4096R/3AA5C34371567BD2 2016-03-10 [expires: 2017-03-10]uid Hubot ssb 4096R/42B317FD4BA89E7A 2016-03-10 导出要使用的密钥对，以此密钥为例 1gpg --armor --export 3AA5C34371567BD2 复制 GPG 密钥 从 -----BEGIN PGP PUBLIC KEY BLOCK----- 开始到 -----END PGP PUBLIC KEY BLOCK----- 结束 添加密钥至 Github在 Github 的设置中找到 SSH and GPG keys 选项，或点击链接：https://github.com/settings/keys 单击 New GPG key（新 GPG 密钥），然后粘贴上一步中复制的密钥 使用密钥在 Git 中设置 GPG 签名密钥，记得替换为自己的 GPG 密钥 ID 1git config --global user.signingkey 3AA5C34371567BD2 为当前的单个项目配置密钥认证 1$ git config commit.gpgsign true 为全部项目配置密钥认证 1$ git config --global commit.gpgsign true 错误解决使用 VS Code 提交代码时报错 error: gpg failed to sign the datafatal: failed to write commit object 同样，使用 IntelliJ IDEA 提交代码时报错： gpg failed to sign the data failed to write commit object 需安装 pinentry-mac 1$ brew install pinentry-mac 获取 pinentry-mac 的路径 1$ which pinentry-mac 返回如：/opt/homebrew/bin/pinentry-mac 新建 ~/.gnupg/gpg-agent.conf，加入 ~/.gnupg/gpg-agent.conf1pinentry-program /opt/homebrew/bin/pinentry-mac 重载 gpg-agent 生效 1$ gpg-connect-agent reloadagent 此时可以正常提交了，commit 时会弹出提示框，按照提示输入 Password 即可 参考链接 Github - 管理提交签名验证 在 GitHub commit 上加上 GPG 簽章","link":"/2020/github-gpg/"},{"title":"Jupyter Notebook 远程访问与后台静默运行","text":"允许远程访问12345jupyter notebook --generate-config # 初始化配置文件echo \"c.NotebookApp.ip = '*'\" &gt;&gt; ~/.jupyter/jupyter_notebook_config.pyecho \"c.NotebookApp.open_browser = False\" &gt;&gt; ~/.jupyter/jupyter_notebook_config.pyecho \"c.NotebookApp.port = 8888\" &gt;&gt; ~/.jupyter/jupyter_notebook_config.py 启动 Notebook12cd ~/ # 进入指定目录nohup jupyter notebook &gt; ./jupyter.log 2&gt;&amp;1 &amp; # 后台静默运行 打开 ./jupyter.log，找到访问路径，将地址改成主机对应的 IP，即可正常访问。 想关掉后台运行的 Jupyter Notebook，可运行 ps -a，知道对应的 PID，然后 kill -9 $PID 如果退出了 SSH 连接，下次连接想关闭后台的 Jupyter Notebook，运行 ps -ef | grep jupyter-notebook，然后 kill -9 $PID。","link":"/2019/jupyter/"},{"title":"在 CentOS 7 中安装 Hadoop 与 HBase","text":"本实验在 VMware 虚拟机中安装 CentOS 系统，搭建 Hadoop 伪分布式模式环境，安装 HBase。实验必要的环境，包括：VMware Workstation Pro 15, CentOS 7.6.1810, Java SE Development Kit 8u201, Hadoop 2.9.2, HBase 1.4.9。 下载地址： VMware Workstation Pro 15：https://pan.baidu.com/s/1LO6DxDw71vecyfvsJXv1jA 提取码: 98kc CentOS：CentOS-7-x86_64-Minimal-1810.iso Java SDK：jdk-8u201-linux-x64.tar.gz Hadoop：hadoop-2.9.2.tar.gz HBase：hbase-1.4.9-bin.tar.gz 本报告一共分为四个部分 第一部分：在 VMware 中安装 CentOS 第二部分：安装 Java，并配置环境变量 第三部分：安装并配置 Hadoop（伪分布式模式） 第四部分：安装并配置 HBase 第一部分：在 VMware 中安装 CentOS安装完成后使用 SSH 客户端连接到虚拟机。 由于写此报告时已经安装完成，并没有将过程截图，此部分略。 设置 CentOS 的部分命令（均使用 root 用户执行）： 关闭防火墙： 12$ systemctl stop firewalld.service #停止firewall$ systemctl disable firewalld.service #禁止firewall开机启动 关闭 selinux： 1$ vim /etc/sysconfig/selinux 修改 SELINUX=enabled 为 SELINUX=disabled，如图 第二部分：安装 Java，并配置环境变量安装 Java进入实验目录，这里选择在 /opt/app 目录下，下载 Java SDK 安装包 12$ cd /opt/app$ wget https://download.oracle.com/otn-pub/java/jdk/8u201-b09/42970487e3af4f5aa5bca3f542482c60/jdk-8u201-linux-x64.tar.gz 使用如下命令解压：tar zxvf ./jdk-8u201-linux-x64.tar.gz，得到目录 /opt/app/jdk1.8.0_201。 添加环境变量修改配置文件，执行 vim /etc/profile，在尾部添加 12export JAVA_HOME=\"/opt/app/jdk1.8.0_201\"export PATH=$JAVA_HOME/bin:$PATH 如图 修改完毕后，执行命令 source /etc/profile 使其生效。 执行命令 java -version，看到下图，即为安装成功 第三部分：安装并配置 Hadoop（伪分布式模式）安装 Hadoop下载 Hadoop 2.9.2 安装包 1$ wget http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-2.9.2/hadoop-2.9.2.tar.gz 使用如下命令解压：tar zxvf ./hadoop-2.9.2.tar.gz，得到目录 /opt/app/hadoop-2.9.2。 添加环境变量修改配置文件，执行 vim /etc/profile，在尾部添加 12export HADOOP_HOME=\"/opt/app/hadoop-2.9.2\"export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH 如图 修改完毕后，执行 source /etc/profile 使其生效。 执行 hadoop version，看到下图，即为安装成功 配置 Hadoop 修改 hadoop-env.sh 执行命令 1$ vim /opt/app/hadoop-2.9.2/etc/hadoop/hadoop-env.sh 修改文件中的 JAVA_HOME 参数为 Java 安装路径，结果如图 配置 core-site.xml 执行命令 1$ vim /opt/app/hadoop-2.9.2/etc/hadoop/core-site.xml 添加如下配置 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.29.127:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/data/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 其中 fs.defaultFS 参数配置的是 HDFS 的地址，192.168.29.127 为本虚拟机 IP 地址。 hadoop.tmp.dir 参数配置的是 Hadoop 临时目录，这里设置为 /opt/data/tmp，并手动创建此目录。 配置 hdfs-site.xml 执行命令 1$ vim /opt/app/hadoop-2.9.2/etc/hadoop/hdfs-site.xml 添加如下配置 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; dfs.replication 参数配置的是 HDFS 存储时的备份数量，因为这里是伪分布式环境只有一个节点，所以这里设置为 1。 格式化 HDFS 执行命令 hdfs namenode -format，当配置文件中设置的临时目录 /opt/data/tmp 下出现 dfs 文件夹时，说明格式化成功。 配置 mapred-site.xml 默认没有 mapred-site.xml 文件，从 mapred-site.xml.template 模板文件复制生成 mapred-site.xml。 在 Hadoop 目录下执行命令 1$ cp etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml 打开 mapred-site.xml，添加如下配置 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 此处设置 mapreduce.framework.name 参数指定 mapreduce 运行在 yarn 框架上。 配置 yarn-site.xml 执行命令 1$ vim /opt/app/hadoop-2.9.2/etc/hadoop/yarn-site.xml 添加如下配置 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;192.168.29.127&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn.nodemanager.aux-services 配置了 yarn 的默认混洗方式，选择为 mapreduce 的默认混洗算法。 yarn.resourcemanager.hostname 指定了 Resourcemanager 运行在哪个节点上，此处为本虚拟机 IP。 设置免密登录 关于免密登录详细的可以查看 Linux 笔记 通常 Hadoop 集群中的各个机器间会相互地通过 SSH 访问，每次访问都输入密码是不现实的，所以要配置免密登录。 这里是伪分布式的，所以只给本机配置就 OK 了。 在本机上生成公钥 执行命令 1$ ssh-keygen -t rsa 一路回车，都设置为默认值，然后再当前用户的目录下的.ssh 目录中会生成公钥文件 id_rsa.pub 和私钥文件 id_rsa。 分发公钥 1$ ssh-copy-id 192.168.29.127 ## 后面填写目的地址，此处是本机IP 至此，开启和关闭 Hadoop 服务都无需输入密码。 启动 Hadoop在 Hadoop 目录下执行命令 sbin/start-all.sh，开始启动 Hadoop 运行命令 jps 可以看到服务均开始运行 进入 http://192.168.27.127:50070 ，可以看到 Namenode 状态为 “active”。 Hadoop 指定 SSH 连接的端口号当因为某些原因自定义 SSH 端口号的时候，使用以下方法告诉 Hadoop 你的 SSH 端口号。 编辑 Hadoop 目录下 /etc/hadoop/hadoop-env.sh，添加 1export HADOOP_SSH_OPTS=\"-p xxx\" //xxx为自定义的SSH端口号 第四部分：安装并配置 HBase安装 HBase下载 HBase 1.4.9 安装包 1$ wget http://mirrors.tuna.tsinghua.edu.cn/apache/hbase/1.4.9/hbase-1.4.9-bin.tar.gz 使用如下命令解压：tar zxvf hbase-1.4.9-bin.tar.gz，得到目录 /opt/app/hbase-1.4.9。 添加环境变量修改配置文件，执行 vim /etc/profile，在尾部添加 12export HBASE_HOME=/opt/app/hbase-1.4.9export PATH=$HBASE_HOME/bin:$PATH 如图 修改完毕后，执行 source /etc/profile 使其生效。 执行 hbase version，看到下图，即为安装成功 配置 HBase 配置 hbase-site.xml 执行命令 1$ vim /opt/app/hbase-1.4.9/conf/hbase-site.xml 添加如下配置 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://192.168.29.127:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/data/hbase/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hbase.rootdir 配置的这个目录是 region server 的共享目录，用来持久化 HBase。 hbase.tmp.dir 指定了 HBase 的临时目录。 hbase.cluster.distributed 配置 HBase 的运行模式，false 是单机模式，true 是分布式模式。 配置 hbase-env.sh 执行命令 1$ vim /opt/app/hbase-1.4.9/conf/hbase-env.sh 添加如下配置 1234export JAVA_HOME=/opt/app/jdk1.8.0_201export HADOOP_HOME=/opt/app/hadoop-2.9.2export HBASE_HOME=/opt/app/hbase-1.4.9export HBASE_CLASSPATH=/opt/app/hadoop-2.9.2/etc/hadoop 启动 HBase 在启动 HBase 前先启动 Hadoop 在 HBase 目录下执行命令 bin/start-hbase.sh，看到以下状态 查看 Java 进程：jps 可以看到 HMaster 进程已经启动了。 至此，Hadoop 和 HBase 在 CentOS 7 上的安装已全部完成。","link":"/2019/installhadoop/"},{"title":"给 Ubuntu 安装 NVIDIA 显卡驱动","text":"在 Ubuntu 18.04 上安装 NVIDIA 有三种方法： 使用标准 Ubuntu 仓库进行自动化安装 使用 PPA 仓库进行自动化安装 使用官方的 NVIDIA 驱动进行手动安装 … 禁用 Nouveau 驱动 在安装之前首先要禁用 Nouveau 的驱动，可在安装系统前禁用，也可以在安装完成后禁用，看情况禁用即可。 在安装系统前禁用摘自：https://blog.csdn.net/tjuyanming/article/details/79267984 安装 Linux 之前先关闭 Security Boot（不同主板引导界面中该选项的位置可能不太一致，但是大多数都是在 boot 选项中的） 在进入 grub 安装界面的时候，在 Install Ubuntu 选项，按 e, 进入命令行模式，然后在 quiet slash -- 后面 (也可能没有 –)，添加以下内容，然后按 F10 重新引导 1acpi_osi=linux nomodeset 修改上述选项可以在开机的时候，禁用 nouveau 显卡 重新引导之后，可能会发现，安装的窗口有一部分屏幕下方，导致部分按钮无法点击。此时，按下 Alt+F7，鼠标会变成手指图标，即将窗口向上拖动即可。 在安装系统后禁用摘自：https://zhuanlan.zhihu.com/p/59618999 执行： 123sudo bash -c \"echo blacklist nouveau &gt; /etc/modprobe.d/blacklist-nouveau.conf\"sudo bash -c \"echo options nouveau modeset=0 &gt;&gt; /etc/modprobe.d/blacklist-nouveau.conf\"sudo update-initramfs -u 执行完上面两条指令后，我们使用如下命令看看是否成功禁用了开源驱动：cat /etc/modprobe.d/blacklist-nouveau.conf。如果和下面一样，表示成功了。 123$ cat /etc/modprobe.d/blacklist-nouveau.confblacklist nouveauoptions nouveau modeset=0 重启系统，完成 安装 NVIDIA 驱动使用标准 Ubuntu 仓库进行自动化安装这种方法几乎是所有的示例中最简单的方法。首先，检测 NVIDIA 显卡型号和推荐的驱动程序。 在命令行中输入如下命令：ubuntu-drivers devices 1234567$ ubuntu-drivers devices== /sys/devices/pci0000:64/0000:64:00.0/0000:65:00.0 ==modalias : pci:v000010DEd00001E04sv00001458sd000037C4bc03sc00i00vendor : NVIDIA Corporationdriver : nvidia-driver-435 - distro non-free recommendeddriver : nvidia-driver-430 - distro non-freedriver : xserver-xorg-video-nouveau - distro free builtin 从输出结果可以看到，目前系统已连接 Nvidia 显卡，建议安装驱动程序是 nvidia-driver-435 版本的驱动。如果您同意该建议，请再次使用 Ubuntu 驱动程序命令来安装所有推荐的驱动程序。 输入以下命令： 1$ sudo ubuntu-drivers autoinstall 一旦安装结束，重新启动系统，就完成了。 使用 PPA 仓库进行自动安装使用图形驱动程序 PPA 存储库允许我们安装 NVIDIA beta 驱动程序，但是这种方法存在不稳定的风险。 首先，将 ppa:graphics-drivers/ppa 存储库添加到系统中： 12$ sudo add-apt-repository ppa:graphics-drivers/ppa$ sudo apt update 接下来，识别显卡模型和推荐的驱动程序： 1$ ubuntu-drivers devices 输入以下命令： 1$ sudo apt install nvidia-440 一旦完成，即可重新启动系统。 可以卸载该源： 12$ sudo add-apt-repository -r ppa:graphics-drivers/ppa$ sudo apt update 使用官方的 NVIDIA 驱动进行手动安装下载 NVIDIA 官方驱动，https://www.nvidia.cn/Download/index.aspx?lang=cn 停止可视化桌面：sudo telinit 3 在相应路径下安装 NVIDIA 驱动: 1$ sudo bash ./NVIDIA-Linux-x86_64-440.31.run 按提示安装即可，重启完成！","link":"/2019/nvidia-ubuntu/"},{"title":"MapReduce 的学习","text":"HDFS 和 MapReduce 是 Hadoop 的两个重要核心，其中 MapReduce 是 Hadoop 的分布式计算模型。MapReduce 主要分为两步 Map 步和 Reduce 步，引用网上流传很广的一个故事来解释，现在你要统计一个图书馆里面有多少本书，为了完成这个任务，你可以指派小明去统计书架 1，指派小红去统计书架 2，这个指派的过程就是 Map 步，最后，每个人统计完属于自己负责的书架后，再对每个人的结果进行累加统计，这个过程就是 Reduce 步。 本代码运行环境为 ubuntu 18.04，使用 Hadoop 2.9.2 版本首先按照之前的方法（在 CentOS 7 中安装 Hadoop 与 HBase）搭建并启动 Hadoop MapReduce 原理 分析 MapReduce 执行过程 Mapper 任务的执行过程详解 Reducer 任务的执行过程详解 导入 MapReduce 的相关依赖 Wordcount 实例 Wordcount 整体流程 WordCount 代码实现 统计访问次数 统计每年最高气温 数据去重 日志分析：分析非结构化文件 MapReduce 原理分析 MapReduce 执行过程MapReduce 运行的时候，会通过 Mapper 运行的任务读取 HDFS 中的数据文件，然后调用自己的方法，处理数据，最后输出。Reducer 任务会接收 Mapper 任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到 HDFS 的文件中。 Mapper 任务的执行过程详解每个 Mapper 任务是一个 java 进程，它会读取 HDFS 中的文件，解析成很多的键值对，经过我们覆盖的 map 方法处理后，转换为很多的键值对再输出。整个 Mapper 任务的处理过程又可以分为以下六个阶段。 第一阶段是把输入文件按照一定的标准分片 (InputSplit)，每个输入片的大小是固定的。默认情况下，输入片 (InputSplit) 的大小与数据块 (Block) 的大小是相同的。如果数据块 (Block) 的大小是默认值 64MB，输入文件有两个，一个是 32MB，一个是 72MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由一个 Mapper 进程处理。这里的三个输入片，会有三个 Mapper 进程处理。 第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键” 是每一行的起始位置 (单位是字节)，“值” 是本行的文本内容。 第三阶段是调用 Mapper 类中的 map 方法。第二阶段中解析出来的每一个键值对，调用一次 map 方法。如果有 1000 个键值对，就会调用 1000 次 map 方法。每一次调用 map 方法会输出零个或者多个键值对。 第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份（如北京、上海、山东等），那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是 Reducer 任务运行的数量。默认只有一个 Reducer 任务。 第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值对 &lt;2,2&gt;,&lt;1,3&gt;,&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是 &lt;1,3&gt;,&lt;2,1&gt;,&lt;2,2&gt;。如果有第六阶段，那么进入第六阶段；如果没有，直接输出到本地的 linux 文件中。 第六阶段是对数据进行归约处理，也就是 reduce 处理。键相等的键值对会调用一次 reduce 方法。经过这一阶段，数据量会减少。归约后的数据输出到本地的 linxu 文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。 Reducer 任务的执行过程详解每个 Reducer 任务是一个 Java 进程。Reducer 任务接收 Mapper 任务的输出，归约处理后写入到 HDFS 中，可以分为几个阶段。 第一阶段是 Reducer 任务会主动从 Mapper 任务复制其输出的键值对。Mapper 任务可能会有很多，因此 Reducer 会复制多个 Mapper 的输出。 第二阶段是把复制到 Reducer 本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。 第三阶段是对排序后的键值对调用 reduce 方法。键相等的键值对调用一次 reduce 方法，每次调用会产生零个或者多个键值对。最后把这些输出的键值对写入到 HDFS 文件中。在整个 MapReduce 程序的开发过程中，我们最大的工作量是覆盖 map 函数和覆盖 reduce 函数。 导入 MapReduce 的相关依赖 使用 Java 语言进行 MapReduce 操作首先需要导入必要的依赖 如果使用的是 maven 管理项目，则需要编辑项目目录的 pom.xml 文件 打开 pom.xml，在 &lt;dependencies&gt;...&lt;dependencies&gt; 之间加入： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt; 如果不使用 Maven 管理，可以手动导入相关包。 Wordcount 实例 功能：统计每一个单词在整个数据集中出现的总次数 WordCount 实现的官方文档：Example: WordCount v1.0 Wordcount 整体流程最简单的 MapReduce 应用程序至少包含 3 个部分：一个 Map 函数、一个 Reduce 函数和一个 main 函数。在运行一个 MapReduce 计算任务时候，任务过程被分为两个阶段：map 阶段和 reduce 阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。main 函数将作业控制和文件输入 / 输出结合起来。 过程如图所示： 对一个有三行文本的文件进行 MapReduce 操作。 Map 过程：并行读取文本，对读取的单词进行 map 操作，每个词都以 &lt;key,value&gt; 形式生成。 Reduce 过程：对 map 的结果进行排序，合并，最后得出词频。 WordCount 代码实现新建文件 input.txt，内容： 123Deer Bear RiverCar Car RiverDeer Car Bear 将其上传到 HDFS 中，在终端运行： 123hadoop fs -mkdir /wordcounthadoop fs -mkdir /wordcount/inputhadoop fs -put input.txt /wordcount/input 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.StringTokenizer;public class WordCount { /** map 函数的输入键、输入值、输出键和输出值 Text 相当于java的string类型 IntWritable 相当于java中的Integer */ public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;{ private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context) throws IOException, InterruptedException{ //content用于输出内容的写入 //map()方法的输入是一个键和一个值。我们首先将包含一行输入的Text值转换成java中的string类型 StringTokenizer itr = new StringTokenizer(value.toString()); //这是一个分割字符串的类，java中默认的分隔符是:\"空格\",\"\\t\"制表符,\"\\n\"换行符,\"\\r\"回车符 while (itr.hasMoreTokens()) { //判断是否还有分隔符 word.set(itr.nextToken()); //下一个字符串转换为Text类型，nextToken()：返回从当前位置到下一个分隔符的字符串。 context.write(word, one); } } } //reduce函数也有四个形式参数类型用于指定输入和输出类型。 //reduce函数的输入类型必须匹配map函数的输出类型:即Text类型和Intwritable在这种情况下,reduce的输出也是Text和Intwritable public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException{ int sum = 0; for (IntWritable val : values){ sum += val.get(); } result.set(sum); context.write(key, result); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"Word Count\"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); //指定要用的map类型 job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); //指定要用的reduce类型 job.setOutputKeyClass(Text.class); //控制reduce函数的输出类型 job.setOutputValueClass(IntWritable.class); //定义输入数据的路径,可以是单个文件,也可以是一个目录(此时,将目录下所有文件当做输入) FileInputFormat.addInputPath(job, new Path(\"hdfs://localhost:8020/wordcount/input/input.txt\")); //定义输出路径,指定reduce函数输出文件的写入目录。在运行作业前该目录是不应该存在的，否则Hadoop会报错并拒绝运行作业 FileOutputFormat.setOutputPath(job, new Path(\"hdfs://localhost:8020/wordcount/output\")); System.exit(job.waitForCompletion(true) ? 0 : 1); }} 实验结果 运行程序，成功后可以看到在 /wordcount/output/ 下多出两个文件 其中第二个文件里为运行完成后的结果 WordCount 完成！ 统计访问次数统计用户在 2019 年度每个自然日的总访问次数。原始数据文件中提供了用户名称与访问日期。这个任务就是要获取以每个自然日为单位的所有用户访问次数的累加值。如果通过 MapReduce 编程实现这个任务，首先要考虑的是，Mapper 与 Reducer 各自的处理逻辑是怎样的；然后根据处理逻辑编写出核心代码；最后在 Idea 中编写完整代码，编译打包后提交给集群运行。 分析思路和逻辑 输入 / 输出格式。 这里社交网站用户的访问日期在格式上都属于文本格式，访问次数为整型数据格式。其组成的键值对为 &lt;访问日期，访问次数&gt;，因此 Mapper 的输出与 Reducer 的输出都选用 Text 类与 IntWritble 类。 Mapper 要实现的计算逻辑 Map 函数的主要任务是读取用户访问文件中的数据，输出所有访问日期与初始次数的键值对。&lt;访问日期，1 &gt; Reducer 要实现的计算逻辑 读取 Mapper 输出的键值对 &lt;访问日期，1&gt;，进行累加。 user_login.txt 访问日期内容如下 将其上传到 HDFS 中，路径为 /DailyAccessCount/input 12345678910111213141516Nehru,2019-01-01Dane,2019-01-01Walter,2019-01-01Gloria,2019-01-01Clarke,2019-01-01Madeline,2019-01-01Kevyn,2019-01-01Rebecca,2019-01-01Calista,2019-01-01Madeline,2019-01-02Kevyn,2019-01-02Rebecca,2019-01-03Calista,2019-01-04Walter,2019-03-12Gloria,2019-03-12Clarke,2019-03-12 程序代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class DailyAccessCount { public static class MyMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; { private final static IntWritable one = new IntWritable(1); public void map(Object key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String array[] = line.split(\",\"); //指定逗号为分隔符，组成数组 String keyOutput = array[1]; //提取数组中的访问日期作为key context.write(new Text(keyOutput), one); //组成键值对 } } public static class MyReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int sum = 0; //定义累加器，初始值为0 for (IntWritable val : values) { sum += val.get(); //将相同键的所有值进行累加 } result.set(sum); context.write(key, result); //输出访问日期，总访问次数 } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"Daily Access Count\"); //新建Job并设置主类 job.setJarByClass(DailyAccessCount.class); job.setMapperClass(MyMapper.class); //为作业设置map类 job.setReducerClass(MyReducer.class); //为作业设置reduce类 job.setMapOutputKeyClass(Text.class); //设置map输出key的格式类 job.setMapOutputValueClass(IntWritable.class); //设置map输出value的格式类 job.setOutputKeyClass(Text.class); //设置输出键的格式类 job.setOutputValueClass(IntWritable.class); //设置输出值的格式类 //定义输入数据的路径,可以是单个文件,也可以是一个目录(此时,将目录下所有文件当做输入) FileInputFormat.addInputPath(job, new Path(\"hdfs://localhost:8020/DailyAccessCount/input/user_login.txt\")); //定义输出路径,指定reduce函数输出文件的写入目录。在运行作业前该目录是不应该存在的，否则Hadoop会报错并拒绝运行作业 FileOutputFormat.setOutputPath(job, new Path(\"hdfs://localhost:8020/DailyAccessCount/output\")); System.exit(job.waitForCompletion(true) ? 0 : 1); //通知集群运行这个作业，并阻塞直到作业完成 }} 实验结果 运行程序，成功后可以看到在 /DailyAccessCount/output/ 下多出两个文件 其中第二个文件里为运行完成后的结果 统计访问次数完成！ 统计每年最高气温有如下文件 input_temperature.txt，其中 2010012325 表示在 2010 年 01 月 23 日的气温为 25 度。使用 MapReduce，计算每一年出现过的最大气温。 将其上传到 HDFS 中，路径为 /Temperature/input 123456789101112131415161718192021222324252627282930313233343536373839402014010114201401021620140103172014010410201401050620120106092012010732201201081220120109192012011023200101011620010102122001010310200101041120010105292013010619201301072220130108122013010929201301102320080101052008010216200801033720080104142008010516200701061920070107122007010812200701099920070110232010010114201001021620100103172010010410201001050620150106492015010722201501081220150109992015011023 设计代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class Temperature { /* * 四个泛型类型分别代表： * KeyIn Mapper的输入数据的Key，这里是每行文字的起始位置（0,11,....） * ValueIn Mapper的输入数据的Value，这里是每行文字 * KeyOut Mapper的输出数据的Key，这里是每行文字中的“年份” * ValueOut Mapper的输出数据的Value，这里是每行文字中的“气温” */ public static class TempMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; { public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { //打印样本：Before Mapper:0,2014010114 System.out.print(\"Before Mapper:\" + key + \",\" + value); String line = value.toString(); String year = line.substring(0, 4); int temperature = Integer.parseInt(line.substring(8)); context.write(new Text(year), new IntWritable(temperature)); //打印样本：After Mapper:2014,14 System.out.println(\" ==&gt; After Mapper:\" + new Text(year) + \",\" + new IntWritable(temperature)); } } /* * 四个泛型类型分别代表： * KeyIn Reducer的输入数据的Key，这里是每行文字中的“年份” * ValueIn Reducer的输入数据的Value，这里是每行文字中的“气温” * KeyOut Reducer的输出数据的Key，这里是不重复的“年份” * ValueOut Reducer的输出数据的Value，这里是这一年中的“最高气温” */ public static class TempReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int maxValue = Integer.MIN_VALUE; StringBuffer sb = new StringBuffer(); //取values的最大值 for (IntWritable value : values) { maxValue = Math.max(maxValue, value.get()); sb.append(value).append(\",\"); } //打印样本：Before Reduce:2001,12,10,11,29,16, System.out.print(\"Before Reduce:\" + key + \",\" + sb.toString()); context.write(key, new IntWritable(maxValue)); //打印样本：After Reduce:2001,29 System.out.println(\" ==&gt; After Reduce:\" + key + \",\" + maxValue); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"Temperature\"); job.setJarByClass(Temperature.class); job.setMapperClass(TempMapper.class); //为作业设置map类 job.setReducerClass(TempReducer.class); //为作业设置reduce类 job.setMapOutputKeyClass(Text.class); //设置map输出key的格式类 job.setMapOutputValueClass(IntWritable.class); //设置map输出value的格式类 job.setOutputKeyClass(Text.class); //设置输出键的格式类 job.setOutputValueClass(IntWritable.class); //设置输出值的格式类 //定义输入数据的路径,可以是单个文件,也可以是一个目录(此时,将目录下所有文件当做输入) FileInputFormat.addInputPath(job, new Path(\"hdfs://localhost:8020/Temperature/input/input_temperature.txt\")); //定义输出路径,指定reduce函数输出文件的写入目录。在运行作业前该目录是不应该存在的，否则Hadoop会报错并拒绝运行作业 FileOutputFormat.setOutputPath(job, new Path(\"hdfs://localhost:8020/Temperature/output\")); System.exit(job.waitForCompletion(true) ? 0 : 1); //通知集群运行这个作业，并阻塞直到作业完成 }} 实验结果 运行程序，成功后可以看到在 /Temperature/output/ 下多出两个文件 其中第二个文件里为运行完成后的结果 统计每年最高气温完成！ 数据去重实验原理 “数据去重” 主要是为了掌握和利用并行化思想来对数据进行有意义的筛选。统计大数据集上的数据种类个数、从网站日志中计算访问地等这些看似庞杂的任务都会涉及数据去重。 数据去重的最终目标是让原始数据中出现次数超过一次的数据在输出文件中只出现一次。在 MapReduce 流程中，map 的输出 &lt;key,value&gt; 经过 shuffle 过程聚集成 &lt;key,value-list&gt; 后交给 reduce。我们自然而然会想到将同一个数据的所有记录都交给一台 reduce 机器，无论这个数据出现多少次，只要在最终结果中输出一次就可以了。具体就是 reduce 的输入应该以数据作为 key，而对 value-list 则没有要求（可以设置为空）。当 reduce 接收到一个 &lt;key,value-list&gt; 时就直接将输入的 key 复制到输出的 key 中，并将 value 设置成空值，然后输出 &lt;key,value&gt;。 实验数据 现有一个某电商网站的数据文件，名为 buyer_favorite1，记录了用户收藏的商品以及收藏的日期，文件 buyer_favorite1 中包含（用户 id，商品 id，收藏日期）三个字段，数据内容以 \\t 分割，由于数据很大，所以为了方便统计我们只截取它的一部分数据，内容如下: 12345678910111213141516171819202122232425262728293031user_id product_id date10181 1000481 2010-04-04 16:54:3120001 1001597 2010-04-07 15:07:5220001 1001560 2010-04-07 15:08:2720042 1001368 2010-04-08 08:20:3020067 1002061 2010-04-08 16:45:3320056 1003289 2010-04-12 10:50:5520056 1003290 2010-04-12 11:57:3520056 1003292 2010-04-12 12:05:2920054 1002420 2010-04-14 15:24:1220055 1001679 2010-04-14 19:46:0420054 1010675 2010-04-14 15:23:5320054 1002429 2010-04-14 17:52:4520076 1002427 2010-04-14 19:35:3920054 1003326 2010-04-20 12:54:4420056 1002420 2010-04-15 11:24:4920064 1002422 2010-04-15 11:35:5420056 1003066 2010-04-15 11:43:0120056 1003055 2010-04-15 11:43:0620056 1010183 2010-04-15 11:45:2420056 1002422 2010-04-15 11:45:4920056 1003100 2010-04-15 11:45:5420056 1003094 2010-04-15 11:45:5720056 1003064 2010-04-15 11:46:0420056 1010178 2010-04-15 16:15:2020076 1003101 2010-04-15 16:37:2720076 1003103 2010-04-15 16:37:0520076 1003100 2010-04-15 16:37:1820076 1003066 2010-04-15 16:37:3120054 1003103 2010-04-15 16:40:1420054 1003100 2010-04-15 16:40:16 将其上传到 HDFS 中，路径为 /Filter/input 要求用 Java 编写 MapReduce 程序，根据商品 id 进行去重，统计用户收藏商品中都有哪些商品被收藏。 实验源码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import java.io.IOException;public class Filter { public static class Map extends Mapper&lt;Object, Text, Text, NullWritable&gt; { private static Text newKey = new Text(); public void map(Object key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); System.out.println(\"line is \" + line); if (line != null) { String arr[] = line.split(\" \"); System.out.println(\"a[1] is \" + arr[1]); newKey.set(arr[1]); context.write(newKey, NullWritable.get()); System.out.println(\"the new key is \" + newKey); } } } public static class Reduce extends Reducer&lt;Text, NullWritable, Text, NullWritable&gt; { public void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException,InterruptedException { context.write(key, NullWritable.get()); } } public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException{ Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"Filter\"); job.setJarByClass(Filter.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); FileInputFormat.addInputPath(job ,new Path(\"hdfs://localhost:8020/Filter/input/buyer_favorite.txt\")); FileOutputFormat.setOutputPath(job, new Path(\"hdfs://localhost:8020/Filter/output\")); System.exit(job.waitForCompletion(true) ? 0 : 1); }} 实验结果 运行程序，成功后可以看到在 /Filter/output/ 下多出两个文件 其中第二个文件里为运行完成后的结果 数据去重完成！ 日志分析：分析非结构化文件根据 tomcat 日志计算 url 访问情况 要求：区别统计 GET 和 POST URL 访问量 结果为：访问方式、URL、访问量 测试数据集： 123456789101112131415161718192021196.168.2.1 - - [03/Jul/2014:23:36:38 +0800] \"GET /course/detail/3.htm HTTP/1.0\" 200 38435 0.038182.131.89.195 - - [03/Jul/2014:23:37:43 +0800] \"GET /html/notes/20140617/888.html HTTP/1.0\" 301 - 0.000196.168.2.1 - - [03/Jul/2014:23:38:27 +0800] \"POST /service/notes/addViewTimes_23.htm HTTP/1.0\" 200 2 0.003196.168.2.1 - - [03/Jul/2014:23:39:03 +0800] \"GET /html/notes/20140617/779.html HTTP/1.0\" 200 69539 0.046196.168.2.1 - - [03/Jul/2014:23:43:00 +0800] \"GET /html/notes/20140318/24.html HTTP/1.0\" 200 67171 0.049196.168.2.1 - - [03/Jul/2014:23:43:59 +0800] \"POST /service/notes/addViewTimes_779.htm HTTP/1.0\" 200 1 0.003196.168.2.1 - - [03/Jul/2014:23:45:51 +0800] \"GET /html/notes/20140617/888.html HTTP/1.0\" 200 70044 0.060196.168.2.1 - - [03/Jul/2014:23:46:17 +0800] \"GET /course/list/73.htm HTTP/1.0\" 200 12125 0.010196.168.2.1 - - [03/Jul/2014:23:46:58 +0800] \"GET /html/notes/20140609/542.html HTTP/1.0\" 200 94971 0.077196.168.2.1 - - [03/Jul/2014:23:48:31 +0800] \"POST /service/notes/addViewTimes_24.htm HTTP/1.0\" 200 2 0.003196.168.2.1 - - [03/Jul/2014:23:48:34 +0800] \"POST /service/notes/addViewTimes_542.htm HTTP/1.0\" 200 2 0.003196.168.2.1 - - [03/Jul/2014:23:49:31 +0800] \"GET /notes/index-top-3.htm HTTP/1.0\" 200 53494 0.041196.168.2.1 - - [03/Jul/2014:23:50:55 +0800] \"GET /html/notes/20140609/544.html HTTP/1.0\" 200 183694 0.076196.168.2.1 - - [03/Jul/2014:23:53:32 +0800] \"POST /service/notes/addViewTimes_544.htm HTTP/1.0\" 200 2 0.004196.168.2.1 - - [03/Jul/2014:23:54:53 +0800] \"GET /service/notes/addViewTimes_900.htm HTTP/1.0\" 200 151770 0.054196.168.2.1 - - [03/Jul/2014:23:57:42 +0800] \"GET /html/notes/20140620/872.html HTTP/1.0\" 200 52373 0.034196.168.2.1 - - [03/Jul/2014:23:58:17 +0800] \"POST /service/notes/addViewTimes_900.htm HTTP/1.0\" 200 2 0.003196.168.2.1 - - [03/Jul/2014:23:58:51 +0800] \"GET /html/notes/20140617/888.html HTTP/1.0\" 200 70044 0.057186.76.76.76 - - [03/Jul/2014:23:48:34 +0800] \"POST /service/notes/addViewTimes_542.htm HTTP/1.0\" 200 2 0.003186.76.76.76 - - [03/Jul/2014:23:46:17 +0800] \"GET /course/list/73.htm HTTP/1.0\" 200 12125 0.0108.8.8.8 - - [03/Jul/2014:23:46:58 +0800] \"GET /html/notes/20140609/542.html HTTP/1.0\" 200 94971 0.077 将其上传到 HDFS 中，路径为 /LogAnalyze/input 由于 Tomcat 日志是不规则的，需要先过滤清洗数据。这里使用 MapReduce 进行操作。 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class LogAnalyze { public static class LogMapper extends Mapper&lt;LongWritable, Text, Text , IntWritable&gt; { private IntWritable val = new IntWritable(1); public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString().trim(); String tmp = handlerLog(line); if (tmp.length() &gt; 0) { context.write(new Text(tmp), val); } } //196.168.2.1 - - [03/Jul/2014:23:36:38 +0800] \"GET /course/detail/3.htm HTTP/1.0\" 200 38435 0.038 private String handlerLog(String line) { String result = \"\"; try { if (line.length() &gt; 20) { if (line.indexOf(\"GET\") &gt; 0) { result = line.substring(line.indexOf(\"GET\"), line.indexOf(\"HTTP/1.0\")).trim(); } else if (line.indexOf(\"POST\") &gt; 0) { result = line.substring(line.indexOf(\"POST\"), line.indexOf(\"HTTP/1.0\")).trim(); } } } catch (Exception e) { System.out.println(line); } return result; } } public static class LogReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException,InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } context.write(key, new IntWritable(sum)); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"Log Analyze\"); job.setJarByClass(LogAnalyze.class); job.setMapperClass(LogMapper.class); job.setReducerClass(LogReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(\"hdfs://localhost:8020/LogAnalyze/input/data.txt\")); FileOutputFormat.setOutputPath(job, new Path(\"hdfs://localhost:8020/LogAnalyze/output\")); System.exit(job.waitForCompletion(true) ? 0 : 1); }} 分析结果 Log 分析完成！","link":"/2019/mapreduce/"},{"title":"Hexo - Next 使用，优化记录","text":"图片路径问题突然发现博客里面所有非网络图床的图片全部失效，排查发现所有图片链接在域名后面都加上了 /.cn/xxxx，后在 Github 找到解决方案 修改博客根目录 node_modules/hexo-asset-image/index.js 文件第 24 行： 1234else {- var endPos = link.lastIndexOf('.');+ var endPos = link.length - 1;} 问题解决～！ Github issue: https://github.com/xcodebuild/hexo-asset-image/issues/47","link":"/2020/hexo-next/"},{"title":"Hadoop HDFS 实践（1）—— 安装 IDEA 并创建项目","text":"本实验使用 IntelliJ IDEA 2019.1 创建项目，使用 maven 管理依赖。 安装 IntelliJ IDEA 2019.1进入官网，下载 Windows 版本的 IDEA（直达链接：https://www.jetbrains.com/idea/download/#section=Windows），这里选择 Community 版本。 首次启动需要进行相关配置，按提示进行。 创建项目点击 Create New Project，在左边选择 Maven，Project SDK 选择 Java 的安装目录，我这里版本是 1.8，选择 Create from archetype，在下方选择 org.apache.maven.archetypes:maven-archetype-quickstart 快速创建一个简单的 Maven 项目。 选择完毕后进入下一步。 填写项目相关信息，GroupId 可以理解成开发者信息，ArtifactId 可以理解成项目名称，Version 不用更改，默认就行。 填好后进入下一步。 此页总结项目的相关信息，无需改动，确认无误后直接下一步。 接下来设置 Project name，此处默认填写上面填入 ArtifactId 的字段。 Project location 是项目存储地址，此处随意选择，我选择了默认。 点击 FINISH，项目创建成功。 添加本次实验需要的相关依赖项目创建成功后会自动打开，Maven 会自动下载初始项目的依赖，右下角弹出 Maven projects need to be imported，这里选择 Enable Auto-Import 就好。 设置 pom.xml使用 Java 语言对 Hadoop HDFS 进行相关操作需要使用相关依赖，这里用 Maven 添加。 进入 https://mvnrepository.com，查找需要的依赖。 搜索包名称，选择相应版本，然后复制红框中的内容。 打开 IDEA 项目，编辑目录下的 pom.xml 文件，首先确定编译器的版本，我这里使用的是 Java SDK 1.8 版本，将文件中 12345&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;&lt;/properties&gt; 中间数字设置为 1.8。 然后在 &lt;dependencies&gt;...&lt;/dependencies&gt; 中间粘贴刚才复制的依赖信息。 以下是我设置好的完整依赖信息。 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 其他部分不作修改，设置完成后，Maven 会自动在后台下载依赖，此时可以开始编写项目代码。 注：在 Maven 下载完依赖前，导入包会有提示错误，这是因为依赖没有下载完成，等待完成后错误会消失。","link":"/2019/operatehadoop01/"},{"title":"Shell 脚本的学习笔记和一些常用脚本","text":"持续更新中 … Github Repo: https://github.com/morooi/shell-script 学习记录TO DO 常用脚本快速安装 Trojan-gotrojan-go-quickstart.sh 12345678# 使用 root 用户运行，或添加 sudowget -N --no-check-certificate \"https://raw.githubusercontent.com/morooi/shell-script/master/trojan-go-quickstart.sh\"chmod +x trojan-go-quickstart.sh./trojan-go-quickstart.sh# 或bash &lt;(wget -qO- https://raw.githubusercontent.com/morooi/shell-script/master/trojan-go-quickstart.sh)# 或bash &lt;(curl -sL https://raw.githubusercontent.com/morooi/shell-script/master/trojan-go-quickstart.sh) 如果无法连接，使用如下 12345678# 使用 root 用户运行，或添加 sudowget -N --no-check-certificate \"https://cdn.jsdelivr.net/gh/morooi/shell-script/trojan-go-quickstart.sh\"chmod +x trojan-go-quickstart.sh./trojan-go-quickstart.sh# 或bash &lt;(wget -qO- https://cdn.jsdelivr.net/gh/morooi/shell-script/trojan-go-quickstart.sh)# 或bash &lt;(curl -sL https://cdn.jsdelivr.net/gh/morooi/shell-script/trojan-go-quickstart.sh) 安装 zsh 和 oh-my-zshinstall-zsh.sh 1234567wget -N --no-check-certificate \"https://raw.githubusercontent.com/morooi/shell-script/master/install-zsh.sh\"chmod +x install-zsh.sh./install-zsh.sh# 或bash &lt;(wget -qO- https://raw.githubusercontent.com/morooi/shell-script/master/install-zsh.sh)# 或bash &lt;(curl -sL https://raw.githubusercontent.com/morooi/shell-script/master/install-zsh.sh) 如果无法连接，使用如下 1234567wget -N --no-check-certificate \"https://cdn.jsdelivr.net/gh/morooi/shell-script/install-zsh.sh\"chmod +x install-zsh.sh./install-zsh.sh# 或bash &lt;(wget -qO- https://cdn.jsdelivr.net/gh/morooi/shell-script/install-zsh.sh)# 或bash &lt;(curl -sL https://cdn.jsdelivr.net/gh/morooi/shell-script/install-zsh.sh)","link":"/2020/shell-script/"},{"title":"Hadoop HDFS 实践（2）—— 使用 Java 编程操作 HDFS 文件","text":"接着上一篇：Hadoop HDFS 实践（1）—— 安装 IDEA 并创建项目 本笔记主要内容如下： 查看 HDFS 中的文件列表 错误处理 在 HDFS 中创建新目录 上传文件到 HDFS 从 HDFS 下载文件到本地 从 HDFS 中删除文件 重命名 HDFS 中的文件或文件夹 流方式读取文件部分内容 查看 HDFS 中的文件列表在项目目录 ~/src/main/java 下创建一个类，命名为 Ls.java，写入代码 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.BlockLocation;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.LocatedFileStatus;import org.apache.hadoop.fs.Path;import org.apache.hadoop.fs.RemoteIterator;public class Ls { public static void main(String[] args) throws Exception { String uri = \"hdfs://192.168.29.127:8020/\"; // HDFS地址 Configuration config = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), config, \"root\"); // 方法1：直接获取 FileStatus[] listStatus = fs.listStatus(new Path(\"/\")); for (FileStatus file : listStatus) { System.out.println(\"[\" + (file.isFile() ? \"file\" : \"dir\") + \"]\" + file.getPath().getName()); } // 方法2：使用迭代器，并递归获取所有子文件夹下的文件 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(\"/\"), true); while (listFiles.hasNext()) { System.out.println(\"========================\"); LocatedFileStatus fileStatus = listFiles.next(); System.out.println(\"块大小：\" + fileStatus.getBlockSize()); System.out.println(\"所属：\" + fileStatus.getOwner()); System.out.println(\"备份数：\" + fileStatus.getReplication()); System.out.println(\"权限：\" + fileStatus.getPermission()); System.out.println(\"名称：\" + fileStatus.getPath().getName()); System.out.println(\"---------块信息---------\"); BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation b : blockLocations) { System.out.println(\"块起始偏移量\" + b.getOffset()); System.out.println(\"块长度\" + b.getLength()); // 块所在的datanode节点 String[] datanodes = b.getHosts(); for (String dn : datanodes) { System.out.println(\"DataNode:\" + dn); } } } }} 首先启动 Hadoop，运行 1$ /opt/app/hadoop-2.9.2/sbin/start-all.sh 待 Hadoop 成功运行后，右键点击代码，选择 Run 'Ls.main()' 运行结果 (由于篇幅限制，此处省略部分输出结果)： 1234567891011121314151617181920212223242526272829303132333435363738[dir]hbase========================块大小：134217728所属：root备份数：1权限：rw-r--r--名称：state-00000000000000000006.log---------块信息---------............========================块大小：134217728所属：root备份数：1权限：rw-r--r--名称：hadoop%2C33452%2C1553933199568.1553944017981---------块信息---------块起始偏移量0块长度83DataNode:hadoop========================块大小：134217728所属：root备份数：1权限：rw-r--r--名称：hbase.version---------块信息---------块起始偏移量0块长度7DataNode:hadoopProcess finished with exit code 0 可以看出这些文件是由 HBase 创建的，推测是之前初始化 HBase 的时候自动生成。 错误处理在初次运行时会出现如图所示报错，且不会有任何输出。 这是由于 log4j 没有配置日志记录的位置，需要配置 log4j.properties。 解决方法 在 src 文件夹下新建 resources 文件夹，并在里面新建文件 log4j.properties，在文件中输入以下内容 123456hadoop.root.logger = DEBUG, consolelog4j.rootLogger = DEBUG, consolelog4j.appender.console = org.apache.log4j.ConsoleAppenderlog4j.appender.console.target = System.outlog4j.appender.console.layout = org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern = %d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n 重新运行代码，会有正常结果输出。 在 HDFS 中创建新目录在 HDFS 中创建目录 /mkdir/a/b 在项目目录 ~/src/main/java 下创建一个类，命名为 Mkdir.java，写入代码 代码如下： 123456789101112131415import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;public class Mkdir { public static void main(String[] args) throws Exception { String uri = \"hdfs://192.168.29.127:8020/\"; // HDFS地址 Configuration config = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), config, \"root\"); boolean mkdirs = fs.mkdirs(new Path(\"/mkdir/a/b\")); System.out.println(\"创建目录结果：\" + mkdirs); }} 运行，输出结果： 1创建目录结果：true 使用 hdfs 命令验证 1$ hdfs dfs -ls /mkdir/a/b 没有报不存在的错误，创建成功！ 上传文件到 HDFS在当前项目目录下新建测试文件，上传到 HDFS 中的 /mkdir 先在项目目录下创建文件 Hello_World.txt，写入内容：This is a test file. 注意要在项目目录下创建，而不能在 src 目录下创建 然后在 ~/src/main/java 里创建一个类，命名为 Put.java，写入代码 代码如下： 12345678910111213141516import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;public class Put { public static void main(String[] args) throws Exception { String uri = \"hdfs://192.168.29.127:8020/\"; //HDFS地址 Configuration config = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), config, \"root\"); String localpath = \"Hello_World.txt\"; fs.copyFromLocalFile(new Path(localpath), new Path(\"/mkdir\")); fs.close(); }} 运行。 使用 hdfs 命令验证 1$ hdfs dfs -ls /mkdir 可以看到文件已经成功上传！ 从 HDFS 下载文件到本地把 HDFS 中的 /mkdir/Hello_World.txt 下载到指定目录下 在项目目录 ~/src/main/java 下创建一个类，命名为 Get.java，写入代码 代码如下： 123456789101112131415import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;public class Get { public static void main(String[] args) throws Exception { String uri = \"hdfs://192.168.29.127:8020/\"; //HDFS地址 Configuration config = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), config, \"root\"); String localpath = \"Hello_World_2.txt\"; fs.copyToLocalFile(new Path(\"/mkdir/Hello_World.txt\"), new Path(localpath)); }} 运行。 完毕后可以看到在项目目录下多出文件 Hello_World_2.txt，打开后内容和之前填入的一致。 下载成功！ 从 HDFS 中删除文件删除 HDFS 中的 /mkdir/Hello_World.txt 文件 在项目目录 ~/src/main/java 下创建一个类，命名为 Del.java，写入代码 代码如下： 123456789101112131415import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;public class Del { public static void main(String[] args) throws Exception { String uri = \"hdfs://192.168.29.127:8020/\"; //HDFS地址 Configuration config = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), config, \"root\"); boolean ret = fs.delete(new Path(\"/mkdir/Hello_World.txt\"), true); System.out.println(\"删除结果：\" + ret); }} 运行，输出结果： 1删除结果：true 使用 hdfs 命令验证 1$ hdfs dfs -ls /mkdir 可以看到文件成功被删除！ 重命名 HDFS 中的文件或文件夹把 HDFS 中的 /mkdir/a 重命名为 /mkdir/a_2 在项目目录 ~/src/main/java 下创建一个类，命名为 Rename.java，写入代码 代码如下： 1234567891011121314import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;public class Rename { public static void main(String[] args) throws Exception { String uri = \"hdfs://192.168.29.127:8020/\"; //HDFS地址 Configuration config = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), config, \"root\"); fs.rename(new Path(\"/mkdir/a\"), new Path(\"/mkdir/a_2\")); }} 运行 使用 hdfs 命令验证 1$ hdfs dfs -ls /mkdir 重命名成功！ 流方式读取文件部分内容上传一个文本文件，然后使用流方式读取部分内容保存到当前项目目录 首先创建一个文件 New.txt，内容为： 1qwerasdfzxcv123456 上传到 HDFS 根目录 1$ hdfs dfs -put New.txt / 在项目目录 ~/src/main/java 下创建一个类，命名为 StreamGet.java，写入代码 代码如下： 1234567891011121314151617181920import java.io.FileOutputStream;import java.net.URI;import org.apache.commons.io.IOUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;public class StreamGet { public static void main(String[] args) throws Exception { String uri = \"hdfs://192.168.29.127:8020/\"; //HDFS地址 Configuration config = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), config, \"root\"); FSDataInputStream inputStream = fs.open(new Path(\"/New.txt\")); inputStream.seek(5); //指定读取的开始位置，这里设置为5，即从's'开始 FileOutputStream outputStream = new FileOutputStream(\"New.txt.part2\"); //新的文件名为New.txt.part2 IOUtils.copy(inputStream, outputStream); }} 运行 可以看到项目目录下多出 New.txt.part2 文件，打开可以看到文件内容。 全文完","link":"/2019/operatehadoop02/"},{"title":"弹性分布式数据集 (Resilient Distributed Datasets, RDD)","text":"RDD 简介RDD，全称 Resilient Distributed Datasets（弹性分布式数据集），是 Spark 最为核心的概念，是 Spark 对数据的抽象。 RDD 是分布式的元素集合，每个 RDD 只支持读操作，且每个 RDD 都被分为多个分区存储到集群的不同节点上。除此之外，RDD 还允许用户显示的指定数据存储到内存和磁盘中。 对 RDD 的操作，从类型上也比较简单，包括：创建 RDD、转化已有的 RDD 以及在已有 RDD 的基础上进行求值。 RDD 编程练习flatMapflatMap() 接收一个函数作为参数，该函数将每个元素转为一个列表，最终 flatMap() 并不是返回由上述列表作为元素组成的 RDD，而是返回一个包含每个列表所有元素的 RDD flatMap() 练习代码： 12345678910111213141516171819import org.apache.log4j.{Level, Logger}import org.apache.spark.{SparkConf, SparkContext}object flatMap { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"RDDFlatMap\") Logger.getLogger(\"org.apache.spark\").setLevel(Level.OFF) val sc = new SparkContext(conf) transformationOps(sc) sc.stop() } def transformationOps(sc:SparkContext): Unit = { val list = List(\"hello you\", \"hello he\", \"hello me\") val listRDD = sc.parallelize(list) val wordsRDD = listRDD.flatMap(line =&gt; line.split(\" \")) wordsRDD.foreach(println) }} 结果如下图： samplesample() 采样变换根据给定的随机种子，从 RDD 中随机地按指定比例选一部分记录，创建新的 RDD。 语法 1def sample(withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] 参数withReplacement: Boolean, True 表示进行替换采样，False 表示进行非替换采样fraction: Double, 在 0~1 之间的一个浮点值，表示要采样的记录在全体记录中的比例seed: 随机种子 sample() 练习代码： 12345678910111213141516171819202122import org.apache.log4j.{Level, Logger}import org.apache.spark.{SparkConf, SparkContext}object sample { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(\"local[2]\").setAppName(sample.getClass.getSimpleName) Logger.getLogger(\"org.apache.spark\").setLevel(Level.OFF) val sc = new SparkContext(conf) transformationOps(sc) sc.stop() } def transformationOps(sc: SparkContext): Unit = { val list = 1 to 1000 val listRDD = sc.parallelize(list) val sampleRDD = listRDD.sample(false, 0.2) sampleRDD.foreach(num =&gt; print(num + \" \")) println println(\"sampleRDD count: \" + sampleRDD.count()) println(\"Another sampleRDD count: \" + sc.parallelize(list).sample(false, 0.2).count()) }} 结果如下图： unionunion() 合并变换将两个 RDD 合并为一个新的 RDD，重复的记录不会被剔除。 语法 1def union(other: RDD[T]): RDD[T] 参数other: 第二个 RDD union() 练习代码： 12345678910111213141516171819202122import org.apache.log4j.{Level, Logger}import org.apache.spark.{SparkConf, SparkContext}object union { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(\"local[2]\").setAppName(union.getClass.getSimpleName) Logger.getLogger(\"org.apache.spark\").setLevel(Level.OFF) val sc = new SparkContext(conf) transformationOps(sc) sc.stop() } def transformationOps(sc: SparkContext): Unit = { val list1 = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) val list2 = List(7, 8, 9, 10, 11, 12) val listRDD1 = sc.parallelize(list1) val listRDD2 = sc.parallelize(list2) val unionRDD = listRDD1.union(listRDD2) unionRDD.foreach(println) }} 结果如下图： groupByKeygroupByKey() 将 RDD 中每个键的值分组为单个序列。 groupByKey() 练习代码： 1234567891011121314151617181920212223import org.apache.log4j.{Level, Logger}import org.apache.spark.{SparkConf, SparkContext}object groupByKey { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(\"local[2]\").setAppName(groupByKey.getClass.getSimpleName) Logger.getLogger(\"org.apache.spark\").setLevel(Level.OFF) val sc = new SparkContext(conf) transformationOps(sc) sc.stop() def transformationOps(sc: SparkContext): Unit = { val list = List(\"hello you\", \"hello he\", \"hello me\") val listRDD = sc.parallelize(list) val wordsRDD = listRDD.flatMap(line =&gt; line.split(\" \")) val pairsRDD = wordsRDD.map(word =&gt; (word, 1)) pairsRDD.foreach(println) val gbkRDD = pairsRDD.groupByKey() println(\"==============================\") gbkRDD.foreach(t =&gt; println(t._1 + \"...\" + t._2)) } }} 结果如下图： reduceByKey与 groupByKey() 类似，却有不同。 如 (a,1), (a,2), (b,1), (b,2)，groupByKey() 产生中间结果为 ((a,1), (a,2)), ((b,1), (b,2))，reduceByKey() 为 (a,3), (b,3)。 reduceByKey() 主要作用是聚合，groupByKey() 主要作用是分组。 reduceByKey() 练习代码： 1234567891011121314151617181920212223import org.apache.log4j.{Level, Logger}import org.apache.spark.rdd.RDDimport org.apache.spark.{SparkConf, SparkContext}object reduceByKey { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(\"local[2]\").setAppName(reduceByKey.getClass.getSimpleName) Logger.getLogger(\"org.apache.spark\").setLevel(Level.OFF) val sc = new SparkContext(conf) transformationOps(sc) sc.stop() } def transformationOps(sc: SparkContext): Unit = { val list = List(\"hello you\", \"hello he\", \"hello me\") val listRDD = sc.parallelize(list) val wordsRDD = listRDD.flatMap(line =&gt; line.split(\" \")) val pairsRDD: RDD[(String, Int)] = wordsRDD.map(word =&gt; (word, 1)) val retRDD: RDD[(String, Int)] = pairsRDD.reduceByKey((v1, v2) =&gt; v1 + v2) retRDD.foreach(t =&gt; println(t._1 + \"...\" + t._2)) }} 结果如下图： sortByKeysortByKey([ascending], [numTasks]) sortByKey() 作用于 Key-Value 形式的 RDD，并对 Key 进行排序。 [ascending] 升序，默认为 true，即升序 12345678910111213141516171819202122232425262728293031323334import org.apache.log4j.{Level, Logger}import org.apache.spark.rdd.RDDimport org.apache.spark.{SparkConf, SparkContext}object sortByKey { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(\"local[2]\").setAppName(sortByKey.getClass.getSimpleName) Logger.getLogger(\"org.apache.spark\").setLevel(Level.OFF) val sc = new SparkContext(conf) transformationOps(sc) sc.stop() } /** * sortByKey：将学生身高进行（降序）排序 * 身高相等，按照年龄排（升序） */ def transformationOps(sc: SparkContext): Unit = { val list = List( \"1,李 磊,22,175\", \"2,刘银鹏,23,175\", \"3,齐彦鹏,22,180\", \"4,杨 柳,22,168\", \"5,敦 鹏,20,175\" ) val listRDD:RDD[String] = sc.parallelize(list) // 使用sortByKey完成操作,只做身高降序排序 val heightRDD:RDD[(String, String)] = listRDD.map(line =&gt; { val fields = line.split(\",\") (fields(3), line) }) val retRDD:RDD[(String, String)] = heightRDD.sortByKey(ascending = false, numPartitions = 1) retRDD.foreach(println) }} combineByKey 与 aggregeteByKey下面的代码分别使用 aggregateByKey() 和 combineByKey() 来模拟 groupByKey() 和 reduceBykey() 使用 aggregateByKey 模拟 groupByKey 12345678910111213141516171819202122232425262728293031323334353637import org.apache.log4j.{Level, Logger}import org.apache.spark.rdd.RDDimport org.apache.spark.{SparkConf, SparkContext}import scala.collection.mutable.ArrayBufferobject aggregateByKey_1 { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(\"local[2]\").setAppName(aggregateByKey_1.getClass.getSimpleName) Logger.getLogger(\"org.apache.spark\").setLevel(Level.OFF) val sc = new SparkContext(conf) aggregateByKey2GroupByKey(sc) sc.stop() /** * 使用aggregateByKey模拟groupByKey */ def aggregateByKey2GroupByKey(sc: SparkContext): Unit = { val list = List(\"hello bo bo\", \"zhou xin xin\", \"hello song bo\") val lineRDD = sc.parallelize(list) val wordsRDD = lineRDD.flatMap(line =&gt; line.split(\" \")) val pairsRDD = wordsRDD.map(word =&gt; (word, 1)) val retRDD: RDD[(String, ArrayBuffer[Int])] = pairsRDD.aggregateByKey(ArrayBuffer[Int]())( (part, num) =&gt; { part.append(num) part }, (part1, part2) =&gt; { part1.++=(part2) part1 } ) retRDD.foreach(println) } }} 运行结果： 使用 aggregateByKey 模拟 reduceByKey 1234567891011121314151617181920212223242526272829import org.apache.log4j.{Level, Logger}import org.apache.spark.rdd.RDDimport org.apache.spark.{SparkConf, SparkContext}object aggregateByKey_2 { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(\"local[2]\").setAppName(aggregateByKey_2.getClass.getSimpleName) Logger.getLogger(\"org.apache.spark\").setLevel(Level.OFF) val sc = new SparkContext(conf) aggregateByKey2ReduceByKey(sc) sc.stop() /** * 使用aggregateByKey模拟reduceByKey */ def aggregateByKey2ReduceByKey(sc: SparkContext): Unit = { val list = List(\"hello bo bo\", \"zhou xin xin\", \"hello song bo\") val lineRDD = sc.parallelize(list) val wordsRDD = lineRDD.flatMap(line =&gt; line.split(\" \")) val pairsRDD = wordsRDD.map(word =&gt; (word, 1)) val retRDD: RDD[(String, Int)] = pairsRDD.aggregateByKey(0)( (partNum, num) =&gt; partNum + num, // 也就是mergeValue (partNum1, partNum2) =&gt; partNum1 + partNum2 // 也就是mergeCombiners ) retRDD.foreach(println) } }} 运行结果： 使用 combineByKey 模拟 reduceByKey 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import org.apache.log4j.{Level, Logger}import org.apache.spark.rdd.RDDimport org.apache.spark.{SparkConf, SparkContext}object combineByKey_1 { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(\"local[2]\").setAppName(combineByKey_1.getClass.getSimpleName) Logger.getLogger(\"org.apache.spark\").setLevel(Level.OFF) val sc = new SparkContext(conf) combineByKey2ReduceByKey(sc) sc.stop() /** * 使用combineByKey模拟reduceByKey */ def combineByKey2ReduceByKey(sc: SparkContext): Unit = { val list = List(\"hello bo bo\", \"zhou xin xin\", \"hello song bo\") val lineRDD = sc.parallelize(list) val wordsRDD = lineRDD.flatMap(line =&gt; line.split(\" \")) val pairsRDD = wordsRDD.map(word =&gt; (word, 1)) /** * 对于createCombiner1, mergeValue1, mergeCombiners1 * 代码的参数已经体现得很清楚了，其实只要理解了combineByKey模拟groupByKey的例子，这个就非常容易了 */ val retRDD: RDD[(String, Int)] = pairsRDD.combineByKey(createCombiner1, mergeValue1, mergeCombiners1) retRDD.foreach(println) } /** * reduceByKey操作，value就是该数值本身，则上面的数据会产生： * (hello, 1) (bo, 1) (bo, 1) * (zhou, 1) (xin, 1) (xin, 1) * (hello, 1) (song, 1) (bo, 1) * 注意有别于groupByKey的操作，它是创建一个容器 */ def createCombiner1(num: Int): Int = { num } /** * 同一partition内，对于有相同key的，这里的mergeValue直接将其value相加 * 注意有别于groupByKey的操作，它是添加到value到一个容器中 */ def mergeValue1(localNum1: Int, localNum2: Int): Int = { localNum1 + localNum2 } /** * 将两个不同partition中的key相同的value值相加起来 * 注意有别于groupByKey的操作，它是合并两个容器 */ def mergeCombiners1(thisPartitionNum1: Int, anotherPartitionNum2: Int): Int = { thisPartitionNum1 + anotherPartitionNum2 } }} 运行结果： 使用 combineByKey 模拟 groupByKey 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import org.apache.log4j.{Level, Logger}import org.apache.spark.rdd.RDDimport org.apache.spark.{SparkConf, SparkContext}import scala.collection.mutable.ArrayBufferobject combineByKey_2 { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(\"local[2]\").setAppName(combineByKey_2.getClass.getSimpleName) Logger.getLogger(\"org.apache.spark\").setLevel(Level.OFF) val sc = new SparkContext(conf) combineByKey2GroupByKey(sc) sc.stop() /** * 使用combineByKey模拟groupByKey */ def combineByKey2GroupByKey(sc: SparkContext): Unit = { val list = List(\"hello bo bo\", \"zhou xin xin\", \"hello song bo\") val lineRDD = sc.parallelize(list) val wordsRDD = lineRDD.flatMap(line =&gt; line.split(\" \")) val pairsRDD = wordsRDD.map(word =&gt; (word, 1)) // 输出每个partition中的map对 pairsRDD.foreachPartition(partition =&gt; { println(\"&lt;=========partition-start=========&gt;\") partition.foreach(println) println(\"&lt;=========partition-end=========&gt;\") }) val gbkRDD: RDD[(String, ArrayBuffer[Int])] = pairsRDD.combineByKey(createCombiner, mergeValue, mergeCombiners) gbkRDD.foreach(println) } /** * 初始化，将value转变成为标准的格式数据 * 是在每个分区中进行的操作，去重后的key有几个，就调用次， * 因为对于每个key，其容器创建一次就ok了，之后有key相同的，只需要执行mergeValue到已经创建的容器中即可 */ def createCombiner(num: Int): ArrayBuffer[Int] = { println(\"----------createCombiner----------\") ArrayBuffer[Int](num) } /** * 将key相同的value，添加到createCombiner函数创建的ArrayBuffer容器中 * 一个分区内的聚合操作，将一个分区内key相同的数据，合并 */ def mergeValue(ab: ArrayBuffer[Int], num: Int): ArrayBuffer[Int] = { println(\"----------mergeValue----------\") ab.append(num) ab } /** * 将key相同的多个value数组，进行整合 * 分区间的合并操作 */ def mergeCombiners(ab1: ArrayBuffer[Int], ab2: ArrayBuffer[Int]): ArrayBuffer[Int] = { println(\"----------mergeCombiners----------\") ab1 ++= ab2 ab1 } }}","link":"/2019/spark/"},{"title":"Spark —— 淘宝双 11 数据分析","text":"问题定义 实验内容 数据集 SparkRDD 实验过程及其结果 实验一：查看日志前 10 个交易日志的商品品牌 实验二：查询前 20 个交易日志中购买商品时的时间和商品的种类 实验三：查询双十一那天有多少人购买了商品 实验四：取给定时间和给定品牌，求当天购买的此品牌商品的数量 实验五：查询有多少用户在双十一点击了该店 实验六：查询双十一那天女性购买商品的数量 实验七：查询双十一那天男性购买商品的数量 实验八：查询某一天在该网站购买商品超过 5 次的用户 ID 使用 ECharts 进行可视化 可视化实验一：双十一所有买家消费行为比例 可视化实验二：双十一当天销量前十的商品类别 可视化实验三：双十一男女买家各个年龄段交易对比 可视化实验四：双十一男女买家交易对比 可视化实验五：各个省份的总成交量对比 实验过程中发现的问题 问题定义实验内容使用 Spark 对数据进行处理，并分析双十一的用户交易等信息，用 Scala 语言进行程序编写，最后将处理的数据结果使用 Echarts 进行可视化。 实验环境以及使用的相关应用： Spark 2.4.3 Scala 2.12 Tomcat 9.0.20 可视化工具：ECharts Java 包：fastjson 系统环境：macOS Mojave 10.14.5 数据集本案列主要分析淘宝双十一的数据，数据集是淘宝 2015 年双十一前 6 个月（包含双十一）的交易数据，数据集 user_log.csv，是记录了用户的行为的日志文件。 日志 user_log.csv 的字段定义如下： 序号 字段 定义 0 user_id 买家 id 1 item_id 商品 id 2 cat_id 商品类别 id 3 merchant_id 卖家 id 4 brand_id 品牌 id 5 month 交易时间：月 6 day 交易事件：日 7 action 行为，取值范围 {0,1,2,3}，0 表示点击，1 表示加入购物车，2 表示购买，3 表示关注商品 8 age_range 买家年龄分段：1 表示年龄 &lt;18；2 表示年龄在 [18,24]；3 表示年龄在 [25,29]；4 表示年龄在 [30,34]；5 表示年龄在 [35,39]；6 表示年龄在 [40,49]；7 和 8 表示年龄 &gt;=50；0 和 NULL 则表示未知 9 gender 性别：0 表示女性，1 表示男性，2 和 NULL 表示未知 10 province 收货地址省份 前五条记录样例： 12345user_id,item_id,cat_id,merchant_id,brand_id,month,day,action,age_range,gender,province328862,323294,833,2882,2661,08,29,0,0,1,内蒙古328862,844400,1271,2882,2661,08,29,0,1,1,山西328862,575153,1271,2882,2661,08,29,0,2,1,山西328862,996875,1271,2882,2661,08,29,0,1,1,内蒙古 SparkRDD 实验过程及其结果实验一：查看日志前 10 个交易日志的商品品牌分析：使用 take() 函数，取去掉首行后的前 10 行数据，按条件为 brand_id 的项进行筛选，并逐个输出 使用 take() 函数取出去掉首行后的前 10 行数据 进行 map 操作，将数据处理成 ($商品品牌) 样式 输出 map 后的结果 核心代码如下： 12345678910def transformationOps(sc: SparkContext): Unit = { // 查看日志前10个交易日志的商品品牌 val raw_data = sc.textFile(\"./data/user_log.csv\") val header = raw_data.first() val lines = raw_data.filter(row =&gt; row != header) lines.take(10).flatMap(line =&gt; line.split(\" \")) .map(line =&gt; (line.split(\",\")(4))) .foreach(println)} 结果如图： 实验二：查询前 20 个交易日志中购买商品时的时间和商品的种类分析：使用 take() 函数，取去掉首行后的前 20 行数据，按条件为 month, day, cat_id 的项进行筛选，并逐个输出。（这里将 month 和 day 合并为一个字符串） 使用 take() 函数取出去掉首行后的前 20 行数据 进行 map 操作，将数据处理成 ($日期, $商品种类) 样式 输出 map 后的结果 核心代码如下： 1234567891011def transformationOps(sc: SparkContext): Unit = { // 查询前20个交易日志中购买商品时的时间和商品的种类 val raw_data = sc.textFile(\"./data/user_log.csv\") val header = raw_data.first() val lines = raw_data.filter(row =&gt; row != header) val data = lines.take(20) .flatMap(line =&gt; line.split(\" \")) .map(line =&gt; (line.split(\",\")(5) +line.split(\",\")(6), line.split(\",\")(2))) data.foreach(println)} 结果如图： 实验三：查询双十一那天有多少人购买了商品分析： 首先进行 map 操作，将数据处理成 ($日期, $用户ID, $用户行为) 样式 然后使用 filter() 函数过滤出 11 月 11 日所有用户行为为购买的条目，具体为日期 month == 11, day == 11，行为 action == 2 考虑到单个用户可能购买多个商品，所以需要去重，使用 distinct() 函数去重 最后使用 count() 函数计数并输出 核心代码如下： 1234567891011def transformationOps(sc: SparkContext): Unit = { // 查询双十一那天有多少人购买了商品 val raw_data = sc.textFile(\"./data/user_log.csv\") val Users = raw_data.flatMap(line =&gt; line.split(\" \")) .map(line =&gt; (line.split(\",\")(5) + line.split(\",\")(6), line.split(\",\")(0), line.split(\",\")(7))) val output = Users.filter(line =&gt; line._3.contains(\"2\")) .filter(line =&gt; line._1.contains(\"1111\")) .distinct() println(\"双十一有 \" + output.count() + \" 人购买了商品。\")} 结果如图： 实验四：取给定时间和给定品牌，求当天购买的此品牌商品的数量分析： 首先进行 map 操作，将数据处理成 ($品牌ID, $日期, $用户行为) 样式 然后使用 filter() 函数过滤出 11 月 11 日品牌 2661 中用户行为为购买的条目，具体为日期 month == 11, day == 11，品牌 IDbrand_id == 2661，行为 action == 2 使用 count() 函数计数并输出 核心代码如下： 1234567891011def transformationOps(sc: SparkContext): Unit = { // 取给定时间和给定品牌，求当天购买的此品牌商品的数量 val raw_data = sc.textFile(\"./data/user_log.csv\") val data = raw_data.flatMap(line =&gt; line.split(\" \")) .map(line =&gt; (line.split(\",\")(4), line.split(\",\")(5) + line.split(\",\")(6), line.split(\",\")(7))) val output = data.filter(line =&gt; line._1.contains(\"2661\")) .filter(line =&gt; line._2.contains(\"1111\")) .filter(line =&gt; line._3.contains(\"2\")) println(\"11月11日，品牌2661的商品售出的数量为：\" + output.count())} 结果如图： 实验五：查询有多少用户在双十一点击了该店分析： 首先进行 map 操作，将数据处理成 ($用户ID, $卖家ID, $日期, $用户行为) 样式 然后使用 filter() 函数过滤出 11 月 11 日店铺 2882 中用户行为为点击的用户，具体为日期 month == 11, day == 11，卖家 IDmerchant_id == 2882，行为 action == 0 考虑到每个用户可能多次点击同一店铺，所以需要去重，使用 distinct() 函数。 核心代码如下： 1234567891011121314def transformationOps(sc: SparkContext): Unit = { // 查询有多少用户在双十一点击了该店 val raw_data = sc.textFile(\"./data/user_log.csv\") val data = raw_data.flatMap(line =&gt; line.split(\" \")) .map(line =&gt; (line.split(\",\")(0), // 用户ID line.split(\",\")(3), // 卖家ID line.split(\",\")(5) + line.split(\",\")(6), // 日期 line.split(\",\")(7))) // 行为 val output = data.filter(line =&gt; line._3.contains(\"1111\")) .filter(line =&gt; line._2.contains(\"2882\")) .filter(line =&gt; line._4.contains(\"0\")).distinct() println(\"一共有\" + output.count() + \"个用户在双十一点击了店铺2882\")} 结果如图： 实验六：查询双十一那天女性购买商品的数量分析：统计同时满足日期 month == 11, day == 11，行为 action == 2，性别 gender == 0 的个数。 首先进行 map 操作，将数据处理成 (($用户ID, $日期, $用户行为), 1) 样式 然后使用 filter() 函数过滤出 11 月 11 日用户行为为购买的用户，具体为日期 month == 11, day == 11，行为 action == 2 然后使用 reduceByKey() 统计所有用户的购买量，然后使用 filter() 筛选出大于 5 次的用户，并逐个输出 核心代码如下： 12345678910111213def transformationOps(sc: SparkContext): Unit = { // 查询双十一那天女性购买商品的数量 val raw_data = sc.textFile(\"./data/user_log.csv\") val data = raw_data.flatMap(line =&gt; line.split(\" \")) .map(line =&gt; (line.split(\",\")(5) + line.split(\",\")(6), // 日期 line.split(\",\")(7), // 行为 line.split(\",\")(9))) // 性别 val res = data.filter(line =&gt; line._1.contains(\"1111\")) .filter(line =&gt; line._2.contains(\"2\")) .filter(line =&gt; line._3.contains(\"0\")) println(\"双十一那天女性购买商品的数量为：\" + res.count())} 结果如图： 实验七：查询双十一那天男性购买商品的数量分析： 首先进行 map 操作，将数据处理成 ($日期, $用户行为, $性别) 样式 然后使用 filter() 函数过滤出 11 月 11 日用户行为为购买的男性用户，具体为日期 month == 11, day == 11，行为 action == 2，性别 gender == 1 然后使用 count() 函数统计所有过滤后的数据量，并输出 核心代码如下： 12345678910111213def transformationOps(sc: SparkContext): Unit = { // 查询双十一那天男性购买商品的数量 val raw_data = sc.textFile(\"./data/user_log.csv\") val data = raw_data.flatMap(line =&gt; line.split(\" \")) .map(line =&gt; (line.split(\",\")(5) + line.split(\",\")(6), // 日期 line.split(\",\")(7), // 行为 line.split(\",\")(9))) // 性别 val res = data.filter(line =&gt; line._1.contains(\"1111\")) .filter(line =&gt; line._2.contains(\"2\")) .filter(line =&gt; line._3.contains(\"1\")) println(\"双十一那天男性购买商品的数量为：\" + res.count())} 结果如图： 实验八：查询某一天在该网站购买商品超过 5 次的用户 ID分析： 首先进行 map 操作，将数据处理成 (($用户ID, $日期, $用户行为), 1) 样式 然后使用 filter() 函数过滤出 11 月 11 日用户行为为购买的用户，具体为日期 month == 11, day == 11，行为 action == 2 然后使用 reduceByKey() 统计所有用户的购买量，然后使用 filter() 筛选出大于 5 次的用户，并逐个输出 核心代码如下： 12345678910111213def transformationOps(sc: SparkContext): Unit = { // 查询某一天在该网站购买商品超过5次的用户ID val raw_data = sc.textFile(\"./data/user_log.csv\") val data = raw_data.flatMap(line =&gt; line.split(\" \")) .map(line =&gt; ((line.split(\",\")(0), //用户ID line.split(\",\")(5) + line.split(\",\")(6), // 日期 line.split(\",\")(7)), 1)) // 行为 val lines = data.filter(line =&gt; line._1._2.contains(\"1111\")).filter(line =&gt; line._1._3.contains(\"2\")) val Users = lines.reduceByKey((v1, v2) =&gt; v1 + v2).filter(t =&gt; t._2 &gt; 5) Users.foreach(userID =&gt; println(\"用户 \" + userID._1._1 + \" 在 \" + userID._1._2 + \" 购买了 \" + userID._2 + \" 件商品\")) println(\"11月11日购买超过5次的用户一共有：\" + Users.count() + \"个\")} 结果如图： 使用 ECharts 进行可视化 ECharts，一个使用 JavaScript 实现的开源可视化库，可以流畅的运行在 PC 和移动设备上，兼容当前绝大部分浏览器（IE8/9/10/11，Chrome，Firefox，Safari 等），底层依赖轻量级的矢量图形库 ZRender，提供直观，交互丰富，可高度个性化定制的数据可视化图表。 首先使用 SparkRDD 处理我们需要的数据，使用阿里巴巴的 Java 库 fastjson，将 SparkRDD 输出的数据转换成 JSON 格式的文档。然后使用 Tomcat 搭建 JSP 服务，用 jQuery 将 *.json 文件内的数据导入到 ECharts，最后呈现数据。 可视化实验一：双十一所有买家消费行为比例数据处理 分析： 首先进行 map 操作，将数据处理成 ($日期, $用户行为) 样式 然后使用 filter() 函数确定时间为 11 月 11 日，具体为日期 month == 11, day == 11 使用 map() 函数将数据处理为 ($用户行为, 1) 使用 reduceByKey() 计算出每个用户行为对应的条目数 将结果转换成 JSON 格式文件，并输出处理结果 数据处理核心代码如下： 1234567891011121314def transformationOps(sc: SparkContext): Unit = { // 双十一所有买家消费行为比例 val raw_data = sc.textFile(\"./data/user_log.csv\") val data = raw_data.flatMap(line =&gt; line.split(\"\\n\")) .map(line =&gt; ((line.split(\",\")(5) + line.split(\",\")(6), // 日期 line.split(\",\")(7)), 1)) // 行为 .filter(line =&gt; line._1._1.equals(\"1111\")) .map(t =&gt; (t._1._2, t._2)) val Output = data.reduceByKey((v1, v2) =&gt; v1 + v2).collect() toJSON_Pie(Output, sc) // 输出成JSON格式 Output.foreach(println)} 转换成 JSON 文件 编写 toJSON_Pie() 函数，将 SparkRDD 处理的结果转换成 JSON 格式的文件，用于生成饼状图 使用阿里巴巴 Java 库 fastjson，新建一个 JSONObject 对象 新建一个数组准备存放绘制 EChart 饼状图要求的 {\"name\":\"\",\"value\":\"\"} 对 使用循环，按行读取，放入 SparkRDD 处理的结果 导出，保存 函数代码如下： 1234567891011121314def toJSON_Pie(data: Array[(String, Int)], sc: SparkContext): Unit = { val JSONObject = new JSONObject() val m = data.length val list: util.List[Any] = new util.ArrayList[Any]() for (i &lt;- 0 until m) { val pair = new util.HashMap[String, Any]() pair.put(\"name\", data(i)._1.toString) pair.put(\"value\", data(i)._2.toString) list.add(pair) } JSONObject.put(\"data\", list) val output = sc.parallelize(List(JSONObject)) output.repartition(1).saveAsTextFile(\"../Visualization/web/data/v_1\")} 导出后的文件为 Spark 处理的文件，名称为 part-00000，需要手动将其加上 json 后缀为 part-00000.json 由于原始数据中代表用户行为的数据为 0~3，数字对可视化呈现不直观，所以手动更改了 json 文件的对应内容 part-00000.json 文件内容为： 1234567891011121314151617181920{ \"data\": [ { \"name\": \"点击\", \"value\": \"9188104\" }, { \"name\": \"加入购物车\", \"value\": \"14725\" }, { \"name\": \"购买\", \"value\": \"1223354\" }, { \"name\": \"关注\", \"value\": \"156450\" } ]} 可视化呈现 使用 Tomcat 搭建本地 Java Web Server，编写 jsp 网页，将 *.json 文件导入，使用 ECharts 呈现 根据 ECharts 官方文档：异步数据加载和更新，使用 jQuery 工具将 json 文件导入 核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647var myChart = echarts.init(document.getElementById('main'), 'shine');myChart.setOption({ title: { text: '双十一所有买家消费行为比例', x: 'center' }, tooltip: { trigger: 'item', formatter: \"{a} &lt;br/&gt; {b} : {c} ({d}%)\" }, legend: { orient: 'vertical', right: 50, top: 150, // bottom: 20, data: ['点击', '加入购物车', '购买', '关注'] }, label: { formatter: '{b}: {c}' }, series: [ { name: '行为', type: 'pie', radius: '60%', center: ['40%', '50%'], data: [], selectedMode: 'single', itemStyle: { emphasis: { shadowBlur: 10, shadowOffsetX: 0, shadowColor: 'rgba(0, 0, 0, 0.5)' } } } ]});$.get('./data/v_1/part-00000.json').done(function (data) { myChart.setOption({ series: [{ data: data.data }] });}); 可视化结果如图所示： 可视化实验二：双十一当天销量前十的商品类别数据处理 分析： 首先进行 map 操作，将数据处理成 ($日期, $商品类别, $用户行为) 样式 使用 filter() 函数确定时间为 11 月 11 日，用户行为为购买的数据，具体为日期 month == 11, day == 11，用户行为 action == 2 使用 map() 函数将数据处理为 ($商品类别, 1) 使用 reduceByKey() 计算出每个商品类别对应的条目数 使用 sortBy() 按条目数量降序排序 使用 take() 取前 10 个数据，然后将结果转换成 JSON 格式文件，并输出处理结果 数据处理核心代码如下： 123456789101112131415161718def transformationOps(sc: SparkContext): Unit = { // 双十一当天销量前十的商品类别 val raw_data = sc.textFile(\"./data/user_log.csv\") val data = raw_data.flatMap(line =&gt; line.split(\"\\n\")) .map(line =&gt; ((line.split(\",\")(5) + line.split(\",\")(6), // 日期 line.split(\",\")(2), // 商品类别 line.split(\",\")(7)), 1)) .filter(_._1._1.equals(\"1111\")) .filter(_._1._3.equals(\"2\")) .map(t =&gt; (t._1._2, t._2)) val Output = data.reduceByKey((v1, v2) =&gt; v1 + v2) .sortBy(_._2, ascending = false, numPartitions = 1).collect() toJSON_Bar(Output.take(10), sc) Output.take(10).foreach(println)} 转换成 JSON 文件 编写 toJSON_Bar() 函数，将 SparkRDD 处理的结果转换成 JSON 格式的文件，用于生成柱状图 使用阿里巴巴 Java 库 fastjson，新建一个 JSONObject 对象 新建两个数组准备存放绘制 EChart 柱状图要求的数据 使用循环，按行读取，放入 SparkRDD 处理的结果 导出，保存 函数代码如下： 1234567891011121314def toJSON_Bar(data: Array[(String, Int)], sc: SparkContext): Unit = { val JSONObject = new JSONObject() val m = data.length val key: Array[Any] = new Array[Any](m) val value: Array[Any] = new Array[Any](m) for (i &lt;- 0 until m) { key(i) = data(i)._1.toString value(i) = data(i)._2.toString } JSONObject.put(\"key\", key) JSONObject.put(\"value\", value) val output = sc.parallelize(List(JSONObject)) output.repartition(1).saveAsTextFile(\"../Visualization/web/data/v_2\")} 导出后的文件为 Spark 处理的文件，名称为 part-00000，需要手动将其加上 json 后缀为 part-00000.json part-00000.json 文件内容为： 1234{ \"value\":[\"48687\",\"41951\",\"29531\",\"28883\",\"28530\",\"28097\",\"27372\",\"26264\",\"25851\",\"23519\"], \"key\":[\"656\",\"1208\",\"602\",\"662\",\"737\",\"1142\",\"389\",\"177\",\"1213\",\"1438\"]} 其中，key 表示商品种类，value 表示对应商品种类的销量 可视化呈现 使用 Tomcat 搭建本地 Java Web Server，编写 jsp 网页，将 *.json 文件导入，使用 ECharts 呈现 根据 ECharts 官方文档：异步数据加载和更新，使用 jQuery 工具将 json 文件导入 核心代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// 基于准备好的dom，初始化echarts实例var myChart = echarts.init(document.getElementById('main'), 'shine');myChart.setOption({ title: { text: '双十一当天销量前10的商品类别', // x: 'center' }, legend: { data: [\"销量\"], top: '3' }, tooltip: { trigger: 'axis', axisPointer: { // 坐标轴指示器，坐标轴触发有效 type: 'shadow' // 默认为直线，可选为：'line' | 'shadow' } }, grid: { left: '3%', right: '4%', bottom: '3%', containLabel: true }, xAxis: [ { type: 'category', data: [], axisTick: { alignWithLabel: true } } ], yAxis: [ { type: 'value' } ], series: [ { name: '销量', type: 'bar', barWidth: '60%', data: [], label: { normal: { show: true, position: 'top' } } } ]});$.get('./data/v_2/part-00000.json').done(function (data) { myChart.setOption({ xAxis: [{ data: data.key }], series: [{ data: data.value }] });}); 可视化结果如图所示： 可视化实验三：双十一男女买家各个年龄段交易对比数据处理 分析： 首先进行 map 操作，将数据处理成 (($日期, $用户行为, $年龄段, $性别), 1) 样式 然后使用 filter() 函数确定时间为 11 月 11 日，用户行为为购买，具体为日期 month == 11, day == 11，用户行为 action == 2 使用 map() 函数将数据处理为 (($年龄段, $性别), 1) 使用 reduceByKey() 计算出每个年龄段每个性别对应的条目数 为了方便后续生成对应的 JSON 格式数据，使用两个 sortBy() 函数将数据按年龄段从小到大，性别 “女，男，未知” 顺序排序 将结果转换成 JSON 格式文件，并输出处理结果 数据处理核心代码如下： 123456789101112131415161718def transformationOps(sc: SparkContext): Unit = { // 双十一男女买家各个年龄段交易对比 val raw_data = sc.textFile(\"./data/user_log.csv\") val data = raw_data.flatMap(line =&gt; line.split(\"\\n\")) .map(line =&gt; ((line.split(\",\")(5) + line.split(\",\")(6), // 日期 line.split(\",\")(7), // 行为 line.split(\",\")(8), // 年龄段 line.split(\",\")(9)), 1)) // 性别 .filter(line =&gt; line._1._1.equals(\"1111\")) .filter(line =&gt; line._1._2.equals(\"2\")) .map(t =&gt; ((t._1._3, t._1._4), t._2)) val Output = data.reduceByKey((v1, v2) =&gt; v1 + v2) .sortBy(_._1._2).sortBy(_._1._1).collect() toJSON_MultiBar(Output, sc) Output.foreach(println)} 转换成 JSON 文件 编写 toJSON_MultiBar() 函数，将 SparkRDD 处理的结果转换成 JSON 格式的文件，用于生成柱状图 使用阿里巴巴 Java 库 fastjson，新建一个 JSONObject 对象 新建四个数组分别存放：年龄段；女性、男性、其他在各个年龄段的交易量 使用循环，按行读取，按照不同性别筛选，将 SparkRDD 处理的结果放入对应的数组 导出，保存 函数代码如下： 123456789101112131415161718192021222324252627282930313233343536373839def toJSON_MultiBar(data: Array[((String, String), Int)], sc: SparkContext): Unit = { val JSONObject = new JSONObject() val m = data.length val age: Array[Any] = new Array[Any](8) val woman: Array[Any] = new Array[Any](8) val man: Array[Any] = new Array[Any](8) val other: Array[Any] = new Array[Any](8) for (j &lt;- 0 to 7) { age(j) = j.toString // 7个对应的年龄段 } JSONObject.put(\"age\", age) var x1 = 0 var x2 = 0 var x3 = 0 for (i &lt;- 0 until m) { if (data(i)._1._2 == \"0\") { woman(x1) = data(i)._2.toString // 将性别女对应的条目写入 woman 数组 x1 = x1 + 1 } else if (data(i)._1._2 == \"1\") { man(x2) = data(i)._2.toString // 将性别男对应的条目写入 man 数组 x2 += 1 } else { other(x3) = data(i)._2.toString // 将性别未知对应的条目写入 other 数组 x3 += 1 } } JSONObject.put(\"woman\", woman) JSONObject.put(\"man\", man) JSONObject.put(\"other\", other) val output = sc.parallelize(List(JSONObject)) output.repartition(1).saveAsTextFile(\"./Visualization/web/data/v_3\")} 导出后的文件为 Spark 处理的文件，名称为 part-00000，需要手动将其加上 json 后缀为 part-00000.json part-00000.json 文件内容为： 123456{ \"other\":[\"51126\",\"50628\",\"50989\",\"51169\",\"50916\",\"50706\",\"51183\",\"51029\"], \"woman\":[\"50590\",\"51337\",\"50854\",\"50978\",\"51259\",\"51244\",\"51517\",\"50872\"], \"man\":[\"50855\",\"50732\",\"51195\",\"51076\",\"50798\",\"50555\",\"50855\",\"50891\"], \"age\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]} 可视化呈现 使用 Tomcat 搭建本地 Java Web Server，编写 jsp 网页，将 *.json 文件导入，使用 ECharts 呈现 根据 ECharts 官方文档：异步数据加载和更新，使用 jQuery 工具将 json 文件导入 核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// 基于准备好的dom，初始化echarts实例var myChart = echarts.init(document.getElementById('main'), 'shine');$.get('./data/v_3/part-00000.json').done(function (data) { myChart.setOption({ title: { text: '双十一男女买家各个年龄段交易对比', x: 'center' }, legend: { data: [\"女\", \"男\", \"未知\"], right: '20', top: '30' }, tooltip: { trigger: 'axis', axisPointer: { // 坐标轴指示器，坐标轴触发有效 type: 'shadow' // 默认为直线，可选为：'line' | 'shadow' } }, grid: { left: '3%', right: '4%', bottom: '3%', containLabel: true }, xAxis: [ { type: 'category', data: data.age, axisTick: { alignWithLabel: true } } ], yAxis: [ { type: 'value' } ], series: [ { type: 'bar', name: '女', data: data.woman }, { type: 'bar', name: '男', data: data.man }, { type: 'bar', name: '未知', data: data.other } ] });}); 可视化结果如图所示： 可视化实验四：双十一男女买家交易对比数据处理 分析： 首先进行 map 操作，将数据处理成 (($日期, $用户行为, $性别), 1) 样式 然后使用 filter() 函数确定时间为 11 月 11 日，用户行为为购买，具体为日期 month == 11, day == 11，用户行为 action == 2 使用 map() 函数将数据处理为 ($性别, 1) 使用 reduceByKey() 计算出每个性别对应的条目数 将结果转换成 JSON 格式文件，并输出处理结果 数据处理核心代码如下： 12345678910111213141516def transformationOps(sc: SparkContext): Unit = { // 双十一男女买家交易对比 val raw_data = sc.textFile(\"./data/user_log.csv\") val data = raw_data.flatMap(line =&gt; line.split(\"\\n\")) .map(line =&gt; ((line.split(\",\")(5) + line.split(\",\")(6), // 日期 line.split(\",\")(7), // 行为 line.split(\",\")(9)), 1)) // 性别 .filter(line =&gt; line._1._1.equals(\"1111\")) .filter(line =&gt; line._1._2.equals(\"2\")) .map(t =&gt; (t._1._3, t._2)) val Output = data.reduceByKey((v1, v2) =&gt; v1 + v2).collect() toJSON_Pie(Output, sc) Output.foreach(println)} 转换成 JSON 文件 编写 toJSON_Pie() 函数，将 SparkRDD 处理的结果转换成 JSON 格式的文件，用于生成饼状图 使用阿里巴巴 Java 库 fastjson，新建一个 JSONObject 对象 新建一个数组准备存放绘制 EChart 饼状图要求的 {\"name\":\"\",\"value\":\"\"} 对 使用循环，按行读取，放入 SparkRDD 处理的结果 导出，保存 函数代码如下： 1234567891011121314def toJSON_Pie(data: Array[(String, Int)], sc: SparkContext): Unit = { val JSONObject = new JSONObject() val m = data.length val list: util.List[Any] = new util.ArrayList[Any]() for (i &lt;- 0 until m) { val pair = new util.HashMap[String, Any]() pair.put(\"name\", data(i)._1.toString) pair.put(\"value\", data(i)._2.toString) list.add(pair) } JSONObject.put(\"data\", list) val output = sc.parallelize(List(JSONObject)) output.repartition(1).saveAsTextFile(\"./Visualization/web/data/v_4\")} 导出后的文件为 Spark 处理的文件，名称为 part-00000，需要手动将其加上 json 后缀为 part-00000.json 由于原始数据中代表用户行为的数据为 0~3，数字对可视化呈现不直观，所以手动更改了 json 文件的对应内容 part-00000.json 文件内容为： 12345678910111213141516{ \"data\": [ { \"name\": \"女\", \"value\": \"408651\" }, { \"name\": \"男\", \"value\": \"406957\" }, { \"name\": \"未知\", \"value\": \"407746\" } ]} 可视化呈现 使用 Tomcat 搭建本地 Java Web Server，编写 jsp 网页，将 *.json 文件导入，使用 ECharts 呈现 根据 ECharts 官方文档：异步数据加载和更新，使用 jQuery 工具将 json 文件导入 核心代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 基于准备好的dom，初始化echarts实例var myChart = echarts.init(document.getElementById('main'), 'shine');// $.getJSON('./data/v_1/part-00000.json').done(function (data) {myChart.setOption({ title: { text: '双十一男女买家交易对比', x: 'center' }, tooltip: { trigger: 'item', formatter: \"{a} &lt;br/&gt; {b}: {c} ({d}%)\" }, label: { formatter: \"{b}: {c}\" }, legend: { orient: 'vertical', right: 50, top: 150, // bottom: 20, data: ['女', '男', '未知'] }, series: [ { name: '性别', type: 'pie', radius: '60%', center: ['40%', '50%'], data: [], selectedMode: 'single', itemStyle: { emphasis: { shadowBlur: 10, shadowOffsetX: 0, shadowColor: 'rgba(0, 0, 0, 0.5)' } } } ]});$.get('./data/v_4/part-00000.json').done(function (data) { myChart.setOption({ series: [{ data: data.data }] });}); 可视化结果如图所示： 可视化实验五：各个省份的总成交量对比数据处理 分析： 首先进行 map 操作，将数据处理成 (($省份, $用户行为), 1) 样式 然后使用 filter() 函数确定用户行为为购买，具体为用户行为 action == 2 使用 map() 函数将数据处理为 ($省份, 1) 使用 reduceByKey() 计算出每个省份对应的条目数 将结果转换成 JSON 格式文件，并输出处理结果 数据处理核心代码如下： 12345678910111213def transformationOps(sc: SparkContext): Unit = { // 各个省份的总成交量对比 val raw_data = sc.textFile(\"./data/user_log.csv\") val data = raw_data.flatMap(line =&gt; line.split(\"\\n\")) .map(line =&gt; ((line.split(\",\")(10), line.split(\",\")(7)), 1)) .filter(line =&gt; line._1._2.contains(\"2\")) .map(t =&gt; (t._1._1, t._2)) val Output = data.reduceByKey((v1, v2) =&gt; v1 + v2).collect() toJSON_Map(Output, sc) Output.foreach(println)} 转换成 JSON 文件 编写 toJSON_Pie() 函数，将 SparkRDD 处理的结果转换成 JSON 格式的文件，用于生成饼状图 使用阿里巴巴 Java 库 fastjson，新建一个 JSONObject 对象 新建一个数组准备存放绘制 EChart 地图要求的 {\"name\":\"\",\"value\":\"\"} 对 使用循环，按行读取，放入 SparkRDD 处理的结果 导出，保存 函数代码如下： 1234567891011121314def toJSON_Map(data: Array[(String, Int)], sc: SparkContext): Unit = { val JSONObject = new JSONObject() val m = data.length val list: util.List[Any] = new util.ArrayList[Any]() for (i &lt;- 0 until m) { val pair = new util.HashMap[String, Any]() pair.put(\"name\", data(i)._1.toString) pair.put(\"value\", data(i)._2.toString) list.add(pair) } JSONObject.put(\"data\", list) val output = sc.parallelize(List(JSONObject)) output.repartition(1).saveAsTextFile(\"./Visualization/web/data/v_5\")} 导出后的文件为 Spark 处理的文件，名称为 part-00000，需要手动将其加上 json 后缀为 part-00000.json part-00000.json 文件内容为： 1234567891011121314151617181920212223242526272829303132333435363738{ \"data\":[ {\"name\":\"天津\",\"value\":\"97134\"}, {\"name\":\"吉林\",\"value\":\"96992\"}, {\"name\":\"四川\",\"value\":\"96743\"}, {\"name\":\"内蒙古\",\"value\":\"96619\"}, {\"name\":\"上海市\",\"value\":\"96893\"}, {\"name\":\"浙江\",\"value\":\"97162\"}, {\"name\":\"青海\",\"value\":\"96991\"}, {\"name\":\"河北\",\"value\":\"96674\"}, {\"name\":\"云南\",\"value\":\"96377\"}, {\"name\":\"香港\",\"value\":\"96587\"}, {\"name\":\"宁夏\",\"value\":\"96494\"}, {\"name\":\"甘肃\",\"value\":\"96822\"}, {\"name\":\"安徽\",\"value\":\"96488\"}, {\"name\":\"江西\",\"value\":\"96679\"}, {\"name\":\"江苏\",\"value\":\"96802\"}, {\"name\":\"湖南\",\"value\":\"97125\"}, {\"name\":\"西藏\",\"value\":\"97116\"}, {\"name\":\"福建\",\"value\":\"96625\"}, {\"name\":\"北京\",\"value\":\"96911\"}, {\"name\":\"陕西\",\"value\":\"96517\"}, {\"name\":\"山西\",\"value\":\"96861\"}, {\"name\":\"广西\",\"value\":\"96675\"}, {\"name\":\"澳门\",\"value\":\"96710\"}, {\"name\":\"湖北\",\"value\":\"96943\"}, {\"name\":\"山东\",\"value\":\"97123\"}, {\"name\":\"广东\",\"value\":\"97338\"}, {\"name\":\"台湾\",\"value\":\"96543\"}, {\"name\":\"辽宁\",\"value\":\"96714\"}, {\"name\":\"新疆\",\"value\":\"96960\"}, {\"name\":\"黑龙江\",\"value\":\"97169\"}, {\"name\":\"重庆\",\"value\":\"96813\"}, {\"name\":\"海南\",\"value\":\"96722\"}, {\"name\":\"河南\",\"value\":\"96980\"}, {\"name\":\"贵州\",\"value\":\"96842\"} ]} 可视化呈现 使用 Tomcat 搭建本地 Java Web Server，编写 jsp 网页，将 *.json 文件导入，使用 ECharts 呈现 根据 ECharts 官方文档：异步数据加载和更新，使用 jQuery 工具将 json 文件导入 核心代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// 基于准备好的dom，初始化echarts实例var myChart = echarts.init(document.getElementById('main'), 'shine');myChart.showLoading();$.getJSON('./data/china.json', function (geoJson) { myChart.hideLoading(); echarts.registerMap('CN', geoJson); myChart.setOption({ title: { text: '各个省份的总成交量对比', x: 'center' }, tooltip: { trigger: 'item', formatter: \"{b} &lt;br/&gt; {c} (件)\" }, toolbox: { show: true, orient: 'vertical', left: 'right', top: 'center', feature: { dataView: {readOnly: false}, restore: {}, saveAsImage: {} } }, visualMap: { min: 96000, max: 98000, text: ['高', '低'], realtime: false, calculable: true, inRange: { color: ['lightskyblue', 'yellow', 'orangered'] } }, series: [ { name: '各个省份的总成交量对比', type: 'map', map: 'CN', // 自定义扩展图表类型 itemStyle: { normal: {label: {show: true}}, emphasis: {label: {show: true}} }, data: [] } ] })});$.get('./data/v_5/part-00000.json').done(function (data) { myChart.setOption({ series: [{ data: data.data }] });}); 可视化结果如图所示： 实验过程中发现的问题在本次实验过程中发现了不少问题，有些通过在网络搜索解决了，但有些没有找到很好的解决方法，在此提出。 在编写可视化部分的代码时，使用 SparkRDD 生成的 JSON 格式的文件没有后缀，这时 jQuery 无法读取到文件的内容，必须手动更改 part-00000 文件名为 part-00000.json 没有找到合适的方法修改 SparkRDD 里的数据，当绘制可视化实验一：双十一所有买家消费行为比例和可视化实验四：双十一男女买家交易对比时，由于原始数据中代表用户行为的数据为 0 ~ 3，代表用户性别的数据为 0 ~ 1，数字对可视化呈现不直观，所以手动需要更改 json 文件的对应内容。同样在绘制可视化实验五：各个省份的总成交量对比时，数据里有些省份的名字和 china.js 中的省份名称不一样，比如北京和北京市，这样可视化的时候数据会不显示，也需要手动更改 JSON 文件内容。","link":"/2019/spark-taobao/"},{"title":"给 CentOS 7 minimal 安装图形界面","text":"以下操作不要使用 SSH 连接，SSH 客户端无法显示图形界面。 安装启用图形界面进入虚拟机，登录。 执行下面命令安装 GNOME Desktop Environment。 1$ yum -y groups install \"GNOME Desktop\" 接下来是漫长的等待时间………….. 待所有安装完成后，运行 1$ startx GNOME 桌面环境，启动！ 配置开机默认进入图形界面用下面命令查看查看是哪个模式。 1$ systemctl get-default 开机默认启动图形界面。 12$ systemctl set-default graphical.target # 图形界面模式$ reboot # 重启系统 如果想改回字符界面模式。 12$ systemctl set-default multi-user.target # 字符界面模式$ reboot # 重启系统 安装 VMware Tools在此之前需要安装开发环境。 1$ yum install perl gcc make kernel-headers kernel-devel -y 在 VMware 中选择虚拟机–&gt; 安装 VMware Tools，按提示操作。 由于我已经安装好，所以图片里显示的是 “重新安装 VMware Tools”。 安装过程一路回车，全部默认设置。 如遇到提示 1234The path \"/usr/src/kernels/3.10.0-957.5.1.el7.x86_64/include\" is not a valid path to the 3.10.0-957.el7.x86_64 kernel headers.Would you like to change it? [yes] 此处输入 no，回车，继续安装。 耐心等待后，安装完成！","link":"/2019/startx/"},{"title":"SSH 相关操作","text":"启用 SSH 并使用 root 账户登录虽说通过应用的形式在 Windows 10 上体验 Linux 是一个不赖的选择，但很多时候使用 Windows 内置的 CMD 或者 PowerShell 来操作 Linux 依旧有着很多不习惯。 而最为关键的是当需要对文件进行操作时，使用交互命令远不如使用 SFTP 来的更为「简单粗暴」。因此只要通过配置 SSH 远程登录，就可以像管理远程服务器那样来操作这个 Linux 系统了。 首先，因为 Ubuntu 系统限制，所以我们需要可以为 root 用户设置新密码，这里输入： 1sudo passwd root 配置好之后，未来使用 SSH 客户端或者 SFTP 客户端登录系统时，我们就可以直接使用 root 权限进行登录。 使用 cp 命令将 SSH 相关配置文件进行备份： 1sudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bak 之后使用 vim 编辑器编辑 sshd_config 文件： 1sudo vim /etc/ssh/sshd_config 编辑并调整以下设置项： 12345Port 22ListenAddress 0.0.0.0PermitRootLogin yes # 修改成 yes#StrictModes yes #注释掉这一行PasswordAuthentication yes # 原来是 no，改成 yes 然后输入命令： 1systemctl restart sshd 启动 SSH 如果要从其他电脑连接，记得防火墙放行端口 22 错误处理如果出现以下错误，可以通过 ssh-keygen 自动产生缺少的 key 1234* Stopping OpenBSD Secure Shell server sshd [ OK ] * Starting OpenBSD Secure Shell server sshd Could not load host key: /etc/ssh/ssh_host_rsa_keyCould not load host key: /etc/ssh/ssh_host_ecdsa_keyCould not load host key: /etc/ssh/ssh_host_ed25519_key 修复方法 执行 1sudo ssh-keygen -A 重启 ssh 服务 123root@Monster:~$ sudo service ssh --full-restart * Stopping OpenBSD Secure Shell server sshd [ OK ] * Starting OpenBSD Secure Shell server sshd [ OK ] 设置免密登录通常来说，启动 root 登录，并使用密码进行连接不仅不安全，而且连接需要输入密码，这里描述如何使用更安全的免密登录。 STEP 1：在本地生成公钥和私钥对1234# RSA 算法，长度 4096（默认为 2048）ssh-keygen -t rsa -b 4096 -f [FILE_NAME] -C [USERNAME]# ED25519 算法ssh-keygen -t ed25519 -f [FILE_NAME] -C [USERNAME] 一路回车，都设置为默认值，即在本地生成了公钥和私钥，设置 passphrase（更安全，但是每次连接前需要输入密码），存储在当前目录下（如果不使用 -f 参数，则会默认生成在～/.ssh/ 目录下） 123456&gt; .ssh tree -a.├── authorized_keys ├── id_rsa├── id_rsa.pub └── known_hosts 其中： id_rsa 是默认生成的私钥 id_rsa.pub 是默认生成的公钥 authorized_keys 文件存放其他主机的公钥，其他主机即可 ssh 登录该机 know_hosts 记录主机登陆过的其他主机的公钥信息。 STEP 2：把公钥加入到服务器的 authorized_keys 文件中使用 ssh-copy-id 命令，ssh-copy-id 命令可以把本地主机的公钥复制到远程主机的 authorized_keys 文件上，ssh-copy-id 命令也会给远程主机的用户主目录中 ~/.ssh/authorized_keys 设置合适的权限。 把本地的 ssh 公钥文件安装到远程主机对应的账户下： 12345ssh-copy-id user@server# 或ssh-copy-id -i ~/.ssh/id_rsa.pub user@server # 指定公钥路径# 或ssh-copy-id -i ~/.ssh/id_rsa.pub -p xxx user@server # 当端口号非默认的22时，使用此命令，xxx处为指定端口号 可选步骤：配置本地 config 文件在本地机器的目录 ~/.ssh 下找到 config 文件，按如下进行配置： 12345Host xxx # xxx：一个服务器昵称 HostName xx.xx.xx.xx # 目的机器的ip User username # ssh登陆时候的用户名 Port 22 # ssh所使用的端口，默认是22 IdentityFile /Users/morooi/.ssh/vultr # 对应服务器公钥的本地私钥文件路径 以后只要 ssh xxx 便可登陆服务器啦～ SCP 命令scp 命令可用于 linux/macOS 之间复制文件和目录。scp 是 secure copy 的缩写，scp 是 linux/macOS 系统下基于 ssh 登陆进行安全的远程文件拷贝命令。scp 是加密的 从本地复制到远程123456789101112131415161718192021222324# 指定了用户名，命令执行后需要再输入密码，仅指定了远程的目录，文件名字不变scp local_file remote_username@remote_ip:remote_folder# 指定了用户名，命令执行后需要再输入密码，指定了文件名scp local_file remote_username@remote_ip:remote_file# 没有指定用户名，命令执行后需要输入用户名和密码，仅指定了远程的目录，文件名字不变scp local_file remote_ip:remote_folder# 没有指定用户名，命令执行后需要输入用户名和密码，指定了文件名scp local_file remote_ip:remote_file# 复制目录，将local_folder复制到remote_folder下scp -r local_folder remote_username@remote_ip:remote_folder scp -r local_folder remote_ip:remote_folder# 实例scp ~/Downloads/paper.pdf morooi@192.168.1.1:~/Downloadsscp ~/Downloads/paper.pdf morooi@192.168.1.1:~/Downloads/paper.pdfscp ~/Downloads/paper.pdf 192.168.1.1:~/Downloadsscp ~/Downloads/paper.pdf 192.168.1.1:~/Downloads/paper.pdfscp -r ~/Downloads/papercode morooi@192.168.1.1:~/Downloads/scp -r ~/Downloads/papercode 192.168.1.1:~/Downloads/ 从远程复制到本地12345# 只要将从本地复制到远程的命令的后2个参数调换顺序即可，如下scp morooi@192.168.1.1:~/Downloads ~/Downloads/paper.pdfscp -r morooi@192.168.1.1:~/Downloads/ ~/Downloads/papercode# 可使用-p指定端口","link":"/2019/ssh_nopassw/"},{"title":"Tmux 的使用和记录","text":"之前听说过 Tmux，有很强大的功能，会话复用，分屏，保存当前会话，共享会话… 今天专门学习了解了下这玩意，本文是一篇安装配置记录（只是怕忘了快捷键 🙂) 内容如下： 安装 Tmux 配置 Tmux 常用命令 快捷键 会话 窗口（标签页） 调整窗口排序 窗格 调整窗格尺寸 其他 其他设置 启动 shell 时自动启动 tmux 文中使用环境为 CentOS 7 安装 TmuxCentOS 7 中使用 yum 安装只有 1.8 版本，所以建议下载源码编译安装 包管理方式安装（除 macOS 外，可能版本不是最新）： 12345678# CentOSyum install tmux# Ubuntu &amp; Debainapt-get install tmux# macOSbrew install tmux 源码编译安装，编译前需先安装：ncurses 和 libevent 1234567# 下载最新的 release 版本: https://github.com/tmux/tmux/releases/latest# 以当前（2020-7-3）最新 3.1b 为例wget https://github.com/tmux/tmux/releases/download/3.1b/tmux-3.1b.tar.gztar -zxvf tmux-3.1b.tar.gzcd tmux-3.1b./configure &amp;&amp; makesudo make install 配置 Tmux在用户目录新建 .tmux.conf 文件，内容如下 12345678910111213141516171819202122232425262728293031set -g prefix C-a # 修改 Prefix 组合键为 Ctrl + aunbind C-b # 取消 Ctrl + b 的快捷键组合bind C-a send-prefix # 修改 Prefix 组合键为 Ctrl + aset -g prefix2 ` # 修改 Prefix 备用组合键为 `set -g base-index 1 # 窗口编号从 1 开始计数set -g pane-base-index 1 # 窗格编号从 1 开始计数set -g status-bg black # 设置状态栏背景黑色set -g status-fg yellow # 设置状态栏前景黄色set -g mouse on # 开启鼠标set -g default-terminal \"screen-256color\" # 设置默认终端模式为 256colorsetw -g allow-rename off # 禁止活动进程修改窗口名setw -g automatic-rename off # 禁止自动命名新窗口setw -g mode-keys vi # 进入复制模式的时候使用 vi 键位（默认是 EMACS）# -----------------------------------------------------------------------------# 使用插件 - via tpm# 1. 执行 git clone https://github.com/tmux-plugins/tpm ~/.tmux/plugins/tpm# 2. 执行 bash ~/.tmux/plugins/tpm/bin/install_plugins 或在 Tmux 内 Prefix + I# -----------------------------------------------------------------------------setenv -g TMUX_PLUGIN_MANAGER_PATH '~/.tmux/plugins'set -g @plugin 'tmux-plugins/tmux-pain-control' # 方便分屏操作set -g @plugin 'tmux-plugins/tmux-resurrect' # 保存会话# tmux-resurrectset -g @resurrect-dir '~/.tmux/resurrect'# 初始化 TPM 插件管理器 (放在配置文件的最后)run '~/.tmux/plugins/tpm/tpm' 常用命令启动新会话： 1tmux [new -s 会话名 -n 窗口名] 恢复会话： 1tmux at [-t 会话名] 列出所有会话： 1tmux ls 关闭会话： 1tmux kill-session -t 会话名 关闭所有会话： 1tmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill 快捷键先按下设置好的 Prefix 键，默认为 Ctrl + b，上面配置文件已修改为 Ctrl + a 或 `，然后按如下按键 会话123:new &lt;回车&gt; 启动新会话s 列出所有会话$ 重命名当前会话 窗口（标签页）1234567c 创建新窗口w 列出所有窗口n 后一个窗口p 前一个窗口f 查找窗口, 重命名当前窗口&amp; 关闭当前窗口 调整窗口排序123swap-window -s 3 -t 1 交换 3 号和 1 号窗口swap-window -t 1 交换当前和 1 号窗口move-window -t 1 移动当前窗口到 1 号 窗格123456789101112131415% 垂直分割\" 水平分割o 交换窗格x 关闭窗格⍽ 左边这个符号代表空格键 - 切换布局q 显示每个窗格是第几个，当数字出现的时候按数字几就选中第几个窗格{ 与上一个窗格交换位置} 与下一个窗格交换位置z 切换窗格最大化/最小化# 使用 tmux-pain-control 插件| 垂直分割- 水平分割\\ 整体垂直分割_ 整体水平分割 调整窗格尺寸123456789101112PREFIX : resize-pane -D 当前窗格向下扩大 1 格PREFIX : resize-pane -U 当前窗格向上扩大 1 格PREFIX : resize-pane -L 当前窗格向左扩大 1 格PREFIX : resize-pane -R 当前窗格向右扩大 1 格PREFIX : resize-pane -D 20 当前窗格向下扩大 20 格PREFIX : resize-pane -t 2 -L 20 编号为 2 的窗格向左扩大 20 格# 使用 tmux-pain-control 插件shift + h 将分割线左移 5 格shift + j 将分割线下移 5 格shift + k 将分割线上移 5 格shift + l 将分割线右移 5 格 其他同步窗格，按下 Prefix，输入如下内容后回车，这个选项值针对某个窗口有效，不会影响别的会话和窗口 1:setw synchronize-panes on/off 杂项： 1234d 退出 tmux（tmux 仍在后台运行）t 窗口中央显示一个数字时钟? 列出所有快捷键: 命令提示符 其他设置启动 shell 时自动启动 tmux下面的配置会尝试只启动一个会话，登录时，如果之前启动过会话，那么它会直接 attach，而不是新开一个。 加入到 .bashrc 或 .zshrc 中。 12345# TMUXif which tmux &gt;/dev/null 2&gt;&amp;1; then #i f not inside a tmux session, and if no session is started, start a new session test -z \"$TMUX\" &amp;&amp; (tmux attach || tmux new-session)fi 参考： Tmux 快捷键 &amp; 速查表 &amp; 简明教程 Tmux 使用手册 Tmux Pain Control","link":"/2020/tmux/"},{"title":"macOS、iOS、Windows 解锁网易云音乐灰色歌曲","text":"废话不多说，先上效果图！ 感谢大神们的开发，本文仅记录自己的配置过程，大部分方法都来自项目的 issue 项目地址：https://github.com/nondanee/UnblockNeteaseMusic 更新：原项目貌似停止维护了，但有大佬们仍在继续更新拓展 (https://github.com/1715173329/UnblockNeteaseMusic) 感谢、感谢、感谢 直接开始！ 使用 Docker 部署https://hub.docker.com/r/nondanee/unblockneteasemusic/ 1docker pull nondanee/unblockneteasemusic 使用自签证书 1234567# 注意替换下方 -p 后的端口，-v 后的路径docker run --name unblockneteasemusic \\ -p 2333:2333 -p 2334:2334 \\ -v /path/to/server.crt:/usr/src/app/server.crt \\ -v /path/to/server.key:/usr/src/app/server.key \\ -d nondanee/unblockneteasemusic \\ -s -e https://自己的域名 -p 2333:2334 使用自带证书 12345# 注意替换下方 -p 后的端口，-v 后的路径docker run --name unblockneteasemusic \\ -p 2333:2333 -p 2334:2334 \\ -d nondanee/unblockneteasemusic \\ -s -e https://自己的域名 -p 2333:2334 旧：以下为手动配置 服务端设置原服务支持在本地运行，为方便我部署在服务器端，使用阿里云的轻量应用服务器，CentOS 7 准备一个域名，并签发 SSL 证书 安装 NodeJS：https://github.com/nodesource/distributions/blob/master/README.md 在 CentOS 7 中安装 NodeJS 13.x 1234567yum install gcc-c++ make# root 用户curl -sL https://rpm.nodesource.com/setup_13.x | bash -# 非 root 用户curl -sL https://rpm.nodesource.com/setup_13.x | sudo bash - 在服务器端 clone 项目 1git clone https://github.com/nondanee/UnblockNeteaseMusic.git —————- 可选步骤 —————- 在本地生成自签名证书 (此步可省略，使用作者提供的证书亦可) 1234567891011121314# 生成 CA 私钥openssl genrsa -out ca.key 2048# 生成 CA 证书 (\"YOURNAME\" 处填上你自己的名字)openssl req -x509 -new -nodes -key ca.key -sha256 -days 1825 -out ca.crt -subj \"/C=CN/CN=UnblockNeteaseMusic Root CA/O=YOURNAME\"# 生成服务器私钥openssl genrsa -out server.key 2048# 生成证书签发请求openssl req -new -sha256 -key server.key -out server.csr -subj \"/C=CN/L=Hangzhou/O=NetEase (Hangzhou) Network Co., Ltd/OU=IT Dept./CN=*.music.163.com\"# 使用 CA 签发服务器证书openssl x509 -req -extfile &lt;(printf \"subjectAltName=DNS:music.163.com,DNS:*.music.163.com\") -days 365 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt 上述步骤完成后 将服务器私钥 (server.key) 和服务器证书 (server.crt) 拷贝到远程服务器仓库中覆盖原有文件 —————- 可选步骤结束 —————- 为解决 iOS 能够使用，这里使用自己的域名进行转发，如没有域名则参考官方信任自签发证书的方法，这里不展开描述：iOS 食用指南 配置 Nginx 服务，在 Nginx 配置后加入如下配置 123456789101112131415server { listen 443 ssl http2; server_name xxx.xxx.com; # 准备的域名 ssl_certificate /root/UnblockNeteaseMusic/xxx.xxx.xxx.crt; # SSL 证书路径 ssl_certificate_key /root/UnblockNeteaseMusic/xxx.xxx.xxx.key; # SSL 密钥路径 ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE; ssl_prefer_server_ciphers on; location / { proxy_pass http://localhost:2333; # 代理到 Unblock 服务地址，端口可自行修改 }} 完事后测试并重载 Nginx 配置 12345$ nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is oknginx: configuration file /etc/nginx/nginx.conf test is successful$ nginx -s reload 使用 service 管理服务，新建 /etc/systemd/system/UnblockNeteaseMusic.service 写入如下内容 /etc/systemd/system/UnblockNeteaseMusic.service12345678910111213[Unit]Description=UnblockNeteaseMusic[Service]Type=simpleExecStart=/usr/bin/node /root/UnblockNeteaseMusic/app.js -s -e https://自己的域名 -p 2333:2334WorkingDirectory=/root/UnblockNeteaseMusicRestart=alwaysRestartSec=10User=root[Install]WantedBy=multi-user.target 注意： ExecStart 和 WorkingDirectory 根据情况改动 ExecStart 中 -e 后接已经签发证书的域名，记得为 https。这一参数的原因详见：iOS 配置经验分享 ExecStart 中 -s 表示启用严格模式，仅允许网易云相关的通过，防止 HTTP 代理滥用，可选 ExecStart 中 -p 后接端口，随意设置，如 2333:2334，第一个端口保持和 nginx 配置中设置的一致 记得在阿里云控制台防火墙中放行第一个端口 2333 启动服务 12systemctl daemon-reloadsystemctl start UnblockNeteaseMusic 服务端完成 管理命令 123456systemctl status UnblockNeteaseMusic # 查看服务状态systemctl start UnblockNeteaseMusic # 启动服务systemctl restart UnblockNeteaseMusic # 重启服务systemctl stop UnblockNeteaseMusic # 停止服务systemctl enable UnblockNeteaseMusic # 开机自启动systemctl disable UnblockNeteaseMusic # 取消开机自启动 macOS 配合 Proxifier 食用 Proxies 添加 HTTPS 代理 添加规则 Applications 添加：NeteaseMusic; com.apple.WebKit.Networking Target Hosts 添加：*.music.163.com; *.netease.com; Action 选择上一步添加的 HTTPS 代理 打开网易云音乐 App，如出现以下提示，则还需要信任一个证书 使用 Safari 打开 music.163.com，会出现危险警告，选择查看详细信息，并点击最后访问此网站，然后信任证书 搞定！ 注意：Proxifier 需要一直在后台运行，才能转发网易云的流量 iOS配合 Quantumult X 食用 手动修改 Quantumult X 配置文件 在 [policy] 下添加组策略： 1static=NeteaseMusic, NeteaseUnlocker, direct, img-url=https://raw.githubusercontent.com/Koolson/Qure/master/IconSet/Netease_Music_Unlock.png 在 [server_local] 下添加 HTTP 代理： 1http=xxx.xxx.xxx.xxx:2333, over-tls=false, fast-open=false, udp-relay=false, tag=NeteaseUnlocker xxx.xxx.xxx.xxx:2333 替换为服务器 IP 和指定端口 在 [filter_local] 下添加规则： 1234DOMAIN-SUFFIX,music.163.com,NeteaseMusicDOMAIN-SUFFIX,api.iplay.163.com,NeteaseMusicDOMAIN-SUFFIX,mam.netease.com,NeteaseMusicDOMAIN-SUFFIX,hz.netease.com,NeteaseMusic 完成！ Windows配置简单，打开网易云音乐的设置 工具 -&gt; 自定义代理 -&gt; HTTP 代理 填入相应的服务器和端口，重启应用生效 完成！ 参考资料： 网易云音乐灰色版权解锁 - UnblockNeteaseMusic 项目食用指南 iOS 配置经验分享 食用指南 进阶配置 一个可以使用的教程 iOS 食用指南","link":"/2020/unblockneteasemusic/"},{"title":"Hexo 博客转移到阿里云","text":"之前小站一直托管在 Github Pages 中，然而国内访问实在太慢，百度也无法收录。正好趁着疫情，在阿里云撸了一台轻量应用服务器。记录下转移过程。 这里服务器选择的系统是 CentOS 7.3 配置过程大致如下：搭建 Git 私库 –&gt; 搭建 Nginx 服务器 –&gt; 启用 https 访问 … 搭建 Git 私有库博客是由 Hexo 在本地生成然后通过 Git 部署到服务器端的，所以要在服务器端搭建一个 Git 私有库 安装 Git1$ yum install -y git 创建用户并配置仓库123456789useradd gitpasswd git # 设置密码su git # 这步很重要，不切换用户后面会很麻烦cd /home/git/mkdir -p projects/blog # 项目存在的真实目录mkdir repos &amp;&amp; cd reposgit init --bare blog.git # 创建一个裸露的仓库cd blog.git/hooks vi post-receive，创建 hook 钩子函数，输入内容如下 12#!/bin/shgit --work-tree=/home/git/projects/blog --git-dir=/home/git/repos/blog.git checkout -f 添加完毕后修改权限 123chmod +x post-receiveexit # 退出到 root 登录chown -R git:git /home/git/repos/blog.git # 添加权限 测试 Git 仓库是否可用在本地电脑 clone 远程仓库 1git clone git@server_ip:/home/git/repos/blog.git 如提示 12git@server_ip: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).fatal: 无法读取远程仓库 则需要在服务器端打开 ssh 允许密码登录，修改 /etc/ssh/sshd_config 中 PasswordAuthentication 项为 yes 建立 ssh 信任关系在本地电脑 12ssh-copy-id -i ~/.ssh/id_rsa.pub git@server_ipssh git@server_ip # 测试能否登录 禁用 Git 用户的 shell 登录权限为了安全起见禁用 git 用户的 shell 登录权限，从而只能用 git clone，git push 等登录 12345cat /etc/shells # 查看 git-shell 是否在登录方式里面which git-shell # 查看是否安装# 添加上步显示出来的路径，通常是 /usr/bin/git-shellvi /etc/shells 修改 /etc/passwd 中的权限，只需将原来的 /bin/shell 修改为 /usr/bin/git-shell 12- git:x:1000:1000::/home/git:/bin/shell+ git:x:1000:1000:,,,:/home/git:/usr/bin/git-shell Hexo 部署到私有库修改博客根目录_config.yml 文件中 deploy 设置 _config.yml1234deploy: - type: git repository: git@xxx.xxx.xxx.xxx:/home/git/repos/blog.git branch: master 在博客根目录运行 hexo g -d 部署到阿里云服务器私有库 搭建 Nginx 服务器安装 Nginx1yum -y install nginx 启动 Nginx，浏览器访问服务器 IP 1nginx 配置 Nginx本站全站使用 https 访问 1nginx -s stop # 停止 Nginx 将 SSL 证书和私钥放在 /home/git/ 目录下 修改 /etc/nginx/nginx.conf server 部分为如下配置 /etc/nginx/nginx.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445server { # 重定向 http 请求 listen 80; server_name morooi.com; rewrite ^ https://$http_host$request_uri? permanent; }# Settings for a TLS enabled server.server { listen 443 ssl http2; # listen [::]:443 ssl http2; server_name morooi.com; root /home/git/projects/blog; ssl_certificate \"/home/git/morooi.com.crt\"; ssl_certificate_key \"/home/git/morooi.com.key\"; ssl_session_cache shared:SSL:1m; ssl_session_timeout 10m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; # HSTS add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains;preload\" always; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; #减少点击劫持 add_header X-Frame-Options DENY; #禁止服务器自动解析资源类型 add_header X-Content-Type-Options nosniff; #防XSS攻击 add_header X-Xss-Protection 1; location / { } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { }} 检查配置是否正确 nginx -t 12nginx: the configuration file /etc/nginx/nginx.conf syntax is oknginx: configuration file /etc/nginx/nginx.conf test is successful 通过检测～ 重载配置 nginx -s reload 访问成功！ 主要参考： 带你跳过各种坑，一次性把 Hexo 博客部署到自己的服务器 Nginx 配置 HTTPS 服务器","link":"/2020/toaliyun/"},{"title":"使用 Docker 自建支持 DoH、DoT 的 DNS 服务器","text":"DoH (DNS over HTTPS) 和 DoT (DNS over TLS) 有效避免了运营商的 DNS 监听和劫持，本文记录在公网搭建一个可以去广告的 DNS 服务器（借助 Adguard Home）。 前期准备 公网服务器一台 域名一个，并且指向该公网服务器 域名指向该公网服务器，并申请好 TLS 证书（可使用 Let’s Encrypt 申请） Docker 部署Docker Hub 镜像地址：https://hub.docker.com/r/adguard/adguardhome 安装好 Docker 后下载镜像，以下任选其一 12docker pull adguard/adguardhome:latest # 稳定版docker pull adguard/adguardhome:edge # 最新的版本，可能不稳定 新建配置目录和工作目录 12mkdir /root/adguardhome/workdirmkdir /root/adguardhome/confdir 启动镜像，完成后进入配置页面：HostIP:3000 123456docker run --name adguardhome \\ -v /root/adguardhome/workdir:/opt/adguardhome/work \\ -v /root/adguardhome/confdir:/opt/adguardhome/conf \\ --restart always \\ -p 443:443 -p 853:853 -p 3000:3000 -p 53:53/udp -p 53:53/tcp \\ -d adguard/adguardhome:edge 注意，第一次进入配置页面需要配置监听端口，配置好后注意修改容器启动时端口的映射，并以新的命令重新启动容器 上游服务器设置进入 Adguard Home 控制台，找到「设置 - DNS 设置」 「上游 DNS 服务器」可填入公共 DNS 服务器 国内常用的公共 DNS 如下： DNS 服务器地址 类型 提供商 https://dns.alidns.com/dns-query DNS over HTTPS 阿里 DNS tls://dns.alidns.com DNS over TLS 阿里 DNS https://doh.360.cn/dns-query DNS over HTTPS 360 DNS 119.29.29.29 常规 DNS DNSPod 182.254.116.116 常规 DNS DNSPod 223.5.5.5 常规 DNS 阿里 DNS 223.6.6.6 常规 DNS 阿里 DNS 114.114.114.114 常规 DNS 114 DNS 「Bootstrap DNS 服务器」填入一到两个常规 DNS 服务器就可以，如 223.5.5.5 和 119.29.29.29 在 「DNS 服务设定中」，勾选「使用客户端的子网地址（EDNS）」，可以使查询的客户端得到离其最近的网站 IP 解析。 DoH &amp; DoT 设置进入「设置 - 加密设置」 勾选「启用加密」，填写准备的域名，「HTTPS 端口」和「DNS-over-TLS 端口」均可自定义，如有改正记得重新修改容器启动时的端口映射配置。 证书可以选择「设置证书路径」和「粘贴证书内容」，按要求配置。 配置完成后可以使用「https://域名」访问控制台 去广告设置进入「过滤器 - DNS 封锁清单」 附一些常用的规则： HalfLife，规则合并自 EasylistChina、EasylistLite、CJX’sAnnoyance 合并规则：https://gitee.com/halflife/list/raw/master/ad.txt EasyPrivacy，隐私保护：https://easylist-downloads.adblockplus.org/easyprivacy.txt I don’t care about cookies，我不关心 Cookie 的问题，屏蔽网站的 cookies 相关的警告：https://www.i-dont-care-about-cookies.eu/abp/ anti-AD，自称是目前中文区命中率最高的广告过滤列表：https://gitee.com/privacy-protection-tools/anti-ad/raw/master/easylist.txt 乘风广告、视频过滤规则：https://gitee.com/xinggsf/Adblock-Rule/raw/master/rule.txt、https://gitee.com/xinggsf/Adblock-Rule/raw/master/mv.txt 至此配置完成。 使用普通 DNS 解析，在客户端填入服务器的 IP 即可，若要使用 DoH 和 DoT 则需使用另外的工具。 最新版的 Chrome 已经支持使用 DoH 解析，在 Chrome 中找到「设置 - 安全 - 使用安全 DNS」，按规则填入域名即可","link":"/2020/%E6%90%AD%E5%BB%BADNS%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"title":"安装 Zsh，并使用 Oh My Zsh","text":"Zsh 是什么shell 俗称壳，c 语言编写的命令解析器程序，是用户使用 linux 的桥梁。Linux/Unix 提供了很多种 Shell。常用的 Shell 有这么几种，sh、bash、csh 等。可以通过以下命令，查看系统有几种 shell： 1$ cat /etc/shells 目前常用的 Linux 系统和 macOS X 系统的默认 Shell 都是 bash。但是真正强大的 Shell 是深藏不露的 zsh，史称『终极 Shell』，由于与 bash 相似，功能又有所加强，zsh 在 Linux 社区获得了关注。但因配置过于复杂，所以初期无人问津。直到国外有个程序员开发出了一个能够快速上手的 zsh 项目，叫做「Oh My Zsh」，Github 网址是：https://github.com/ohmyzsh/ohmyzsh 安装 zsh2020.12.18 更新：撸了个安装脚本，传送门 -&gt; Github 查看系统中有无 zsh ，以及其版本 123$ cat /etc/shells$ zsh --version$ echo $ZSH_VERSION 若系统中没有 zsh ，则需要安装 12345$ sudo apt-get install zsh # Ubuntu$ sudo yum install zsh # CentOS# macOS 系统自带了zsh, 一般不是最新版，如果需要最新版可通过Homebrew来安装$ brew install zsh 修改默认 shell 12345$ chsh -s /bin/zsh# 或$ chsh -s /usr/bin/zsh# 或$ chsh -s $(which zsh) 安装 Oh My Zsh一键安装，方式如下： 12345678910#方法一：wget方式自动化安装oh my zsh：sh -c \"$(wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"#方法二：sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" #官网上的另外一种写法 curl -Lo install.sh https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.shsh install.sh 美化、插件、配置 以下是自用配置，做个备份 修改主题1$ vim ~/.zshrc 修改 ZSH_THEME=\"robbyrussell\" 为 ZSH_THEME=\"ys\" 修改显示方式1$ vim ~/.oh-my-zsh/themes/ys.zsh-theme 修改最下面部分为 12345678910PROMPT=\"%{$terminfo[bold]$fg[blue]%}#%{$reset_color%} \\%(#,%{$bg[yellow]%}%{$fg[black]%}%n%{$reset_color%},%{$fg[cyan]%}%n) \\%{$fg[white]%}in \\%{$terminfo[bold]$fg[yellow]%}%~%{$reset_color%}\\${hg_info}\\${git_info}\\ \\%{$fg[white]%}[%*] $exit_code%{$terminfo[bold]$fg[red]%}$ %{$reset_color%}\" 或改为（2020.1.7）： 123456789101112PROMPT=\"%{$terminfo[bold]$fg[blue]%}#%{$reset_color%} \\%(#,%{$bg[yellow]%}%{$fg[black]%}%n%{$reset_color%},%{$fg[cyan]%}%n) \\%{$fg[white]%}@ \\%{$fg[green]%}morooi's MacBook Pro \\%{$fg[white]%}in \\%{$terminfo[bold]$fg[yellow]%}%~%{$reset_color%}\\${hg_info}\\${git_info}\\ \\%{$fg[white]%}[%*] $exit_code%{$terminfo[bold]$fg[red]%}$ %{$reset_color%}\" 插件：zsh-syntax-highlightingshell 命令的代码高亮，Github 地址：https://github.com/zsh-users/zsh-syntax-highlighting 安装： Clone this repository in oh-my-zsh’s plugins directory: 1git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting Activate the plugin in ~/.zshrc: 1plugins=( [plugins...] zsh-syntax-highlighting) Restart zsh (such as by opening a new instance of your terminal emulator). 插件：zsh-autosuggestions在输入命令的过程中根据你的历史记录显示你可能想要输入的命令，按 tab 补全。 安装： Clone this repository into $ZSH_CUSTOM/plugins (by default ~/.oh-my-zsh/custom/plugins) 1git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Add the plugin to the list of plugins for Oh My Zsh to load (inside ~/.zshrc): 1plugins=(zsh-autosuggestions) Start a new terminal session. 插件：extract, zextract 用于解压任何压缩文件，不必根据压缩文件的后缀名来记忆压缩软件 z 用于目录间快速跳转 添加到 ~/.zshrc 1plugins=(extract z) 若提示字体颜色显示不正常，执行1$ echo \"export TERM=xterm-256color\" &gt;&gt; ~/.zshrc 自用 .zshrc 文件备份123456789101112131415161718192021222324252627282930313233343536373839404142434445464748export ZSH=\"/Users/morooi/.oh-my-zsh\"ZSH_THEME=\"ys\"HIST_STAMPS=\"yyyy-mm-dd\"plugins=(git zsh-completions z zsh-syntax-highlighting zsh-autosuggestions extract)source $ZSH/oh-my-zsh.sh# Example aliases# alias zshconfig=\"mate ~/.zshrc\"# alias ohmyzsh=\"mate ~/.oh-my-zsh\"# Brewalias bs=\"brew search\"alias bi=\"brew install\"alias bu=\"brew rmtree\"alias bup=\"brew upgrade\"alias bci=\"brew install --cask\"alias bcup=\"brew cu -a -f --cleanup\"alias bcu=\"brew uninstall --cask\"alias cleanbrewcache=\"rm -rf /Users/morooi/Library/Caches/Homebrew\"export PATH=\"/usr/local/sbin:$PATH\"# Javaexport JAVA_HOME_8=\"/Library/Java/JavaVirtualMachines/jdk1.8.0_231.jdk/Contents/Home\"export JAVA_HOME_11=\"/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home\"export JAVA_HOME_13=\"/Library/Java/JavaVirtualMachines/openjdk-13.jdk/Contents/Home\"export JAVA_HOME=$JAVA_HOME_13alias jdk8=\"export JAVA_HOME=$JAVA_HOME_8\"alias jdk11=\"export JAVA_HOME=$JAVA_HOME_11\"alias jdk13=\"export JAVA_HOME=$JAVA_HOME_13\"# NodeJSexport PATH=\"/usr/local/opt/node@12/bin:$PATH\"# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;# !! Contents within this block are managed by 'conda init' !!__conda_setup=\"$('/usr/local/miniconda3/bin/conda' 'shell.zsh' 'hook' 2&gt; /dev/null)\"if [ $? -eq 0 ]; then eval \"$__conda_setup\"else if [ -f \"/usr/local/miniconda3/etc/profile.d/conda.sh\" ]; then . \"/usr/local/miniconda3/etc/profile.d/conda.sh\" else export PATH=\"/usr/local/miniconda3/bin:$PATH\" fifiunset __conda_setup# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;","link":"/2019/zsh/"}],"tags":[{"name":"macOS","slug":"macOS","link":"/tags/macOS/"},{"name":"Conda","slug":"Conda","link":"/tags/Conda/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"acme.sh","slug":"acme-sh","link":"/tags/acme-sh/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"CentOS","slug":"CentOS","link":"/tags/CentOS/"},{"name":"dockerfile-maven-plugin","slug":"dockerfile-maven-plugin","link":"/tags/dockerfile-maven-plugin/"},{"name":"GPG","slug":"GPG","link":"/tags/GPG/"},{"name":"Github","slug":"Github","link":"/tags/Github/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"HBase","slug":"HBase","link":"/tags/HBase/"},{"name":"显卡驱动","slug":"显卡驱动","link":"/tags/%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8/"},{"name":"Ubuntu","slug":"Ubuntu","link":"/tags/Ubuntu/"},{"name":"MapReduce","slug":"MapReduce","link":"/tags/MapReduce/"},{"name":"wordcount","slug":"wordcount","link":"/tags/wordcount/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Next","slug":"Next","link":"/tags/Next/"},{"name":"Maven","slug":"Maven","link":"/tags/Maven/"},{"name":"Idea","slug":"Idea","link":"/tags/Idea/"},{"name":"Shell","slug":"Shell","link":"/tags/Shell/"},{"name":"RDD","slug":"RDD","link":"/tags/RDD/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"ECharts","slug":"ECharts","link":"/tags/ECharts/"},{"name":"ssh","slug":"ssh","link":"/tags/ssh/"},{"name":"Tmux","slug":"Tmux","link":"/tags/Tmux/"},{"name":"网易云音乐","slug":"网易云音乐","link":"/tags/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90/"},{"name":"iOS","slug":"iOS","link":"/tags/iOS/"},{"name":"Windows","slug":"Windows","link":"/tags/Windows/"},{"name":"DNS","slug":"DNS","link":"/tags/DNS/"},{"name":"DoH","slug":"DoH","link":"/tags/DoH/"},{"name":"DoT","slug":"DoT","link":"/tags/DoT/"},{"name":"AdguardHome","slug":"AdguardHome","link":"/tags/AdguardHome/"},{"name":"zsh","slug":"zsh","link":"/tags/zsh/"}],"categories":[{"name":"记录","slug":"记录","link":"/categories/%E8%AE%B0%E5%BD%95/"},{"name":"学习笔记","slug":"学习笔记","link":"/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"Linux 使用","slug":"Linux-使用","link":"/categories/Linux-%E4%BD%BF%E7%94%A8/"},{"name":"大数据","slug":"大数据","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"pages":[]}